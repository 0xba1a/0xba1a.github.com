{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Balakumaran Kannan <p>Welcome to my blog! </p> <p>I'm Bala, a Principal Software Engineer at  Microsoft.</p> <p>With a background as a Linux Systems Engineer, I've recently embarked on an exciting journey into the world of Artificial Intelligence. My current focus lies at the intersection of these domains\u2014exploring how Large Language Models (LLMs) can better understand Linux code to enhance developer productivity and streamline the development process.</p> <p>As a Stable Maintainer of Azure Linux (Microsoft's in-house Linux distribution), I bring hands-on experience with enterprise-grade Linux systems. Through this blog, I share my learnings, insights, and discoveries as I navigate the evolving landscape of technology.</p>"},{"location":"#linkedin","title":"LinkedIn","text":""},{"location":"#my-experience","title":"My Experience","text":"Microsoft <ul> <li>Building a multi-agent CVE patching system using SWE-agent</li> <li>Stable-Maintainer of Azure Linux</li> <li>Build &amp; Release systems design-to-development for Azure Linux and customer support</li> <li>LISA</li> </ul>  Flipkart <ul> <li>Life-cycle management for all Baremetal Operating Systems</li> <li>Minute level hardware usage metric collection from more than 150,000 systems</li> <li>Litmus - A data-center simulation for verifying baremetals health</li> <li>VM life-cycle orchestrator development</li> </ul>  HPE <ul> <li>Owner of Service-OS for ARM based switches</li> <li>Board bring-up, U-boot and Linux port to ARM platforms</li> </ul>  Sony <ul> <li>Support userspace and kernel network stack for Linux based SONY devices</li> </ul>"},{"location":"#patents","title":"Patents","text":"<p> Highly available DHCP service by running DHCP servers on a blockchain network Multiple-site private network secured by IPsec using blockchain network for key exchange System and method of optimizing vm disk data transfer time for cold migration by preloading page-cache  Ind. patent application - 202141004517: Common IP based network communication between virtual machine and host</p>"},{"location":"#0xba1a","title":"0xba1a","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"blog/tags/#tag:arm","title":"ARM","text":"<ul> <li>            Anatomy of Linux system call in ARM64          </li> </ul>"},{"location":"blog/tags/#tag:armv8","title":"ARMv8","text":"<ul> <li>            Anatomy of Linux system call in ARM64          </li> <li>            kexec - A travel to the purgatory          </li> </ul>"},{"location":"blog/tags/#tag:azure","title":"Azure","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            Fine-Tuning LLMs to Predict NIFTY50 Prices          </li> </ul>"},{"location":"blog/tags/#tag:c","title":"C","text":"<ul> <li>            Variadic functions with unknown argument count          </li> </ul>"},{"location":"blog/tags/#tag:coding-agent","title":"Coding Agent","text":"<ul> <li>            GPT-5 For Programmers          </li> </ul>"},{"location":"blog/tags/#tag:fine-tuning","title":"Fine-Tuning","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            Fine-Tuning LLMs to Predict NIFTY50 Prices          </li> </ul>"},{"location":"blog/tags/#tag:gpt","title":"GPT","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            GPT-5 For Programmers          </li> </ul>"},{"location":"blog/tags/#tag:gpt-5","title":"GPT-5","text":"<ul> <li>            GPT-5 For Programmers          </li> </ul>"},{"location":"blog/tags/#tag:llm","title":"LLM","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            Fine-Tuning LLMs to Predict NIFTY50 Prices          </li> <li>            GPT-5 For Programmers          </li> </ul>"},{"location":"blog/tags/#tag:linux","title":"Linux","text":"<ul> <li>            Anatomy of Linux system call in ARM64          </li> </ul>"},{"location":"blog/tags/#tag:nifty50","title":"NIFTY50","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            Fine-Tuning LLMs to Predict NIFTY50 Prices          </li> </ul>"},{"location":"blog/tags/#tag:openai","title":"OpenAI","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            GPT-5 For Programmers          </li> </ul>"},{"location":"blog/tags/#tag:raspberrypi","title":"RaspberryPi","text":"<ul> <li>            KGDB/KDB over serial with Raspberry Pi          </li> <li>            KGDBoE on RaspberryPi - building out of the kernel tree module with yocto          </li> <li>            Raspberry Pi dishes from Yocto cuisine          </li> </ul>"},{"location":"blog/tags/#tag:stock-market","title":"Stock Market","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            Fine-Tuning LLMs to Predict NIFTY50 Prices          </li> </ul>"},{"location":"blog/tags/#tag:time-series","title":"Time Series","text":"<ul> <li>            Fine-Tuning GPT model to Predict NIFTY50 Prices          </li> <li>            Fine-Tuning LLMs to Predict NIFTY50 Prices          </li> </ul>"},{"location":"blog/tags/#tag:about","title":"about","text":"<ul> <li>            About me          </li> </ul>"},{"location":"blog/tags/#tag:about-me","title":"about me","text":"<ul> <li>            About me          </li> </ul>"},{"location":"blog/tags/#tag:bitbucket","title":"bitbucket","text":"<ul> <li>            Auto deploy from Bitbucket or any other git repository          </li> </ul>"},{"location":"blog/tags/#tag:build","title":"build","text":"<ul> <li>            KGDBoE on RaspberryPi - building out of the kernel tree module with yocto          </li> <li>            Raspberry Pi dishes from Yocto cuisine          </li> </ul>"},{"location":"blog/tags/#tag:c","title":"c","text":"<ul> <li>            The Volatile keyword          </li> </ul>"},{"location":"blog/tags/#tag:chatbot","title":"chatbot","text":"<ul> <li>            Github from Facebook          </li> </ul>"},{"location":"blog/tags/#tag:clone","title":"clone","text":"<ul> <li>            The Linux COW          </li> </ul>"},{"location":"blog/tags/#tag:compiler","title":"compiler","text":"<ul> <li>            The Volatile keyword          </li> </ul>"},{"location":"blog/tags/#tag:cow","title":"cow","text":"<ul> <li>            The Linux COW          </li> </ul>"},{"location":"blog/tags/#tag:crazy-debugging","title":"crazy debugging","text":"<ul> <li>            kexec - A travel to the purgatory          </li> </ul>"},{"location":"blog/tags/#tag:debian","title":"debian","text":"<ul> <li>            Quick kernel upgrade with kexec          </li> </ul>"},{"location":"blog/tags/#tag:debug","title":"debug","text":"<ul> <li>            Debugging application with cross-GDB in Yocto environment          </li> <li>            KGDB/KDB over serial with Raspberry Pi          </li> </ul>"},{"location":"blog/tags/#tag:devpost","title":"devpost","text":"<ul> <li>            Github from Facebook          </li> </ul>"},{"location":"blog/tags/#tag:digitalocean","title":"digitalocean","text":"<ul> <li>            Add swap space to Ubuntu          </li> <li>            VNC setup in Digitalocean          </li> </ul>"},{"location":"blog/tags/#tag:facebook","title":"facebook","text":"<ul> <li>            Github from Facebook          </li> </ul>"},{"location":"blog/tags/#tag:fork","title":"fork","text":"<ul> <li>            The Linux COW          </li> </ul>"},{"location":"blog/tags/#tag:gcc","title":"gcc","text":"<ul> <li>            The Volatile keyword          </li> </ul>"},{"location":"blog/tags/#tag:gdb","title":"gdb","text":"<ul> <li>            Anatomy of Linux system call in ARM64          </li> <li>            Debugging application with cross-GDB in Yocto environment          </li> </ul>"},{"location":"blog/tags/#tag:getpid","title":"getpid","text":"<ul> <li>            The Linux COW          </li> </ul>"},{"location":"blog/tags/#tag:git","title":"git","text":"<ul> <li>            Auto deploy from Bitbucket or any other git repository          </li> </ul>"},{"location":"blog/tags/#tag:github","title":"github","text":"<ul> <li>            Github from Facebook          </li> </ul>"},{"location":"blog/tags/#tag:hack","title":"hack","text":"<ul> <li>            Copy paste in tmux session inside ssh          </li> <li>            Windows-10 port forwarding          </li> </ul>"},{"location":"blog/tags/#tag:kernel","title":"kernel","text":"<ul> <li>            64-bit Mainline kernel on Raspberry Pi 3          </li> <li>            Custom build kernel for Raspberry Pi          </li> <li>            Custom perf with custom kernel          </li> <li>            KGDB/KDB over serial with Raspberry Pi          </li> <li>            Quick kernel upgrade with kexec          </li> <li>            kexec - A travel to the purgatory          </li> <li>            perf setup          </li> </ul>"},{"location":"blog/tags/#tag:kernel-module","title":"kernel module","text":"<ul> <li>            KGDBoE on RaspberryPi - building out of the kernel tree module with yocto          </li> </ul>"},{"location":"blog/tags/#tag:kexec","title":"kexec","text":"<ul> <li>            Quick kernel upgrade with kexec          </li> </ul>"},{"location":"blog/tags/#tag:linux","title":"linux","text":"<ul> <li>            64-bit Mainline kernel on Raspberry Pi 3          </li> <li>            Copy paste in tmux session inside ssh          </li> <li>            Custom build kernel for Raspberry Pi          </li> <li>            Custom perf with custom kernel          </li> <li>            Quick kernel upgrade with kexec          </li> <li>            kexec - A travel to the purgatory          </li> <li>            perf kvm to profile vm_exit          </li> <li>            perf setup          </li> </ul>"},{"location":"blog/tags/#tag:memory-management","title":"memory management","text":"<ul> <li>            Add swap space to Ubuntu          </li> <li>            Virtual memory to Physical memory          </li> </ul>"},{"location":"blog/tags/#tag:memory_management","title":"memory_management","text":"<ul> <li>            The Linux COW          </li> </ul>"},{"location":"blog/tags/#tag:nodejs","title":"nodejs","text":"<ul> <li>            Auto deploy from Bitbucket or any other git repository          </li> <li>            Github from Facebook          </li> </ul>"},{"location":"blog/tags/#tag:perf","title":"perf","text":"<ul> <li>            Custom perf with custom kernel          </li> <li>            perf kvm to profile vm_exit          </li> <li>            perf setup          </li> </ul>"},{"location":"blog/tags/#tag:portproxy","title":"portproxy","text":"<ul> <li>            Windows-10 port forwarding          </li> </ul>"},{"location":"blog/tags/#tag:proc","title":"proc","text":"<ul> <li>            Virtual memory to Physical memory          </li> </ul>"},{"location":"blog/tags/#tag:programming","title":"programming","text":"<ul> <li>            Variadic functions with unknown argument count          </li> </ul>"},{"location":"blog/tags/#tag:raspberrypi","title":"raspberrypi","text":"<ul> <li>            64-bit Mainline kernel on Raspberry Pi 3          </li> <li>            Custom build kernel for Raspberry Pi          </li> </ul>"},{"location":"blog/tags/#tag:security","title":"security","text":"<ul> <li>            Quick kernel upgrade with kexec          </li> </ul>"},{"location":"blog/tags/#tag:setup","title":"setup","text":"<ul> <li>            Configure Vim          </li> <li>            VNC setup in Digitalocean          </li> <li>            perf setup          </li> </ul>"},{"location":"blog/tags/#tag:stack","title":"stack","text":"<ul> <li>            The Linux COW          </li> </ul>"},{"location":"blog/tags/#tag:stretch","title":"stretch","text":"<ul> <li>            Quick kernel upgrade with kexec          </li> </ul>"},{"location":"blog/tags/#tag:swap","title":"swap","text":"<ul> <li>            Add swap space to Ubuntu          </li> </ul>"},{"location":"blog/tags/#tag:system-call","title":"system call","text":"<ul> <li>            Anatomy of Linux system call in ARM64          </li> </ul>"},{"location":"blog/tags/#tag:tmux","title":"tmux","text":"<ul> <li>            Copy paste in tmux session inside ssh          </li> </ul>"},{"location":"blog/tags/#tag:ubuntu","title":"ubuntu","text":"<ul> <li>            VNC setup in Digitalocean          </li> <li>            perf setup          </li> </ul>"},{"location":"blog/tags/#tag:upgrade","title":"upgrade","text":"<ul> <li>            Quick kernel upgrade with kexec          </li> </ul>"},{"location":"blog/tags/#tag:vim","title":"vim","text":"<ul> <li>            Configure Vim          </li> </ul>"},{"location":"blog/tags/#tag:virtual-memory","title":"virtual memory","text":"<ul> <li>            Virtual memory to Physical memory          </li> </ul>"},{"location":"blog/tags/#tag:virtual_memory","title":"virtual_memory","text":"<ul> <li>            The Linux COW          </li> </ul>"},{"location":"blog/tags/#tag:vnc","title":"vnc","text":"<ul> <li>            VNC setup in Digitalocean          </li> </ul>"},{"location":"blog/tags/#tag:windows","title":"windows","text":"<ul> <li>            Windows-10 port forwarding          </li> </ul>"},{"location":"blog/tags/#tag:witai","title":"wit.ai","text":"<ul> <li>            Github from Facebook          </li> </ul>"},{"location":"blog/tags/#tag:yocto","title":"yocto","text":"<ul> <li>            Debugging application with cross-GDB in Yocto environment          </li> <li>            KGDB/KDB over serial with Raspberry Pi          </li> <li>            KGDBoE on RaspberryPi - building out of the kernel tree module with yocto          </li> <li>            Raspberry Pi dishes from Yocto cuisine          </li> </ul>"},{"location":"blog/2018/03/02/virtual-memory-to-physical-memory/","title":"Virtual memory to Physical memory","text":"<p>We all know that processes running in Linux acts only in virtual address space. So whenever a process wants to access a data (okay datum) it requests CPU for a virtual address. The CPU intern converts it into physical address and fetches the data. It will be nice to have a program that converts virtual address to physical address, won't it?</p> <p>Linux from 2.5.26 provides a <code>proc</code> interface, <code>pagemap</code> that contains information what we want. Each process has its <code>pagemap</code> at <code>/proc/p_id/pagemap</code>. According to the Documentation it is a binary file contains a sequence of 64-bit words. Each word contains information regarding one virtual page for full virtual address space. Among them bits 0-54 (55-bits) represents the address of the physical frame number (<code>PFN</code>). I think that's all we need. Adding the <code>offset</code> of a variable from virtual page address to the <code>PFN</code> will give us the physical memory address.</p> <p>WARNING: Don't try to read the <code>pagemap</code> file directly. <code>cat /proc/self/pagemap</code> or <code>vim /proc/p_id/pagemap</code> is not going to return anytime soon.</p> <p>We'll write a small C program and the let's try to get physical address of a variable used in that C program. As the <code>PFN</code> data will be present only if the data is not moved to swap, lets use <code>mlock()</code> to lock the memory in physical memory.</p> <p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;sys/mman.h&gt;   /* for mlock() */\n#include &lt;stdlib.h&gt;     /* for malloc() */\n#include &lt;string.h&gt;     /* for memset() */\n\n/* for getpid() */\n#include &lt;sys/types.h&gt;\n#include &lt;unistd.h&gt;\n\n#define MEM_LENGTH 1024\n\nint main()\n{\n    /* Allocate 1024 bytes in heap */\n    char *ptr = NULL;\n    ptr = malloc(MEM_LENGTH);\n    if (!ptr) {\n        perror(\"malloc fails. \");\n        return -1;\n    }\n\n    /* obtain physical memory */\n    memset(ptr, 1, MEM_LENGTH);\n\n    /* lock the allocated memory in RAM */\n    mlock(ptr, MEM_LENGTH);\n\n    /* print the pid and vaddr. Thus we can work on him */\n    printf(\"my pid: %d\\n\\n\", getpid());\n    printf(\"virtual address to work: 0x%lx\\n\", (unsigned long)ptr);\n\n    /* make the program to wait for user input */\n    scanf(\"%c\", &amp;ptr[16]);\n\n    return 0;\n}\n</code></pre> </p> <p>Run the <code>specimen.c</code> program, get its <code>p_id</code> and start the dissection.</p> <p><pre><code>$ gcc specimen.c -o specimen\n$ ./specimen\nmy pid: 11953\n\nvirtual address to work: 0x55cd75821260\n</code></pre> </p> <p>In a 64-bit machine, virtual address-space is from 0x00 and to 2^64 - 1. First we have to calculate the page offset for the given virtual address [find on which virtual memory page, the address resides]. And multiply that with 8 as each virtual page table has 8-byte information word in the <code>pagemap</code> file.</p> <p><pre><code>#define PAGEMAP_LENGTH 8\npage_size = getpagesize();\noffset = (vaddr page_size) * PAGEMAP_LENGTH;\n</code></pre> </p> <p>Open the <code>pagemap</code> file and seek to that offset location.</p> <p><pre><code>pagemap = fopen(filename, \"rb\");\nfseek(pagemap, (unsigned long)offset, SEEK_SET)\n</code></pre> </p> <p>Now cursor is on the first byte of 64-bit word containing the information we need. According to the Documentation bits 0-54 represents the physical page frame number (<code>PFN</code>). So read 7-bytes and discard most significant bit.</p> <p><pre><code>fread(&amp;paddr, 1, (PAGEMAP_LENGTH-1), pagemap)\npaddr = paddr &amp; 0x7fffffffffffff;\n</code></pre> </p> <p>This is the <code>PFN</code>. Add offset of the virtual address from its virtual page base address to the page shifted <code>PFN</code> to get the physical address of the memory.</p> <p><pre><code>offset = vaddr % page_size;\n/* PAGE_SIZE = 1U &lt;&lt; PAGE_SHIFT */\nwhile (!((1UL &lt;&lt; ++page_shift) &amp; page_size));\npaddr = (unsigned long)((unsigned long)paddr &lt;&lt; page_shift) + offset;\n</code></pre> </p> <p>Here is the complete program.</p> <p><pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;errno.h&gt;\n\n#define PAGE_SHIFT 12\n#define PAGEMAP_LENGTH 8\n\nint main(int argc, char **argv)\n{\n    unsigned long vaddr, pid, paddr = 0, offset;\n    char *endptr;\n    FILE *pagemap;\n    char filename[1024] = {0};\n    int ret = -1;\n    int page_size, page_shift = -1;\n\n    page_size = getpagesize();\n    pid = strtol(argv[1], &amp;endptr, 10);\n    vaddr = strtol(argv[2], &amp;endptr, 16);\n    printf(\"getting page number of virtual address %lu of process %ld\\n\",vaddr, pid);\n\n    sprintf(filename, \"/proc/%ld/pagemap\", pid);\n\n    printf(\"opening pagemap %s\\n\", filename);\n    pagemap = fopen(filename, \"rb\");\n    if (!pagemap) {\n        perror(\"can't open file. \");\n        goto err;\n    }\n\n    offset = (vaddr / page_size) * PAGEMAP_LENGTH;\n    printf(\"moving to %ld\\n\", offset);\n    if (fseek(pagemap, (unsigned long)offset, SEEK_SET) != 0) {\n        perror(\"fseek failed. \");\n        goto err;\n    }\n\n    if (fread(&amp;paddr, 1, (PAGEMAP_LENGTH-1), pagemap) &lt; (PAGEMAP_LENGTH-1)) {\n        perror(\"fread fails. \");\n        goto err;\n    }\n    paddr = paddr &amp; 0x7fffffffffffff;\n    printf(\"physical frame address is 0x%lx\\n\", paddr);\n\n    offset = vaddr % page_size;\n\n    /* PAGE_SIZE = 1U &lt;&lt; PAGE_SHIFT */\n    while (!((1UL &lt;&lt; ++page_shift) &amp; page_size));\n\n    paddr = (unsigned long)((unsigned long)paddr &lt;&lt; page_shift) + offset;\n    printf(\"physical address is 0x%lx\\n\", paddr);\n\n    ret = 0;\nerr:\n    fclose(pagemap);\n    return ret;\n}\n</code></pre> </p> <p>And the output</p> <p><pre><code>$ sudo ./a.out 11953 0x55cd75821260\ngetting page number of virtual address 94340928115296 of process 11953\nopening pagemap /proc/11953/pagemap\nmoving to 184259625224\nphysical frame address is 0x20508\nphysical address is 0x20508260\n</code></pre> </p>","tags":["proc","memory management","virtual memory"]},{"location":"blog/2018/03/02/virtual-memory-to-physical-memory/#references","title":"References","text":"<ol> <li>https://www.kernel.org/doc/Documentation/vm/pagemap.txt</li> <li>https://elixir.bootlin.com/linux/latest/source/arch/x86/include/asm/page_types.h</li> <li>man pages</li> </ol>","tags":["proc","memory management","virtual memory"]},{"location":"blog/2018/02/28/add-swap-space-to-ubuntu/","title":"Add swap space to Ubuntu","text":"WARNING: Adding a swap space in your Digitalocean droplet or any other cloud VM is not recommended! Because continuous reads and write is will reduce the life of underlying flash.  <p>Truffle compilation was crashing with memory-overrun issues in my 512 MB basic droplet. So I added 2 GB of swap space with following commands.</p> <p>Make sure you have no swap config previously. There should be no output. <pre><code>$ sudo swapon --show\n$\n</code></pre></p> <p>The simple way of doing this is by creating a swap file. Lets look at all the partitions to see how much free space in each of them. <pre><code>$ df -h\nkaba@ubuntu-512mb-blr1-01:~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\nudev            238M     0  238M   0% /dev\ntmpfs            49M  5.9M   43M  13% /run\n/dev/vda1        20G  5.2G   15G  27% /\ntmpfs           245M   24M  221M  10% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\ntmpfs           245M     0  245M   0% /sys/fs/cgroup\n/dev/vda15      105M  3.4M  102M   4% /boot/efi\ntmpfs            49M     0   49M   0% /run/user/0\ntmpfs            49M  8.0K   49M   1% /run/user/1000\n$\n</code></pre>  * Other than the ones with <code>/dev/</code> are virtual filesystem and are not usable.  * I'm gonna create my swap file in <code>/dev/vda1</code>.</p> <p>Use <code>fallocate</code> utility to create a preallocated file in root directory. <pre><code>$ sudo fallocate -l 2G /swapfile\n</code></pre>  * As in previous command output, <code>dev/vda1</code> is mounted on <code>/</code>  * 2G specifies 2 GB of space. Use whatever size you want in that place.</p> <p>Make the is file accessible to <code>root</code> user alone <pre><code>sudo chmod 600 /swapfile\n</code></pre></p> <p>Mark the file as swap space <pre><code>$ sudo mkswap /swapfile\n</code></pre></p> <p>Enable the swap file, so your system will start using it <pre><code>$ sudo swapon /swapfile\n</code></pre></p> <p>Done!</p> <p>Now <code>swapon</code> command shouldn't return an empty result <pre><code>$ sudo swapon --show\nNAME      TYPE SIZE   USED PRIO\n/swapfile file   2G 771.6M   -1\n</code></pre></p> <p> Note: The swap file will be used only until the system reboots. You have to use the `swapon` command every time your system reboots. Or read further.  </p>","tags":["swap","digitalocean","memory management"]},{"location":"blog/2018/02/28/add-swap-space-to-ubuntu/#make-swap-settings-permanent","title":"Make swap settings permanent","text":"<p>You need to edit <code>/etc/fstab</code> file. So better take a backup of it. <pre><code>$ sudo mv /etc/fstab /etc/fstab.orig\n</code></pre></p> <p>Update the file with swap information <pre><code>$ echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\n</code></pre></p>","tags":["swap","digitalocean","memory management"]},{"location":"blog/2018/02/28/about-me/","title":"About me","text":"<p>Balakumaran Kannan works as a System software engineer mail me at kumaran.4353@gmail.com</p>","tags":["about","about me"]},{"location":"blog/2018/03/01/auto-deploy-from-bitbucket-or-any-other-git-repository/","title":"Auto deploy from Bitbucket or any other git repository","text":"<p>Update aptitude first <pre><code>$ sudo apt-get update\n</code></pre></p> <p>Install expect <pre><code>$ sudo apt-get install expect\n</code></pre></p> <p>Create a directory named <code>trigger</code> in your home directory <pre><code>$ cd\n$ mkdir trigger\n$ cd trigger\n</code></pre></p> <p>Create following files inside the <code>trigger</code> directory. trigger.js <pre><code>$ cat trigger.js\nvar server_port = &lt;new_port_for_trigger&gt;;\n\nvar sys = require('sys');\nvar exec = require('child_process').exec;\nvar child;\n\nvar http = require('http');\nvar express = require('express');\nvar app = express();\nvar server_get = require('http').Server(app);\n\napp.post('/update', function(req, res) {\n    child = exec(\"./update_repo.sh\", function(error, stdout, stderr) {\n                console.log('stdout: ' + stdout);\n                console.log('stderr: ' + stderr);\n                if (error !== null) {\n                    console.log('exec error: ' + error);\n                }\n    });\n\n    res.send(\"SUCCESS\");\n});\n\napp.listen(server_port, function() {\n    console.log('Example app listening on port ' + server_port);\n}\n</code></pre></p> <p>package.json <pre><code>$ cat package.json\n{\n    \"name\": \"Trigger\",\n    \"version\": \"0.0.1\",\n    \"scripts\": {\n        \"start\": \"node server\"\n    },\n    \"dependencies\": {\n        \"express\": \"^4.14.0\",\n        \"http\": \"0.0.0\",\n        \"https\": \"^1.0.0\"\n    }\n}\n</code></pre></p> <p>update_repo.sh <pre><code>$ cat update_repo.sh\n#!/bin/bash\ncd &lt;your_git_repo&gt;\n~/trigger/git_pull_helper.sh\npm2 restart server\n</code></pre></p> <p>git_pull_helper.sh <pre><code>$ cat git_pull_helper.sh\n#!/usr/bin/expect -f\nspawn git pull\nexpect \"ass\"\nsend \"&lt;your_ssh_key_pass_phrase&gt;\\r\"\ninteract\n</code></pre></p> <p>Update permission for update_repo.sh and git_pull_helper.sh <pre><code>$ chmod +x update_repo.sh\n$ chmod +x git_pull_helper.sh\n</code></pre></p> <p>Start your trigger NodeJS server <pre><code>$ pm2 start trigger.js\n</code></pre></p> <ul> <li>Configure you <code>nginx</code> if you are using one</li> <li>Don't forget to enable firewall to allow the new port</li> </ul>","tags":["bitbucket","nodejs","git"]},{"location":"blog/2018/03/01/auto-deploy-from-bitbucket-or-any-other-git-repository/#update-in-bitbucket","title":"Update in bitbucket","text":"<ul> <li>Got Settings</li> <li>Select Webhooks in Workflow section</li> <li>Click Add webhook button</li> <li>Give name</li> <li>Enter your url as <code>*&lt;your_domain_name&gt;.&lt;ext&gt;:&lt;your_port_for_trigger&gt;/update*</code></li> <li>Press save button</li> </ul> <p>Or follow the steps provided by your <code>git</code> server.</p>","tags":["bitbucket","nodejs","git"]},{"location":"blog/2017/09/01/configure-vim/","title":"Configure Vim","text":"<p>The above picture is from my favorite serial <code>Mr.Robot</code>. Elliot Alderson is writing a script to hack Evil Corp.</p> <p>I have three development environments  * One Cent-OS machine for C programming in office  * Ubuntu-16.04 Digitalocean and AWS virtual machines to host some servers and Ethereum developemnt  * And one Raspberry PI 3 at home as an SSH client</p> <p>As the Cent-OS machine has older version of vim, Plugins that require latest version of vim and plugins that are only needed for Web/Ethereum development are kept under an if condition. So modify it based on your need.</p> <p>Here I didn't give any explanation for any config or plugin. Copy pasting them in Google will give you the information.</p> <p>Backup you <code>vimrc</code> file <pre><code>$ mv ~/.vimrc ~/.vimrc.orig\n</code></pre></p> <p>Clone Vundle into your <code>~/.vim</code> directory <pre><code>$ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim\n</code></pre></p> <p>Copy this <code>vimrc</code> file into your home directory.</p>  I use `h, j, k, l` for navigation. Modify the `vimrc` file if necessary.  <p>Install plugins using <code>PluginInstall</code> command. Open vim with no file. Ignore if any error shows up. Press <code>Esc</code> key then <code>:</code> and type the command <code>PluginInstall</code> followed by <code>Enter</code> key. A new window will open and install all plugins. <pre><code>$ vim\n\n:PluginInstall\n</code></pre></p> <p>Don't forget to install 'YouCompleteMe' in your machine. <pre><code>$ cd ~/.vim/bundle/YouCompleteMe/\n$ ./install.py --js-completer --clang-completer --system-libclang\n</code></pre> Installing auto-complete feature for JavaScript and C Language. See YouCompleteMe GitHub page for more details.</p>","tags":["setup","vim"]},{"location":"blog/2018/03/01/vnc-setup-in-digitalocean/","title":"VNC setup in Digitalocean","text":"<p>Install <code>xfce</code> desktop environment <pre><code>$ sudo apt-get install xfce4 xfce4-goodies\n</code></pre></p> <p>Install <code>TightVNC</code> for VNC server <pre><code>$ sudo apt-get install tightvncserver\n</code></pre></p> <p>Initialize VNC server + run VNC server once + enter and verify password + ignore view-only password <pre><code>$ vncserver\nPassword:\nVerify:\nView-only password:\n</code></pre></p> <p>To configure VNC, first kill the running VNC server <pre><code>$ vncserver -kill :1\nKilling Xtightvnc process ID 13456\n</code></pre></p> <p>Backup <code>xstartup</code> configuration file <pre><code>$ mv ~/.vnc/xstartup ~/.vnc/xstartup.orig\n</code></pre></p> <p>Edit <code>xstartup</code> file <pre><code>$ vim ~/.vnc/xstartup\n</code></pre></p> <p>And add following content <pre><code>*~/.vnc/xstartup*\n#!/bin/bash\nxrdb $HOME/.Xresources\nstartxfce4 &amp;\n</code></pre> + Look at <code>~/.Xresources</code> file for VNC's GUI framework information + Start XFCE whenever VNC server is started</p> <p>Provide executable permission for <code>~/.vnc/xstartup</code> file <pre><code>$ sudo chmod +x ~/.vnc/xstartup\n</code></pre></p> <p>Start VNC server <pre><code>$ vncserver -geometry 1280x800\n</code></pre></p>","tags":["vnc","digitalocean","ubuntu","setup"]},{"location":"blog/2018/03/01/github-from-facebook/","title":"Github from Facebook","text":"<p>There are multiple J.A.R.V.I.S projects available in GitHub. All are based on some chat frameworks like api.ai, Alexa, etc. This is yet another rather vaguely intelligent system that I developed for Facebook developer circle challenge hosted at Devpost.</p> <p>I have developed it just keeping J.A.R.V.I.S in mind. First of all J.A.R.V.I.S is not a simple t-shirt throwing bot like the one Zuckerberg developed. Tony Stark actively uses him during his project development. I mean he is not just a personal assistant but an intelligent co-developer. So I developed one to help you doing your GitHub related activities. You can simply create repos, open close issues, comment on others' issues just from a Facebook chat window. It will save your time and effort from navigating to multiple windows and clicking a dozens of buttons. With voice interface enabled for Facebook chat, he will be your personal J.A.R.V.I.S.</p> <p>Here is a quick demo.</p> <p></p> <p>Code is available at github.com/kaba-official/MO.</p>","tags":["github","wit.ai","facebook","chatbot","devpost","nodejs"]},{"location":"blog/2018/03/05/the-linux-cow/","title":"The Linux COW","text":"<p>In our last post we have seen how to get the physical memory address using a virtual memory address. In this post lets try to check Copy on Write(COW from here) implemented by Linux. For those who try to recollect what is COW, on <code>fork()</code>, Linux will not immediately replicate the address space of parent process. As the child process is most-likely to load a new binary, copying the current process's address space will be of no use in many cases. So unless a write occurs (either by the parent process or by the child process), the physical address space mapped to both processes' virtual address space will be same. Please go through Wikipedia if you still have no clue.</p> <p>So lets write a C program that forks itself and check whether both are sharing the physical address space. We are going to reuse the <code>get_paddr.c</code> function we wrote in our previous post to get the physical memory address. <pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt; /* fork() */\n\nint main()\n{\n    int a;\n\n    fork();\n\n    printf(\"my pid is %d, and the address to work is 0x%lx\\n\", getpid(), (unsigned long)&amp;a);\n    scanf(\"%d\\n\", &amp;a);\n\n    return 0;\n}\n</code></pre></p> <p>Execute this program in one console and while it is waiting for the user input, go to the next console and get the physical address of variable <code>a</code> using our <code>get_paddr</code> binary. <pre><code>$ gcc specimen.c -o specimen\n$ ./specimen\nmy pid is 5912, and the address to work is 0x7ffd055923e4\nmy pid is 5913, and the address to work is 0x7ffd055923e4\n</code></pre> <pre><code>$ sudo ./get_paddr 5912 0x7ffd055923e4\ngetting page number of virtual address 140724693181412 of process 5912\nopening pagemap /proc/5912/pagemap\nmoving to 274852916368\nphysical frame address is 0x509c9\nphysical address is 0x509c93e4\n$\n$ sudo ./get_paddr 5913 0x7ffd055923e4\ngetting page number of virtual address 140724693181412 of process 5913\nopening pagemap /proc/5913/pagemap\nmoving to 274852916368\nphysical frame address is 0x64d2a\nphysical address is 0x64d2a3e4\n$\n</code></pre></p> <p>OOPS! The physical address is not same! How? Why? What's wrong? As per my beloved Robert Love's Linux System Programming book, the physical memory copy will occur only when a write occurs.</p> <p>{% blockquote Robert Love , Linux System Programming %} The MMU can intercept the write operation and raise an exception; the kernel, in response, will transparently create a new copy of the page for the writing process, and allow the write to continue against the new page. We call this approach *copy-on-write(COW). Effectively, processes are allowed read access to shared data, which saves space. But when a process wants to write to a shared page, it receives a unique copy of that page on fly, thereby allowing the kernel to act as if the process always had its own private copy. As copy-on write occurs on a page-by-page basis, with the technique a huge file may be efficiently shared among many processes, and the individual processes will receive unique physical pages only for those pages to which thy themselves write.</p> <p>It is so clear that in our program <code>specimen.c</code> we never write to the only variable <code>a</code> unless the <code>scanf</code> completes. But we tested the COW before input-ing to the <code>scanf</code>. So by the time there would be no write and as per the design of COW physical memory space should be shared across both parent and child. Even though we have written the program carefully, sometimes the libraries, compiler and even the OS will act intelligently (stupid! They themselves says it) and fills us with surprise. So lets run the handy <code>strace</code> on our <code>specimen</code> binary to see what it actually does. <pre><code>$ strace ./specimen\nexecve(\"./specimen\", [\"./specimen\"], [/* 47 vars */]) = 0\nbrk(NULL)                               = 0x55de17b57000\naccess(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\naccess(\"/etc/ld.so.preload\", R_OK)      = -1 ENOENT (No such file or directory)\nopenat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3\nfstat(3, {st_mode=S_IFREG|0644, st_size=94696, ...}) = 0\nmmap(NULL, 94696, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7fdbad5b8000\nclose(3)                                = 0\naccess(\"/etc/ld.so.nohwcap\", F_OK)      = -1 ENOENT (No such file or directory)\nopenat(AT_FDCWD, \"/lib/x86_64-linux-gnu/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3\nread(3, \"\\177ELF\\2\\1\\1\\3\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0&gt;\\0\\1\\0\\0\\0\\340\\22\\2\\0\\0\\0\\0\\0\"..., 832) = 832\nfstat(3, {st_mode=S_IFREG|0755, st_size=1960656, ...}) = 0\nmmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fdbad5b6000\nmmap(NULL, 4061792, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7fdbacfc9000\nmprotect(0x7fdbad19f000, 2097152, PROT_NONE) = 0\nmmap(0x7fdbad39f000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE,\n        3, 0x1d6000) = 0x7fdbad39f000\nmmap(0x7fdbad3a5000, 14944, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS,\n        -1, 0) = 0x7fdbad3a5000\nclose(3)                                = 0\narch_prctl(ARCH_SET_FS, 0x7fdbad5b7500) = 0\nmprotect(0x7fdbad39f000, 16384, PROT_READ) = 0\nmprotect(0x55de15e94000, 4096, PROT_READ) = 0\nmprotect(0x7fdbad5d0000, 4096, PROT_READ) = 0\nmunmap(0x7fdbad5b8000, 94696)           = 0\nclone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD,\n        child_tidptr=0x7fdbad5b77d0) = 5932\ngetpid()                                = 5931\nfstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 2), ...}) = 0\nmy pid is 5932, and the address to work is 0x7fff9823b814\nbrk(NULL)                               = 0x55de17b57000\nbrk(0x55de17b78000)                     = 0x55de17b78000\nwrite(1, \"my pid is 5931, and the address \"..., 58my pid is 5931, and the address\n        to work is 0x7fff9823b814\n) = 58\nfstat(0, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 2), ...}) = 0\nread(0,\n</code></pre></p> <p>As the program is waiting on <code>scanf</code>, it is waiting for user input in <code>read</code> system-call from <code>fd 0</code>, <code>stdin</code>. But the surprise part here is the <code>clone</code> system call. If you don't remember, read the <code>specimen.c</code> program once again. We never used the <code>clone</code> system call. And where is the <code>fork</code>? It seems <code>glibc</code> does something in a so called intelligent way. Lets read the <code>man pages</code> once again for pointers. <pre><code>C library/kernel differences\n       Since  version  2.3.3, rather than invoking the kernel's fork() system call,\n       the glibc fork() wrapper that is provided as part of the NPTL threading\n       implementation invokes clone(2) with flags that provide the same effect as\n       the traditional system call.  (A call to fork() is equivalent to a call to\n       clone(2) specifying flags as just  SIGCHLD.) The glibc wrapper invokes any\n       fork handlers that have been established using pthread_atfork(3).\n</code></pre></p> <p>Doesn't look completely true. Though the <code>man page</code> says it calls <code>clone()</code> with flags as just <code>SIGCHLD</code>, <code>strace</code> uncovers the dirty truth - <code>flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD</code>. Nobody escapes from an armed system programmer.  Anyway this explains <code>clone</code> but no-<code>fork</code> surprise. But where comes the write that causes <code>COW</code>. See the last argument of <code>clone()</code> in <code>strace</code>, <code>child_tidptr=0x7fdbad5b77d0</code> some place holder stack variable passed to be filled. And what about the two flags other than <code>SIGCHLD</code>? Lets go to the <code>man pages</code> once again, but for <code>clone()</code> this time. <pre><code>    CLONE_CHILD_CLEARTID (since Linux 2.5.49)\n        Clear (zero) the child thread ID at the location ctid in child memory when\n        the child exits, and do a wakeup on the futex at that address.  The address\n        involved  may be changed by the set_tid_address(2) system call.  This is\n        used by threading libraries.\n\n    CLONE_CHILD_SETTID (since Linux 2.5.49)\n        Store the child thread ID at the location ctid in the child's memory.\n        The store operation completes before clone() returns control to user space.\n</code></pre></p> <p>Ahh! The culprit is <code>CLONE_CHILD_SETTID</code>. It makes the OS to write thread ID of the child into its memory region. This write triggers the <code>COW</code> and gets the child a fresh copy of physical memory. Sorry Linux, I doubted you.</p> <p>Okay. Lets modify our <code>specimen</code> with <code>clone()</code> and no <code>CLONE_CHILD_SETTID</code> flag. <pre><code>/* clone() */\n#define _GNU_SOURCE\n#include &lt;sched.h&gt;\n\n#include &lt;stdio.h&gt;\n\n/* getpid() */\n#include &lt;sys/types.h&gt;\n#include &lt;unistd.h&gt;\n\n#include &lt;signal.h&gt; /* SIGCHLD */\n#include &lt;stdlib.h&gt; /* malloc() */\n\nint run(void *arg)\n{\n    printf(\"my pid is %d and address to check is 0x%lx\\n\", getpid(), (unsigned long) arg);\n    scanf(\"%d\\n\", (int *)arg);\n\n    return 0;\n}\n\nint main()\n{\n    int a;\n    void *stack_ptr = malloc(1024*1024);\n    if (stack_ptr == NULL) {\n        printf(\"No virtual memory available\\n\");\n        return -1;\n    }\n    /*fork();*/\n\n    clone(run, (stack_ptr + 1024*1024), CLONE_CHILD_CLEARTID|SIGCHLD, &amp;a);\n    if (a &lt; 0)\n        perror(\"clone failed. \");\n\n    run(&amp;a);\n\n    return 0;\n}\n</code></pre></p> <p>Unlike <code>fork()</code>, <code>clone()</code> doesn't start the execution of child from the point where it returns.Instead it takes a function as an argument and runs the function in child process. Here we are passing <code>run()</code> function which prints the <code>pid</code> and <code>virtual address</code> of the variable <code>a</code>. After <code>clone()</code> the parent also calls the function <code>run()</code> so that we'll get the <code>pid</code> of parent process.</p> <p>Have you noted the second argument of <code>clone()</code>? It is the <code>stack</code> where child is going to execute. Again unlike <code>fork()</code>, <code>clone()</code> allows parent and child to share their resources like memory, signal handlers, open file descriptors, etc. As both cannot run in same <code>stack</code>, the parent process must allocate some space and give it to the child to use it as stack. <code>stack</code> grows downwards on all processors that run Linux, so the child-stack should point out to the topmost address of the memory region. That's why we are passing the end of <code>malloc</code>ed memory. Lets execute the program and see whether both process share the same physical memory space. <pre><code>$ ./specimen\nmy pid is 3129 and address to check is 0x7fff396fdd6c\nmy pid is 3130 and address to check is 0x7fff396fdd6c\n</code></pre> <pre><code>$ sudo ./get_paddr 3129 0x7fff396fdd6c\ngetting page number of virtual address 140734157020524 of process 3129\nopening pagemap /proc/3129/pagemap\nmoving to 274871400424\nphysical frame address is 0x5e35a\nphysical address is 0x5e35ad6c\n$\n$ sudo ./get_paddr 3130 0x7fff396fdd6c\ngetting page number of virtual address 140734157020524 of process 3130\nopening pagemap /proc/3130/pagemap\nmoving to 274871400424\nphysical frame address is 0x6a7cd\nphysical address is 0x6a7cdd6c\n$\n</code></pre></p> <p>Different again! Linux can't be wrong. We should make sure we didn't mess-up anything. Are we sure there will be no write in stack after <code>clone()</code> call?  * The child's execution starts at function <code>run()</code>  * No variables are written until <code>scanf()</code> returns. Okay we'll complete our examination before providing any input.  * But the functions? OOPS! After clone, parent calls <code>run()</code> and then <code>printf()</code> but child calls <code>printf()</code> directly. All these function calls will make write in stack which triggers COW.</p> <p>Lets come-up with a different C-program with no function calls after <code>clone()</code> in both parent and child.</p> <p><pre><code>/* clone() */\n#define _GNU_SOURCE\n#include &lt;sched.h&gt;\n\n#include &lt;stdio.h&gt;\n\n/* getpid() */\n#include &lt;sys/types.h&gt;\n#include &lt;unistd.h&gt;\n\n#include &lt;signal.h&gt; /* SIGCHLD */\n#include &lt;stdlib.h&gt; /* malloc() */\n\n#define STACK_LENGTH (1024*1024)\n\nint run(void *arg)\n{\n    while(*(int*)arg);\n\n    return 0;\n}\n\nint main()\n{\n    int a = 1;\n    void *stack_ptr = malloc(STACK_LENGTH);\n    if (stack_ptr == NULL) {\n        printf(\"No virtual memory available\\n\");\n        return -1;\n    }\n    /*fork();*/\n\n    printf(\"my pid is %d and address to check is 0x%lx\\n\", getpid(), (unsigned long) &amp;a);\n    clone(run, (stack_ptr + STACK_LENGTH), CLONE_CHILD_CLEARTID|SIGCHLD, &amp;a);\n    if (a &lt; 0)\n        perror(\"clone failed. \");\n\n    while (a);\n\n    return 0;\n}\n</code></pre> We are not printing anything after <code>clone()</code> to avoid stack overwrite. So we have to assume the child process's <code>pid</code> is parent <code>pid</code> + 1. This assumption is true most of the times. And we use infinite <code>while</code> loop to pause both processes. Loops use <code>jump</code> statements. So they will not cause any stack write. So this time we should see same physical address has been used by both processes. Lets see. <pre><code>$ ./specimen\nmy pid is 3436 and address to check is 0x7fffa920c1ec\n</code></pre> <pre><code>$ sudo ./get_paddr 3436 0x7fffa920c1ec\ngetting page number of virtual address 140736030884332 of process 3436\nopening pagemap /proc/3436/pagemap\nmoving to 274875060320\nphysical frame address is 0x67337\nphysical address is 0x673371ec\n$\n$ sudo ./get_paddr 3437 0x7fffa920c1ec\ngetting page number of virtual address 140736030884332 of process 3437\nopening pagemap /proc/3437/pagemap\nmoving to 274875060320\nphysical frame address is 0x67337\nphysical address is 0x673371ec\n</code></pre></p> <p>GREAT! At last we conquered. We saw the evidence of our beloved COW. Now I can sleep peacefully.</p>","tags":["memory_management","fork","clone","getpid","cow","stack","virtual_memory"]},{"location":"blog/2018/03/05/the-linux-cow/#for-pure-engineer","title":"For pure Engineer","text":"<p>Still I feel the urge to make a stack write in child process and see the physical address change. For those curious people out there who want to see the things break, lets run it one more time. Change the <code>run()</code> function as follows and execute <code>specimen.c</code>. <pre><code>int run(void *arg)\n{\n    *(int*)arg = 10; //stack write\n    while(*(int*)arg);\n\n    return 0;\n}\n</code></pre> <pre><code>$ ./specimen\nmy pid is 3549 and address to check is 0x7ffdd8e1e9ac\n</code></pre> <pre><code>$ sudo ./get_paddr 3549 0x7ffdd8e1e9ac\ngetting page number of virtual address 140728242137516 of process 3549\nopening pagemap /proc/3549/pagemap\nmoving to 274859847920\nphysical frame address is 0x634e6\nphysical address is 0x634e69ac\n$\n$ sudo ./get_paddr 3550 0x7ffdd8e1e9ac\ngetting page number of virtual address 140728242137516 of process 3550\nopening pagemap /proc/3550/pagemap\nmoving to 274859847920\nphysical frame address is 0x55976\nphysical address is 0x559769ac\n$\n</code></pre> Good Nightzzz...</p>","tags":["memory_management","fork","clone","getpid","cow","stack","virtual_memory"]},{"location":"blog/2018/04/25/raspberry-pi-dishes-from-yocto-cuisine/","title":"Raspberry Pi dishes from Yocto cuisine","text":"<p>Recently I got some board bring-up work where I come across Yocto project. To complete that project, I had to understand a little more than the basics of Yocto. So I had some hands on like Yocto Project Quick Start and Yocto Project Linux Kernel Development Manual. Seems Yocto is so powerful thus I thought of starting to use it for my Raspberry Pi hacking. As expected there are already people tried doing so and there are some good blogs about what they have accomplished. But to my surprise, there is already a meta layer for Raspberry Pi 3. It makes the work even simpler.</p> <p>I created a directory in Desktop as base for the development. Inside it I created <code>build</code>, <code>downloads</code>, <code>sstate</code>, <code>tmp</code> and <code>yocto</code> layers directories. <pre><code>$ ls\nbuild  downloads  sstate  sstate-cache  tmp  yocto\n</code></pre> With reference to the meta-raspberry quick start page, I have cloned poky, meta-openembedded and meta-raspberry inside the <code>yocto</code> directory. And checkout to specific branches. <pre><code>$ cd yocto\n$ git clone git://git.yoctoproject.org/poky.git\n$ cd poky\n$ git checkout origin/master\n$ $ git rev-parse HEAD\nda3625c52e1ab8985fba4fc3d133edf92142f182\n$\n$ cd ..\n$ git clone git://git.openembedded.org/meta-openembedded\n$ cd meta-openembedded\n$ $ git rev-parse HEAD\n271ce3576be25d8012af15961f827aa2235e6434\n$\n$ cd ..\n$ git clone git://git.yoctoproject.org/meta-raspberrypi\n$ cd meta-raspberry\n$ git checkout master\n$ git rev-parse HEAD\n693f36dded2f29fd77720f7b2e7c88ea76554466\n</code></pre> You can find all the available layers here</p> <p>Only the above branches and commits worked for me. With rocko branch had pypi.class error</p> <p>Sourced the <code>oe-init</code> script to setup build environment. And then configured the <code>conf/bblayers.conf</code> file to include <code>meta-layers</code> from meta-openembedded and meta-raspberry layers. The meta-raspberry quick start specifies <code>meta-oe</code>, <code>meta-multimedia</code>, <code>meta-networking</code> and <code>meta-python</code> as dependencies for <code>meta-raspberry</code>. So I'm including all of the into my <code>conf/bblayers.conf</code> file. <pre><code>$ source yocto/poky/oe-init-build-env build/rpi3/\n\n### Shell environment set up for builds. ###\n\nYou can now run 'bitbake &lt;target&gt;'\n\nCommon targets are:\n    core-image-minimal\n    core-image-sato\n    meta-toolchain\n    meta-ide-support\n\nYou can also run generated qemu images with a command like 'runqemu qemux86'\n$ pwd\n/home/kaba/Desktop/yocto/build/rpi3\n$ cat conf/bblayers.conf\n# POKY_BBLAYERS_CONF_VERSION is increased each time build/conf/bblayers.conf\n# changes incompatibly\nPOKY_BBLAYERS_CONF_VERSION = \"2\"\n\nBBPATH = \"${TOPDIR}\"\nBBFILES ?= \"\"\n\nBBLAYERS ?= \" \\\n  ${TOPDIR}/../../yocto/poky/meta \\\n  ${TOPDIR}/../../yocto/poky/meta-poky \\\n  ${TOPDIR}/../../yocto/poky/meta-yocto-bsp \\\n  ${TOPDIR}/../../yocto/meta-openembedded/meta-oe \\\n  ${TOPDIR}/../../yocto/meta-openembedded/meta-multimedia \\\n  ${TOPDIR}/../../yocto/meta-openembedded/meta-networking \\\n  ${TOPDIR}/../../yocto/meta-openembedded/meta-python \\\n  ${TOPDIR}/../../yocto/meta-raspberrypi \\\n  \"\n$\n</code></pre> Then made following changes in <code>conf/local.conf</code>.  * Set the <code>MACHINE</code> variable to point raspberry Pi 3 [raspberrypi3-64].  * Set <code>SSTATE_DIR</code>, <code>TMPDIR</code> and <code>DL_DIR</code> to point to <code>sstate</code>, <code>tmp</code> and <code>downloads</code> directories respectively.  * I usually run my Pi headless. So I enabled <code>ssh-server-openssh</code> EXTRA_IMAGE_FEATURE. Its up to your choice.  * Left the other default options as it was.</p> <p><pre><code>$ ls ../../yocto/meta-raspberrypi/conf/machine/\ninclude                 raspberrypi2.conf     raspberrypi-cm3.conf\nraspberrypi0.conf       raspberrypi3-64.conf  raspberrypi-cm.conf\nraspberrypi0-wifi.conf  raspberrypi3.conf     raspberrypi.conf\n$\n$ cat conf/local.conf \nMACHINE = \"raspberrypi3-64\"\nDISTRO ?= \"poky\"\nPACKAGE_CLASSES ?= \"package_rpm\"\nEXTRA_IMAGE_FEATURES ?= \"debug-tweaks ssh-server-openssh\"\nUSER_CLASSES ?= \"buildstats image-mklibs image-prelink\"\nPATCHRESOLVE = \"noop\"\nCONF_VERSION = \"1\"\n\nSSTATE_DIR ?= \"${TOPDIR}/../../sstate-cache\"\nTMPDIR ?= \"${TOPDIR}/../../tmp\"\nDL_DIR ?= \"${TOPDIR}/../../downloads\"\n\nPACKAGECONFIG_append_pn-qemu-native = \" sdl\"\nPACKAGECONFIG_append_pn-nativesdk-qemu = \" sdl\"\nBB_DISKMON_DIRS ??= \"\\\n    STOPTASKS,${TMPDIR},1G,100K \\\n    STOPTASKS,${DL_DIR},1G,100K \\\n    STOPTASKS,${SSTATE_DIR},1G,100K \\\n    STOPTASKS,/tmp,100M,100K \\\n    ABORT,${TMPDIR},100M,1K \\\n    ABORT,${DL_DIR},100M,1K \\\n    ABORT,${SSTATE_DIR},100M,1K \\\n    ABORT,/tmp,10M,1K\"\n\n#SDKMACHINE ?= \"i686\"\n#ASSUME_PROVIDED += \"libsdl-native\"\n$\n</code></pre> The variable <code>${TOPDIR}</code> represents the build directory by default. <code>debug-tweaks</code> will enable root account without password. <code>ssh-server-openssh</code> will install an ssh server using openssh. And the ssh server will be started automatically during boot-up.</p> <p>Build a basic image - <code>core-image-base</code> <pre><code>$ bitbake core-image-base\nWARNING: Layer openembedded-layer should set LAYERSERIES_COMPAT_openembedded-layer in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Layer multimedia-layer should set LAYERSERIES_COMPAT_multimedia-layer in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Layer networking-layer should set LAYERSERIES_COMPAT_networking-layer in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Layer meta-python should set LAYERSERIES_COMPAT_meta-python in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Layer openembedded-layer should set LAYERSERIES_COMPAT_openembedded-layer in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Layer multimedia-layer should set LAYERSERIES_COMPAT_multimedia-layer in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Layer networking-layer should set LAYERSERIES_COMPAT_networking-layer in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Layer meta-python should set LAYERSERIES_COMPAT_meta-python in its conf/layer.conf file to list the core layer names it is compatible with.\nWARNING: Host distribution \"ubuntu-17.10\" has not been validated with this version of the build system; you may possibly experience unexpected failures. It is recommended that you use a tested distribution.\nParsing recipes: 100% |#################################################################################################################################################| Time: 0:03:49\nParsing of 2081 .bb files complete (0 cached, 2081 parsed). 2940 targets, 116 skipped, 0 masked, 0 errors.\nNOTE: Resolving any missing task queue dependencies\n\nBuild Configuration:\nBB_VERSION           = \"1.37.0\"\nBUILD_SYS            = \"x86_64-linux\"\nNATIVELSBSTRING      = \"ubuntu-17.10\"\nTARGET_SYS           = \"aarch64-poky-linux\"\nMACHINE              = \"raspberrypi3-64\"\nDISTRO               = \"poky\"\nDISTRO_VERSION       = \"2.5\"\nTUNE_FEATURES        = \"aarch64\"\nTARGET_FPU           = \"\"\nmeta                 \nmeta-poky            \nmeta-yocto-bsp       = \"HEAD:da3625c52e1ab8985fba4fc3d133edf92142f182\"\nmeta-oe              \nmeta-multimedia      \nmeta-networking      \nmeta-python          = \"master:271ce3576be25d8012af15961f827aa2235e6434\"\nmeta-raspberrypi     = \"master:693f36dded2f29fd77720f7b2e7c88ea76554466\"\n\nNOTE: Fetching uninative binary shim from http://downloads.yoctoproject.org/releases/uninative/1.9/x86_64-nativesdk-libc.tar.bz2;sha256sum=c26622a1f27dbf5b25de986b11584b5c5b2f322d9eb367f705a744f58a5561ec\nInitialising tasks: 100% |##############################################################################################################################################| Time: 0:00:04\nNOTE: Executing SetScene Tasks\nNOTE: Executing RunQueue Tasks\nNOTE: Tasks Summary: Attempted 3407 tasks of which 106 didn't need to be rerun and all succeeded.\n\nSummary: There were 9 WARNING messages shown.\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$\n</code></pre></p>","tags":["RaspberryPi","yocto","build"]},{"location":"blog/2018/04/25/raspberry-pi-dishes-from-yocto-cuisine/#installation-and-booting","title":"Installation and booting","text":"<p>If everything goes well, you can see the final image <code>tmp/deploy/images/raspberrypi3-64/core-image-base-raspberrypi3-64.rpi-sdimg</code>. The path is relative to your build directory - <code>build/rpi3</code>. It is a soft-link to your latest build image. <code>dd</code> it to your sd-card - <code>/dev/sdb</code> in my case - and boot the Raspberry Pi 3 with it. <pre><code>$ sudo umount /dev/sdb*\n$ sudo dd if=tmp/deploy/images/raspberrypi3-64/core-image-base-raspberrypi3-64.rpi-sdimg of=/dev/sdb bs=1M\n$ sudo umount /dev/sdb*\n</code></pre></p>","tags":["RaspberryPi","yocto","build"]},{"location":"blog/2018/04/25/raspberry-pi-dishes-from-yocto-cuisine/#wifi-configuration-in-raspberry-pi","title":"WiFi configuration in Raspberry Pi","text":"<p>To access Raspberry Pi over ssh it should be part of network first. So for the first boot, connect a monitor to the board and boot with your sd-card.</p> <p>Edit <code>wpa_supplicant.conf</code> file to input WiFi access point related information. <pre><code>root@raspberrypi3-64:~# cat /etc/wpa_supplicant.conf \nctrl_interface=/var/run/wpa_supplicant\nctrl_interface_group=0\nupdate_config=1\n\nnetwork={\n    ssid=\"Bluetooth\"\n    psk=\"**********\"\n}\nroot@raspberrypi3-64:~#\n</code></pre> Enter your WiFi access point password against psk.</p> <p>And bring-up interface <code>wlan0</code> on boot-up <pre><code>root@raspberrypi3-64:~# cat /etc/init.d/networking\n.\n.\nstart)\n    .\n    .\n    ifup wlan0\n    .\n    .\n.\n.\n</code></pre> Configure sticky IP in your access point for your Raspberry Pi corresponding to its mac. So every time after reboot you can straightaway ssh to Raspberry Pi.</p>","tags":["RaspberryPi","yocto","build"]},{"location":"blog/2018/04/25/raspberry-pi-dishes-from-yocto-cuisine/#references","title":"References","text":"<ul> <li>[https://media.readthedocs.org/pdf/meta-raspberrypi/latest/meta-raspberrypi.pdf]</li> <li>[http://www.jumpnowtek.com/rpi/Raspberry-Pi-Systems-with-Yocto.html]</li> <li>[https://raspinterest.wordpress.com/2016/11/30/yocto-project-on-raspberry-pi-3/]</li> <li>[https://stackoverflow.com/questions/35904769/what-are-the-differences-between-open-embedded-core-and-meta-openembedded]</li> <li>[https://github.com/agherzan/meta-raspberrypi/issues/195]</li> <li>[https://patchwork.openembedded.org/patch/36139/]</li> </ul>","tags":["RaspberryPi","yocto","build"]},{"location":"blog/2018/05/05/kgdboe-on-raspberrypi---building-out-of-the-kernel-tree-module-with-yocto/","title":"KGDBoE on RaspberryPi - building out of the kernel tree module with yocto","text":"WARNING: KGDBoE cannot be used in Raspberry Pi due to lack of polling support in its network drivers. This post will be useful in future if polling is added. Also this post can be used as a reference for building kernel module that is out of kernel tree.","tags":["RaspberryPi","yocto","kernel module","build"]},{"location":"blog/2018/05/05/kgdboe-on-raspberrypi---building-out-of-the-kernel-tree-module-with-yocto/#on-host","title":"On host","text":"<p>Building module in Raspberry Pi itself will take more time. Also it requires multiple development tools to be added to final image which will drastically increase the image size. To cross-compile we need SDK for Raspberry Pi. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ source poky/oe-init-build-env ../build/rpi3/\n\n### Shell environment set up for builds. ###\n\nYou can now run 'bitbake &lt;target&gt;'\n\nCommon targets are:\n    core-image-minimal\n    core-image-sato\n    meta-toolchain\n    meta-ide-support\n\nYou can also run generated qemu images with a command like 'runqemu qemux86'\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$ bitbake core-image-base -c populate_sdk\nWARNING: Host distribution \"ubuntu-17.10\" has not been validated with this version of the build system; you may possibly experience unexpected failures. It is recommended that you use a tested distribution.\nLoading cache: 100% |#####################################################################################################| Time: 0:00:00\nLoaded 2783 entries from dependency cache.\nNOTE: Resolving any missing task queue dependencies\n\nBuild Configuration:\nBB_VERSION           = \"1.36.0\"\nBUILD_SYS            = \"x86_64-linux\"\nNATIVELSBSTRING      = \"universal\"\nTARGET_SYS           = \"aarch64-poky-linux\"\nMACHINE              = \"raspberrypi3-64\"\nDISTRO               = \"poky\"\nDISTRO_VERSION       = \"2.4.2\"\nTUNE_FEATURES        = \"aarch64\"\nTARGET_FPU           = \"\"\nmeta                 \nmeta-poky            \nmeta-yocto-bsp       = \"rocko:fdeecc901196bbccd7c5b1ea4268a2cf56764a62\"\nmeta-oe              \nmeta-multimedia      \nmeta-networking      \nmeta-python          = \"rocko:a65c1acb1822966c3553de9fc98d8bb6be705c4e\"\nmeta-raspberrypi     = \"rocko:20358ec57a8744b0a02da77b283620fb718b0ee0\"\n\nInitialising tasks: 100% |################################################################################################| Time: 0:00:11\nNOTE: Executing SetScene Tasks\nNOTE: Executing RunQueue Tasks\nNOTE: Tasks Summary: Attempted 3589 tasks of which 3589 didn't need to be rerun and all succeeded.\n\nSummary: There was 1 WARNING message shown.\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n</code></pre> Once done, the script to install SDK will be present in <code>./tmp/deploy/sdk/</code>. Run the script <code>poky-glibc-x86_64-core-image-base-aarch64-toolchain-2.4.2.sh</code>. This will install the SDK in <code>/opt/</code> directory by default. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$ ./tmp/deploy/sdk/poky-glibc-x86_64-core-image-base-aarch64-toolchain-2.4.2.sh \nPoky (Yocto Project Reference Distro) SDK installer version 2.4.2\n=================================================================\nEnter target directory for SDK (default: /opt/poky/2.4.2): \nThe directory \"/opt/poky/2.4.2\" already contains a SDK for this architecture.\nIf you continue, existing files will be overwritten! Proceed[y/N]? N\nInstallation aborted!\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$ ls /opt/poky/2.4.2/\nenvironment-setup-aarch64-poky-linux  sysroots/                             \nsite-config-aarch64-poky-linux        version-aarch64-poky-linux            \nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$ \n</code></pre> I didn't overwrite files as I have installed it already. The script <code>environment-setup-aarch64-poky-linux</code> is the one that will setup the cross-build environment for us.</p> <p>Clone the KGDBoE source from Github. Setup cross-build environment and build the module. <pre><code>$ git clone https://github.com/sysprogs/kgdboe.git\nCloning into 'kgdboe'...\nremote: Counting objects: 172, done.\nremote: Total 172 (delta 0), reused 0 (delta 0), pack-reused 171\nReceiving objects: 100% (172/172), 51.95 KiB | 81.00 KiB/s, done.\nResolving deltas: 100% (114/114), done.\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile\n$\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile\n$ cd kgdboe\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$ source /opt/poky/2.4.2/environment-setup-aarch64-poky-linux \nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$ make -C /home/kaba/Desktop/yocto/build/rpi3/tmp/work/raspberrypi3_64-poky-linux/linux-raspberrypi/1_4.9.59+gitAUTOINC+e7976b2aff-r0/linux-raspberrypi3_64-standard-build M=$(pwd)\nmake: Entering directory '/home/kaba/Desktop/yocto/build/rpi3/tmp/work/raspberrypi3_64-poky-linux/linux-raspberrypi/1_4.9.59+gitAUTOINC+e7976b2aff-r0/linux-raspberrypi3_64-standard-build'\n  LD      /home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/built-in.o\n  CC [M]  /home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/irqsync.o\n  CC [M]  /home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe_main.o\n  CC [M]  /home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe_io.o\n/home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe_io.c: In function 'force_single_cpu_mode':\n/home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe_io.c:119:3: error: implicit declaration of function 'cpu_down'; did you mean 'cpu_die'? [-Werror=implicit-function-declaration]\n   cpu_down(i);\n   ^~~~~~~~\n   cpu_die\ncc1: some warnings being treated as errors\n/home/kaba/Desktop/yocto/build/rpi3/tmp/work-shared/raspberrypi3-64/kernel-source/scripts/Makefile.build:293: recipe for target '/home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe_io.o' failed\nmake[3]: *** [/home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe_io.o] Error 1\n/home/kaba/Desktop/yocto/build/rpi3/tmp/work-shared/raspberrypi3-64/kernel-source/Makefile:1493: recipe for target '_module_/home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe' failed\nmake[2]: *** [_module_/home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe] Error 2\nMakefile:150: recipe for target 'sub-make' failed\nmake[1]: *** [sub-make] Error 2\nMakefile:24: recipe for target '__sub-make' failed\nmake: *** [__sub-make] Error 2\nmake: Leaving directory '/home/kaba/Desktop/yocto/build/rpi3/tmp/work/raspberrypi3_64-poky-linux/linux-raspberrypi/1_4.9.59+gitAUTOINC+e7976b2aff-r0/linux-raspberrypi3_64-standard-build'\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$\n</code></pre> <code>cpu_down</code> function is not available in ARM platform. It is used to shutdown all but one core when debugging, because debugging with single CPU is desirable unless e debug SMP related issues. I couldn't find equivalent ARM function. So let's comment out this call and disable CPUs during boot using <code>bootargs</code>. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$ git diff\ndiff --git a/kgdboe_io.c b/kgdboe_io.c\nindex 1d02360..0d32c36 100644\n--- a/kgdboe_io.c\n+++ b/kgdboe_io.c\n@@ -115,8 +115,8 @@ void force_single_cpu_mode(void)\n        printk(KERN_INFO \"kgdboe: single-core mode enabled. Shutting down all cores except #0. This is slower, but safer.\\n\");\n        printk(KERN_INFO \"kgdboe: you can try using multi-core mode by specifying the following argument:\\n\");\n        printk(KERN_INFO \"\\tinsmod kgdboe.ko force_single_core = 0\\n\");\n-       for (int i = 1; i &lt; nr_cpu_ids; i++)\n-               cpu_down(i);\n+       /*for (int i = 1; i &lt; nr_cpu_ids; i++)*/\n+               /*cpu_down(i);*/\n }\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$\n</code></pre> This time the build gave following linker error. <pre><code>aarch64-poky-linux-ld: unrecognized option '-Wl,-O1'\n</code></pre> This is because the <code>LDFLAGS</code> environment variable. Kernel module Makefile requires <code>LDFLAGS</code> to be set as only the options without <code>-Wl</code>. So it appends whatever there in <code>LDFLAGS</code> with <code>-Wl</code>. As <code>LDFLAGS</code> has another <code>-Wl</code>, we are getting unrecognized option error. So remove all <code>-Wl</code>s from <code>LDFLAGS</code>. <pre><code>$ echo $LDFLAGS \n-Wl,-O1 -Wl,--hash-style=gnu -Wl,--as-needed\n$ export LDFLAGS=\"-O1 --hash-style=gnu --as-needed\"\n$ echo $LDFLAGS \n-O1 --hash-style=gnu --as-needed\n</code></pre> And build now. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$ make -C /home/kaba/Desktop/yocto/build/rpi3/tmp/work/raspberrypi3_64-poky-linux/linux-raspberrypi/1_4.9.59+gitAUTOINC+e7976b2aff-r0/linux-raspberrypi3_64-standard-build M=$(pwd)\nmake: Entering directory '/home/kaba/Desktop/yocto/build/rpi3/tmp/work/raspberrypi3_64-poky-linux/linux-raspberrypi/1_4.9.59+gitAUTOINC+e7976b2aff-r0/linux-raspberrypi3_64-standard-build'\n  LD [M]  /home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe.o\n  Building modules, stage 2.\n  MODPOST 1 modules\n  CC      /home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe.mod.o\n  LD [M]  /home/kaba/Desktop/yocto/build/rpi3/cross-compile/kgdboe/kgdboe.ko\nmake: Leaving directory '/home/kaba/Desktop/yocto/build/rpi3/tmp/work/raspberrypi3_64-poky-linux/linux-raspberrypi/1_4.9.59+gitAUTOINC+e7976b2aff-r0/linux-raspberrypi3_64-standard-build'\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$ ls kgdboe.ko\n-rw-r--r-- 1 kaba kaba 2.1M May  5 16:11 kgdboe.ko\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$\n</code></pre> Now <code>scp</code> the module <code>kgdboe.ko</code> to Raspberry Pi. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3/cross-compile/kgdboe\n$ scp kgdboe.ko root@192.168.0.101\n</code></pre></p>","tags":["RaspberryPi","yocto","kernel module","build"]},{"location":"blog/2018/05/05/kgdboe-on-raspberrypi---building-out-of-the-kernel-tree-module-with-yocto/#on-target","title":"On Target","text":"<p>Add <code>maxcpus=1</code> option to Raspberry Pi's kernel command line to disable all but one core. The <code>bootargs</code> can be found in the file <code>cmdline.txt</code> in first partition of the SD card. <pre><code>$ ssh root@192.168.0.101\nLast login: Sat May  5 07:59:44 2018 from 192.168.0.108\nroot@raspberrypi3-64:~# ls\nkgdboe.ko\nroot@raspberrypi3-64:~# mount /dev/mmcblk0p1 /boot\nroot@raspberrypi3-64:~# cat /boot/cmdline.txt \ndwc_otg.lpm_enable=0 console=serial0,115200 root=/dev/mmcblk0p2 rootfstype=ext4 rootwait maxcpus=1\nroot@raspberrypi3-64:~#\n</code></pre> Reboot the Pi and try installing the module. <pre><code>root@raspberrypi3-64:~# cat /proc/cmdline \n8250.nr_uarts=0 cma=256M bcm2708_fb.fbwidth=1920 bcm2708_fb.fbheight=1080 bcm2708_fb.fbswap=1 vc_mem.mem_base=0x3ec00000 vc_mem.mem_size=0x40000000  dwc_otg.lpm_enable=0 console=ttyS0,115200 root=/dev/mmcblk0p2 rootfstype=ext4 rootwait maxcpus=1\nroot@raspberrypi3-64:~# insmod kgdboe.ko \ninsmod: ERROR: could not insert module kgdboe.ko: Invalid parameters\nroot@raspberrypi3-64:~# dmesg | tail -5\n[ 1211.816691] kgdboe: Failed to setup netpoll for eth0, code -524\n[ 8814.348313] netpoll: kgdboe: local IP 1.1.1.101\n[ 8814.348344] netpoll: kgdboe: eth0 doesn't support polling, aborting\n[ 8814.348360] kgdboe: Failed to setup netpoll for eth0, code -524\nroot@raspberrypi3-64:~#\n</code></pre> Due to lack of polling support, Kernel debugging over Ethernet is not available for as of now. If you are reading this, you are part of the clan having hope.</p>","tags":["RaspberryPi","yocto","kernel module","build"]},{"location":"blog/2018/05/05/kgdboe-on-raspberrypi---building-out-of-the-kernel-tree-module-with-yocto/#references","title":"References","text":"<ul> <li>[http://sysprogs.com/VisualKernel/kgdboe/tutorial/]</li> <li>[https://github.com/raspberrypi/linux/blob/rpi-4.14.y/arch/arm64/Kconfig]</li> <li>[https://sysprogs.com/w/forums/topic/two-problems-1-linuxkerneldebughelper-seg-fault-and-2-kgdboe-build-fails/]</li> <li>[https://stackoverflow.com/questions/25407320/what-is-the-signification-of-ldflags]</li> <li>[https://github.com/raspberrypi/linux/issues/1877]</li> </ul>","tags":["RaspberryPi","yocto","kernel module","build"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/","title":"KGDB/KDB over serial with Raspberry Pi","text":"","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/#hardware-setup","title":"Hardware setup","text":"<p>We need a USB to serial converter to connect Raspberry Pi to the PC serially. This is the cheapest among all converters available in Amazon. This is based on PL2302 chip. I'm not sure it's original or Chinese replica. In my case it worked out of the box with Ubuntu-17.10. In case if it throws error-10, try downgrading your PL2303 driver. Because the manufacturer blocked all counterfeit chips in his latest driver. I ordered this one and a set of female-to-female jumper wires. Wait for three days and continue with this article.</p> <p> Interfacing is simple - Connect 5V to 5V - Connect TX of converter with RxD of Raspberry's GPIO UART - Connect RX of converter with TxD of Raspberry's GPIO UART - Connect the ground to ground</p> <p> Below the Raspberry Pi GPIO pin layout   PL2303's pin layout   And the connection goes like this,   As power is supplied via GPIO pin, there is no need of external power supply. I don't know what will happen if both power sources are connected. I'm not dare to try.</p>","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/#software-setup","title":"Software setup","text":"<ul> <li>Kernel has to be built with debug support</li> <li>Enable <code>tui</code> support for GDB - if required</li> </ul> <p>For that I have created a custom layer with following tree structure. <pre><code>meta-kaba-hacks/\n\u251c\u2500\u2500 conf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 layer.conf\n\u251c\u2500\u2500 COPYING.MIT\n\u251c\u2500\u2500 recipes-devtools\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gdb\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 gdb-%.bbappend\n\u2514\u2500\u2500 recipes-kernel\n    \u2514\u2500\u2500 linux\n        \u251c\u2500\u2500 linux-raspberrypi\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 debug.cfg\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 enable_proc_zconfig.cfg\n        \u2514\u2500\u2500 linux-raspberrypi_4.9.bbappend\n</code></pre> Enable debug symbols and KGDB/KDB related configs in Linux. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ cat meta-kaba-hacks/recipes-kernel/linux/linux-raspberrypi_4.9.bbappend\nFILESEXTRAPATHS_prepend := \"${THISDIR}/${PN}:\"\nSRC_URI += \"\\\n            file://debug.cfg \\\n            file://enable_proc_zconfig.cfg \\\n            \"\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ cat meta-kaba-hacks/recipes-kernel/linux/linux-raspberrypi/debug.cfg\n# CONFIG_STRICT_KERNEL_RWX is not set\nCONFIG_DEBUG_INFO=y\nCONFIG_FRAME_POINTER=y\nCONFIG_KGDB=y\nCONFIG_KGDB_SERIAL_CONSOLE=y\nCONFIG_KGDB_KDB=y\nCONFIG_KDB_KEYBOARD=y\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ cat meta-kaba-hacks/recipes-kernel/linux/linux-raspberrypi/enable_proc_zconfig.cfg\nCONFIG_IKCONFIG=y\nCONFIG_IKCONFIG_PROC=y\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$\n</code></pre> By default <code>tui</code> options is disabled for <code>GDB</code> in yocto. So I'm overriding it to enable it. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ cat meta-kaba-hacks/recipes-devtools/gdb/gdb-%.bbappend\nEXTRA_OECONF += \" --enable-tui\"\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$\n</code></pre></p> <p>Build <code>kernel</code> and <code>populate_sdk</code>. Refer this and this if you need help on working with Yocto. And copy the newly built image to Raspberry Pi.</p>","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/#enable-serial-in-raspberry-pi","title":"Enable Serial in Raspberry Pi","text":"<p>By default serial interface is not enabled in yocto built Raspberry Pi distribution. We have to enable it in the config.txt file. Connect the SD-card written with Raspberry Pi image to PC and mount first partition. Append <code>enable_uart=1</code> to the config.txt file. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$ mount | grep sdb1\n/dev/sdb1 on /media/kaba/raspberrypi type vfat (rw,nosuid,nodev,relatime,uid=1000,gid=1000,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,showexec,utf8,flush,errors=remount-ro,uhelper=udisks2)\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$ tail /media/kaba/raspberrypi/config.txt\n#dtparam=pwr_led_gpio=35\n# Enable VC4 Graphics\ndtoverlay=vc4-fkms-v3d,cma-256\n# have a properly sized image\ndisable_overscan=1\n# Enable audio (loads snd_bcm2835)\ndtparam=audio=on\n# Load correct Device Tree for Aarch64\ndevice_tree=bcm2710-rpi-3-b.dtb\nenable_uart=1\nkaba@kaba-Vostro-1550:~/Desktop/yocto/build/rpi3\n$\n</code></pre> In the host machine open serial console using <code>screen</code>. And connect the USB to ttl converter with PC. You should see the logs of Raspberry Pi booting. <pre><code>$ sudo screen /det/ttyUSB0 115200\n</code></pre></p>","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/#debug-after-boot-complete","title":"Debug after boot complete","text":"<p>Configure <code>KDBoC</code> module to use <code>ttyS0</code> and enter <code>KDB</code> mode using <code>sysrq-trigger</code>. In <code>KDB</code> console, enter <code>kgdb</code> to make kernel listen to remote <code>GDB</code> debugger. <pre><code>root@raspberrypi3-64:~# echo ttyS0 &gt; /sys/module/kgdboc/parameters/kgdboc\n[  219.105202] KGDB: Registered I/O driver kgdboc\nroot@raspberrypi3-64:~# echo g &gt; /proc/sysrq-trigger\n[  255.963036] sysrq: SysRq : DEBUG\n\nEntering kdb (current=0xfffffff2f7f60000, pid 396) on processor 3 due to Keyboard Entry\n[3]kdb&gt; kgdb\nEntering please attach debugger or use $D#44+ or $3#33\n</code></pre> Raspberry Pi will be waiting for <code>GDB</code> client debugger connect serially.</p>","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/#debug-during-boot","title":"Debug during boot","text":"<p>If you want to debug something during boot, you have to connect <code>GDB</code> at very early stage of booting. Linux provides a command line argument option to achieve this. Configure <code>kgdboc</code> and <code>kgdbwait</code> in kernel <code>bootargs</code>. So kernel will wait after minimal initialization of hardware.</p> <pre><code>root@raspberrypi3-64:~# cat /boot/cmdline.txt\ndwc_otg.lpm_enable=0 console=serial0,115200 kgdboc=ttyS0,115200 kgdbwait root=/dev/mmcblk0p2 rootfstype=ext4 rootwait\nroot@raspberrypi3-64:~# reboot\n</code></pre> WARNING: As mentioned [here](https://github.com/raspberrypi/linux/issues/2245), it is a known issue that Raspberry Pi 3 doesn't boot with `kgdboc` set. So this will not work as of now. Let me find a work-around and update that in a future post.","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/#gdb-connect","title":"GDB connect","text":"<p>When Pi started waiting for <code>GDB</code> to connect, run the <code>cross-GDB</code> from host. You have to run this as a root. <pre><code>root@kaba-Vostro-1550:/home/kaba/Desktop/yocto/build/rpi3/tmp/work/raspberrypi3_64-poky-linux/linux-raspberrypi/1_4.9.59+gitAUTOINC+e7976b2aff-r0/image/boot# /home/kaba/Desktop/yocto/build/rpi3/tmp/work/x86_64-nativesdk-pokysdk-linux/gdb-cross-canadian-aarch64/8.0-r0/image/opt/poky/2.4.2/sysroots/x86_64-pokysdk-linux/usr/bin/aarch64-poky-linux/aarch64-poky-linux-gdb -tui ./vmlinux-4.9.59\n</code></pre> Once <code>GDB</code> is connected to the board, it will look like this. </p>","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/05/23/kgdbkdb-over-serial-with-raspberry-pi/#references","title":"References","text":"<ul> <li>[https://kaiwantech.wordpress.com/2013/07/04/a-kdb-kgdb-session-on-the-popular-raspberry-pi-embedded-linux-board/]</li> <li>[https://github.com/FooDeas/raspberrypi-ua-netinst/issues/122]</li> <li>[https://www.raspberrypi.org/forums/viewtopic.php?t=19186]</li> </ul>","tags":["RaspberryPi","yocto","kernel","debug"]},{"location":"blog/2018/06/03/debugging-application-with-cross-gdb-in-yocto-environment/","title":"Debugging application with cross-GDB in Yocto environment","text":"<p>GDB is a very useful tool when debugging applications. Embedded devices lack sophisticated features that Development machines enjoy like better processor speed, huge RAM, etc. In such cases running GDB on the target will be painfully slow. But thanks to remote-debugging support from GDB which saves us from such situations. In this post we'll see how to do remote debugging of an application running on Raspberry Pi 3.</p> <p>You can read previous posts to have better understanding about Yocto build environment and how to setup serial connection with Raspberry pi 3.</p>","tags":["gdb","debug","yocto"]},{"location":"blog/2018/06/03/debugging-application-with-cross-gdb-in-yocto-environment/#target-program-and-compilation","title":"Target program and compilation","text":"<p>Going along with the world, we'll start with traditional <code>Hello World</code> program. Write the C program and cross-compile it to <code>arm64</code> platform. Secure copy the executable binary to Raspberry Pi 3. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ cat syscall.c\n#include &lt;stdio.h&gt;\n\nint main()\n{\n    printf(\"Hello world!\\n\");\n    return 0;\n}\nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ source /opt/poky/2.4.2/environment-setup-aarch64-poky-linux \nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ $CC -g syscall.c -o syscall.arm64.debug\nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ $CC syscall.c -o syscall.arm64\nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ ls\ntotal 36K\n-rwxr-xr-x 1 kaba kaba 14K Jun  3 12:07 syscall.arm64\n-rwxr-xr-x 1 kaba kaba 16K Jun  3 12:07 syscall.arm64.debug\n-rw-r--r-- 1 kaba kaba  73 May 31 06:48 syscall.c\nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ scp ./syscall.arm64 root@192.168.0.101:\nsyscall.arm64                                                                                                                                                             100%   13KB 488.8KB/s   00:00    \n\nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ \n</code></pre> We have two binaries compiled here. We have copied the one without symbols to target board. And the one with debug symbols will be passed as an argument <code>cross-GDB</code>.</p>","tags":["gdb","debug","yocto"]},{"location":"blog/2018/06/03/debugging-application-with-cross-gdb-in-yocto-environment/#build-gdb-server","title":"Build GDB-server","text":"<p>Add <code>gdbserver</code> tool to the target image by enabling <code>tools-debug</code> in <code>EXTRA_IMAGE_FEATURES</code>. Run the <code>Hello World</code> program in target, attached with <code>gdbserver</code> that listens on network. <pre><code>root@raspberrypi3-64:~# gdbserver 192.168.0.101:2345 ./syscall.arm64 \nProcess ./syscall.arm64 created; pid = 1671\nListening on port 2345\n</code></pre> It will start the program and wait for <code>remote-gdb</code> to attach.</p>","tags":["gdb","debug","yocto"]},{"location":"blog/2018/06/03/debugging-application-with-cross-gdb-in-yocto-environment/#build-gdb-with-tui-support","title":"Build GDB with TUI support","text":"<p><code>GDB</code> has a <code>tui</code> [Text User Interface] mode which will be very much useful while debugging. With <code>tui</code> enabled, we can see the code that runs, equivalent assembly instruction, current register values, etc., simultaneously. But by default the <code>GDB</code> in Yocto doesn't build with <code>tui</code> support. Append <code>--enable-tui</code> option to <code>gdb-cross</code> bbfile. And build the SDK as mentioned here. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ tree meta-kaba-hacks/\nmeta-kaba-hacks/\n\u251c\u2500\u2500 conf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 layer.conf\n\u251c\u2500\u2500 COPYING.MIT\n\u251c\u2500\u2500 recipes-devtools\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gdb\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 gdb-%.bbappend\n\u2514\u2500\u2500 recipes-kernel\n    \u2514\u2500\u2500 linux\n        \u251c\u2500\u2500 linux-raspberrypi\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 debug.cfg\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 enable_proc_zconfig.cfg\n        \u2514\u2500\u2500 linux-raspberrypi_4.9.bbappend\n\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ cat meta-kaba-hacks/recipes-devtools/gdb/gdb-%.bbappend \nEXTRA_OECONF += \" --enable-tui\"\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ bitbake core-image-base -c populate_sdk\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ ./tmp/deploy/sdk/poky-glibc-x86_64-core-image-base-aarch64-toolchain-2.4.2.sh\nkaba@kaba-Vostro-1550:~/Desktop/yocto/yocto\n$ \n</code></pre></p>","tags":["gdb","debug","yocto"]},{"location":"blog/2018/06/03/debugging-application-with-cross-gdb-in-yocto-environment/#launch","title":"Launch","text":"<p>Run <code>cross-GDB</code> with <code>tui</code> enabled and connect it to the target. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ source /opt/poky/2.4.2/environment-setup-aarch64-poky-linux \nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ $GDB -tui ./syscall.arm64.debug \n</code></pre> <code>GDB</code> window will open-up similar to the screenshot at the top of this post (zoom-in your web page to see the image details clearly). Below I'm copying only the command-panel of <code>GDB</code>. Based on the commands you run, the other panels will also get changed.</p> <p>Connect to target program waiting for remote-GDB. <pre><code>(gdb) target remote 192.168.0.101:2345\nRemote debugging using 192.168.0.101:2345\nReading /lib/ld-linux-aarch64.so.1 from remote target...\nwarning: File transfers from remote targets can be slow. Use \"set sysroot\" to access files locally instead.\nReading /lib/ld-linux-aarch64.so.1 from remote target...\nReading symbols from target:/lib/ld-linux-aarch64.so.1...Reading /lib/ld-2.26.so from remote target...\nReading /lib/.debug/ld-2.26.so from remote target...\n(no debugging symbols found)...done.\n0x0000007fb7fd2f40 in ?? () from target:/lib/ld-linux-aarch64.so.1\n(gdb) \n</code></pre></p> <p>Now it connected to the target program. But it is not very much helpful. Because it tries to load the symbols and source from target board. The target has no source or symbol. Lets direct <code>GDB</code> to load source and symbol from host machine. <pre><code>(gdb) set sysroot /opt/poky/2.4.2/sysroots/aarch64-poky-linux/\nwarning: .dynamic section for \"/opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/ld-linux-aarch64.so.1\" is not at the expected address (wrong library or version mismatch?)\nReading symbols from /opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/ld-linux-aarch64.so.1...Reading symbols from /opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/.debug/ld-2.26.so...done.\ndone.\nReading symbols from /opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/ld-linux-aarch64.so.1...Reading symbols from /opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/.debug/ld-2.26.so...done.\ndone.\n(gdb) info sharedlibrary  \nFrom                To                  Syms Read   Shared Object Library\n0x0000007fb7fd2dc0  0x0000007fb7fe9fc8  Yes         /opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/ld-linux-aarch64.so.1\n(gdb)\n</code></pre> Set a break-point in <code>main</code> and continue. So the program will pause once it reaches main. Now if you see, there will be another shared library loaded. <pre><code>(gdb) break main\nBreakpoint 1 at 0x40056c: file syscall.c, line 5.\n(gdb) c\nContinuing.\n\nBreakpoint 1, main () at syscall.c:5\n(gdb) info sharedlibrary\nFrom                To                  Syms Read   Shared Object Library\n0x0000007fb7fd2dc0  0x0000007fb7fe9fc8  Yes         /opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/ld-linux-aarch64.so.1\n0x0000007fb7ea0700  0x0000007fb7f8a118  Yes         /opt/poky/2.4.2/sysroots/aarch64-poky-linux/lib/libc.so.6\n(gdb) \n</code></pre> Now you can see the line <code>printf</code> on the top panel will be highlighted. It means, that is the current line to be executed. Lets step in to the definition of <code>printf</code>. <pre><code>(gdb) s\n_IO_puts (str=0x400638 \"Hello world!\") at /usr/src/debug/glibc/2.26-r0/git/libio/ioputs.c:36\n(gdb)\n</code></pre> The control went into the library function. But we can see the top panel starts showing <code>No Source Available</code> error. This is because <code>GDB</code> searches the source files in wrong directory. Have a second look at last output. <code>GDB</code> searches <code>ioputs.c</code> in <code>/usr/src/debug/glibc/2.26-r0/git/libio</code> directory. Lets set the path to shared library correct. <pre><code>(gdb) set substitute-path /usr/src/debug/ /home/kaba/Desktop/yocto/build/rpi3/tmp/work/aarch64-poky-linux/\n(gdb)\n</code></pre> Now you can see the source started appearing in the top-panel. Read more about <code>GDB</code> commands and have fun.</p>","tags":["gdb","debug","yocto"]},{"location":"blog/2018/06/03/debugging-application-with-cross-gdb-in-yocto-environment/#references","title":"References","text":"<ul> <li>[https://sourceware.org/gdb/onlinedocs/gdb/Source-Path.html]</li> <li>[http://visualgdb.com/gdbreference/commands/sharedlibrary]</li> <li>[http://visualgdb.com/gdbreference/commands/set_solib-search-path]</li> <li>[https://www.yoctoproject.org/docs/1.4.2/adt-manual/adt-manual.html]</li> <li>[https://www.yoctoproject.org/docs/latest/mega-manual/mega-manual.html]</li> </ul>","tags":["gdb","debug","yocto"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/","title":"Anatomy of Linux system call in ARM64","text":"<p>The purpose of an Operating System is to run user applications. But the OS cannot provide user applications full control due to security reasons. So to do some privileged operations applications should ask OS to do the job on behalf of themselves. The primary interaction mechanism in Linux and similar Operating Systems is System Call. In this article we are going to see the anatomy of Linux System Calls in ARM64 architecture. Most people working with applications doesn't require it. It is only for those who have a printed copy of <code>Hitchhiker's guide to the galaxy</code>. DON'T PANIC!</p> <p>ARMv8 has four exception levels - EL0 to EL3. EL0 has lowest privilege where user applications run. EL3 has the highest privilege for Secure Monitor firmware (usually proprietary). Hypervisor runs in EL2 for virtualisation platforms. And our beloved Linux kernel runs in EL1. Elevation from one exception level to next exception level are achieved by setting exceptions. These exceptions will be set by one level and the next level will handle it. Explaining all types of exceptions is out of the scope of this article.</p> <p>The instruction used to set a synchronous exception [used for system call mechanism] to elevate from EL0 to EL1 is <code>svc</code> - supervisor call. Thus an application runs in Linux should issue <code>svc</code> with registers set with appropriate values. To know what are those appropriate values, Lets see how kernel handles <code>svc</code>.</p>","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#kernel-part","title":"Kernel Part","text":"NOTE: All the code references given in this post are from Linux-4.9.57.","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#vector-table-definition-in-kernel","title":"Vector table definition in Kernel","text":"<p>As I have mentioned already, there are multiple exceptions can be set by applications [EL0] which will be taken by Kernel [EL1]. The handlers for these exceptions are stored in a vector table. In ARMv8 the register that mentions the base address of that vector table is <code>VBAR_EL1</code> [Vector Base Address Register for EL1].</p> <p>{% blockquote ARM infocenter %} When an exception occurs, the processor must execute handler code which corresponds to the exception. The location in memory where the handler is stored is called the exception vector. In the ARM architecture, exception vectors are stored in a table, called the exception vector table. Each Exception level has its own vector table, that is, there is one for each of EL3, EL2 and EL1. The table contains instructions to be executed, rather than a set of addresses. Vectors for individual exceptions are located at fixed offsets from the beginning of the table. The virtual address of each table base is set by the Vector Based Address Registers VBAR_EL3, VBAR_EL2 and VBAR_EL1.</p> <p>As explained above the exception-handlers reside in a continuous memory and each vector spans up to 32 instructions long. Based on type of the exception, the execution will start from an instruction in a particular offset from the base address <code>VBAR_EL1</code>. Below is the ARM64 vector table. For example when an synchronous exception is set from EL0 is set, the handler at <code>VBAR_EL1 +0x400</code> will execute to handle the exception.</p> Offset from VBAR_EL1 Exception type Exception set level +0x000 Synchronous Current EL with SP0 +0x080 IRQ/vIRQ \" +0x100 FIQ/vFIQ \" +0x180 SError/vSError \" +0x200 Synchronous Current EL with SPx +0x280 IRQ/vIRQ \" +0x300 FIQ/vFIQ \" +0x380 SError/vSError \" +0x400 Synchronous Lower EL using ARM64 +0x480 IRQ/vIRQ \" +0x500 FIQ/vFIQ \" +0x580 SError/vSError \" +0x600 Synchronous Lower EL with ARM32 +0x680 IRQ/vIRQ \" +0x700 FIQ/vFIQ \" +0x780 SError/vSError \" <p>Linux defines the vector table at <code>arch/arm64/kernel/entry.S +259</code>. Each <code>ventry</code> is 32 instructions long. As an instruction in ARMv8 is 4 bytes long, next <code>ventry</code> will start at +0x80 of current <code>ventry</code>. <pre><code>ENTRY(vectors)\n    ventry    el1_sync_invalid           // Synchronous EL1t\n    ventry    el1_irq_invalid            // IRQ EL1t\n    ventry    el1_fiq_invalid            // FIQ EL1t\n    ventry    el1_error_invalid          // Error EL1t\n\n    ventry    el1_sync                   // Synchronous EL1h\n    ventry    el1_irq                    // IRQ EL1h\n    ventry    el1_fiq_invalid            // FIQ EL1h\n    ventry    el1_error_invalid          // Error EL1h\n\n    ventry    el0_sync                   // Synchronous 64-bit EL0\n    ventry    el0_irq                    // IRQ 64-bit EL0\n    ventry    el0_fiq_invalid            // FIQ 64-bit EL0\n    ventry    el0_error_invalid          // Error 64-bit EL0\n\n#ifdef CONFIG_COMPAT\n    ventry    el0_sync_compat            // Synchronous 32-bit EL0\n    ventry    el0_irq_compat             // IRQ 32-bit EL0\n    ventry    el0_fiq_invalid_compat     // FIQ 32-bit EL0\n    ventry    el0_error_invalid_compat   // Error 32-bit EL0\n#else\n    ventry    el0_sync_invalid           // Synchronous 32-bit EL0\n    ventry    el0_irq_invalid            // IRQ 32-bit EL0\n    ventry    el0_fiq_invalid            // FIQ 32-bit EL0\n    ventry    el0_error_invalid          // Error 32-bit EL0\n#endif\nEND(vectors)\n</code></pre> And loads the vector table into <code>VBAR_EL1</code> at <code>arch/arm64/kernel/head.S +433</code>. <pre><code>    adr_l   x8, vectors                 // load VBAR_EL1 with virtual\n    msr     vbar_el1, x8                // vector table address\n    isb\n</code></pre> <code>VBAR_EL1</code> is an system register. So it cannot be accessed directly. Special system instructions <code>msr</code> and <code>mrs</code> should be used manipulate system registers.</p> Instruction Description <code>adr_l x8, vector</code> loads the address of vector table into general purpose register <code>X8</code> <code>msr vbar_el1, x8</code> moves value in <code>X8</code> to system register <code>VBAR_EL1</code> <code>isb</code> instruction sync barrier","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#system-call-flow-in-kernel","title":"System call flow in Kernel","text":"<p>Now lets see what happens when an application issues the instruction <code>svc</code>. From the table, we can see for AArch64 synchronous exception from lower level, the offset is <code>+0x400</code>. In the Linux vector definition <code>VBAR_EL1+0x400</code> is <code>el0_sync</code>. Lets go to the <code>el0_sync</code> definition at <code>arch/arm64/kernel/entry.S +458</code> <pre><code>el0_sync:\n    kernel_entry 0\n    mrs    x25, esr_el1                     // read the syndrome register\n    lsr    x24, x25, #ESR_ELx_EC_SHIFT      // exception class\n    cmp    x24, #ESR_ELx_EC_SVC64           // SVC in 64-bit state\n    b.eq    el0_svc\n    cmp    x24, #ESR_ELx_EC_DABT_LOW        // data abort in EL0\n    b.eq    el0_da\n    cmp    x24, #ESR_ELx_EC_IABT_LOW        // instruction abort in EL0\n    b.eq    el0_ia\n    cmp    x24, #ESR_ELx_EC_FP_ASIMD        // FP/ASIMD access\n    b.eq    el0_fpsimd_acc\n    cmp    x24, #ESR_ELx_EC_FP_EXC64        // FP/ASIMD exception\n    b.eq    el0_fpsimd_exc\n    cmp    x24, #ESR_ELx_EC_SYS64           // configurable trap\n    b.eq    el0_sys\n    cmp    x24, #ESR_ELx_EC_SP_ALIGN        // stack alignment exception\n    b.eq    el0_sp_pc\n    cmp    x24, #ESR_ELx_EC_PC_ALIGN        // pc alignment exception\n    b.eq    el0_sp_pc\n    cmp    x24, #ESR_ELx_EC_UNKNOWN         // unknown exception in EL0\n    b.eq    el0_undef\n    cmp    x24, #ESR_ELx_EC_BREAKPT_LOW     // debug exception in EL0\n    b.ge    el0_dbg\n    b    el0_inv\n</code></pre> The subroutine is nothing but a bunch of <code>if</code> conditions. The synchronous exception can have multiple reasons which will be  stored in the syndrome register <code>esr_el1</code>. Compare the value in syndrome register with predefined macros and branch to the corresponding subroutine.</p> Instruction Description <code>kernel_entry 0</code> It is a macro defined at<code>arch/arm64/kernel/entry.S +71</code>. It storesall the general purpose registers intoCPU stack as the sys_* functionexpects its arguments fromCPU stack only <code>mrs x25, esr_el1</code> Move system register <code>esr_el1</code> to generalpurpose register <code>X25</code>. <code>esr_el1</code> is the exception syndrome register. It will have the syndrome code that caused the exception. <code>lsr x24, x25, #ESR_ELx_EC_SHIFT</code> Left shift <code>X25</code> by <code>ESR_ELx_EC_SHIFT</code> bits and store the result in <code>X24</code> <code>cmp x24, #ESR_ELx_EC_SVC64</code> Compare the value in <code>X24</code> with <code>ESR_ELx_EC_SVC64</code>. If both are equal <code>Z</code> bit will be set in <code>NZCV</code> special purpose register. <code>b.eq el0_svc</code> If <code>Z</code> flag is set in <code>NZCV</code>, branch to <code>el0_svc</code>. It is just <code>b</code> not <code>bl</code>. So the control will not come back to caller. The condition check will happen until it finds the appropriate reason. If all are wrong <code>el0_inv</code> will be called. <p>In a system call case, control will be branched to <code>el0_svc</code>. It is defined at <code>arm64/kernel/entry.S +742</code> as follows <pre><code>/*\n * SVC handler.\n */\n    .align    6\nel0_svc:\n    adrp    stbl, sys_call_table            // load syscall table pointer\n    uxtw    scno, w8                        // syscall number in w8\n    mov     sc_nr, #__NR_syscalls\nel0_svc_naked:                              // compat entry point\n    stp     x0, scno, [sp, #S_ORIG_X0]      // save the original x0 and syscall number    enable_dbg_and_irq\n    ct_user_exit 1\n\n    ldr     x16, [tsk, #TI_FLAGS]           // check for syscall hooks\n    tst     x16, #_TIF_SYSCALL_WORK\n    b.ne    __sys_trace\n    cmp     scno, sc_nr                     // check upper syscall limit\n    b.hs    ni_sys\n    ldr     x16, [stbl, scno, lsl #3]       // address in the syscall table\n    blr     x16                             // call sys_* routine\n    b       ret_fast_syscall\nni_sys:\n    mov     x0, sp\n    bl      do_ni_syscall\n    b       ret_fast_syscall\nENDPROC(el0_svc)\n</code></pre> Before going through the code, let me introduce the aliases <code>arch/arm64/kernel/entry.S +229</code> <pre><code>/*\n * These are the registers used in the syscall handler, and allow us to\n * have in theory up to 7 arguments to a function - x0 to x6.\n *\n * x7 is reserved for the system call number in 32-bit mode.\n */\nsc_nr   .req    x25        // number of system calls\nscno    .req    x26        // syscall number\nstbl    .req    x27        // syscall table pointer\ntsk     .req    x28        // current thread_info\n</code></pre> Now lets walk through function <code>el0_svc</code>,</p> Instruction Description <code>adrp stbl, sys_call_table</code> I'll come to the <code>sys_call_table</code> in next section. It is the table indexed with syscall number and corresponding function. It has to be in a 4K aligned memory. Thus this instruction adds the top 22-bits of <code>sys_call_table</code> address with top 52-bit of <code>PC</code> (program counter) and stores the value at <code>stbl</code>. Actually it forms the PC-relative address to 4KB page. <code>uxtw scno, w8</code> unsigned extract from 32-bit word. Read 32-bit form of General purpose register <code>X8</code> and store it in <code>scno</code> <code>mov sc_nr, #__NR_syscalls</code> Load <code>sc_nr</code> with number of system calls <code>stp x0, scno, [sp, #S_ORIG_X0]</code> Store a pair of registers <code>X0</code> and <code>scno</code> at the memory location stack-pointer + <code>S_ORIG_X0</code>. Value of <code>S_ORIG_X0</code> is not really important. As long as stack-pointer is not modified, we can access the stored values anytime. <code>enable_dbg_and_irq</code> it is a macro defined at <code>arch/arm64/include/asm/assembler.h +88</code>. It actually enables IRQ and debugging by setting appropriate value at special purpose register <code>DAIF</code>. <code>ct_user_exit 1</code> another macro not much important unless you bother about <code>CONFIG_CONTEXT_TRACKING</code> <code>ldr x16, [tsk, #TI_FLAGS]</code> <code>tst x16, #_TIF_SYSCALL_WORK</code> <code>b.ne __sys_trace</code> These instructions are related to syscall hooks. If syscall hooks are set, call <code>__sys_trace</code>. For those who got confused about <code>b.ne</code> like me, <code>.ne</code> only check whether <code>Z</code> flag is non zero. <code>tst</code> instruction does an bitwise AND of both operands. If both are equal, <code>Z</code> flag will be non zero. <code>cmp scno, sc_nr</code> <code>b.hs ni_sys</code> Just an error check. If the syscall number is greater than<code>sc_nr</code> go to <code>ni_sys</code> <code>ldr x16, [stbl, scno, lsl #3]</code> Load the address of corresponding <code>sys_*</code> function into <code>X16</code>.Will explain detail in next section <code>blr x16</code> subroutine call to the actual <code>sys_*</code> function <code>b ret_fast_syscall</code> Maybe a house-keeping function. Control will not flow further down.","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#sys_call_table","title":"sys_call_table","text":"<p>It is nothing but an array of function pointer indexed with the system call number. It has to be placed in an 4K aligned memory. For ARM64 <code>sys_call_table</code> is defined at <code>arch/arm64/kernel/sys.c +55</code>. <pre><code>#undef __SYSCALL\n#define __SYSCALL(nr, sym)  [nr] = sym,\n\n/*\n * The sys_call_table array must be 4K aligned to be accessible from\n * kernel/entry.S.\n */\nvoid * const sys_call_table[__NR_syscalls] __aligned(4096) = {\n    [0 ... __NR_syscalls - 1] = sys_ni_syscall,\n#include &lt;asm/unistd.h&gt;\n};\n</code></pre>  * <code>__NR_syscalls</code> defines the number of system call. This varies from architecture to architecture.  * Initially all the system call numbers were set <code>sys_ni_syscall</code> - not implemented system call. If a system call is removed, its system call number will not be reused. Instead it will be assigned with <code>sys_ni_syscall</code> function.  * And the include goes like this <code>arch/arm64/include/asm/unistd.h</code> -&gt; <code>arch/arm64/include/uapi/asm/unistd.h</code> -&gt; <code>include/asm-generic/unistd.h</code> -&gt; <code>include/uapi/asm-generic/unistd.h</code>. The last file has the definition of all system calls. For example the <code>write</code> system call is defined here as <pre><code>#define __NR_write 64\n__SYSCALL(__NR_write, sys_write)\n</code></pre>  * The <code>sys_call_table</code> is an array of function pointers. As in ARM64 a function pointer is 8 bytes long, to calculate the address of actual system call, system call number <code>scno</code> is left shifted by 3 and added with system call table address <code>stbl</code> in the <code>el0_svc</code> subroutine - <code>ldr x16, [stbl, scno, lsl #3]</code></p>","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#system-call-definition","title":"System call definition","text":"<p>Each system call is defined with a macro <code>SYSCALL_DEFINEn</code> macro. <code>n</code> is corresponding to the number of arguments the system call accepts. For example the <code>write</code> is implemented at <code>fs/read_write.c +599</code> <pre><code>SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf,\n        size_t, count)\n{\n    struct fd f = fdget_pos(fd);\n    ssize_t ret = -EBADF;\n\n    if (f.file) {\n        loff_t pos = file_pos_read(f.file);\n        ret = vfs_write(f.file, buf, count, &amp;pos);\n        if (ret &gt;= 0)\n            file_pos_write(f.file, pos);\n        fdput_pos(f);\n    }\n\n    return ret;\n}\n</code></pre> This macro will expand into <code>sys_write</code> function definition and other aliases functions as mentioned in this LWN article. The expanded function will have the compiler directive <code>asmlinkage</code> set. It instructs the compiler to look for arguments in CPU stack instead of registers. This is to implement system calls architecture independent. That's why <code>kernel_entry</code> macro in <code>el0_sync</code> pushed all general purpose registers into stack. In ARM64 case registers <code>X0</code> to <code>X7</code> will have the arguments.</p>","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#application-part","title":"Application part","text":"<p>An user application should do following steps to make a system call  * Set the lower 32-bit of general purpose register <code>X8</code> with appropriate system call number - <code>el0_svc</code> loads the system call number from <code>W0</code>  * And then issue the <code>svc</code> instruction  * Now kernel implemented <code>sys_*</code> function will run on behalf of the application</p> <p>Normally all the heavy lifting will be done by the <code>glibc</code> library. Dependency on <code>glibc</code> also makes the application portable across platforms having <code>glibc</code> support. Each platform will have different system call number and <code>glibc</code> takes care of that while compiling.</p>","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#printf-implementation","title":"<code>printf</code> implementation","text":"<p>In this section we'll see how <code>glibc</code> implements the <code>printf</code> system call. Going deep into all <code>glibc</code> macro expansion is out of scope of this post. End of the day <code>printf</code> has to be a <code>write</code> towards the <code>stdout</code> file. Lets see that.</p> <p>Wrote a simple hello world program. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ cat syscall.c\n#include &lt;stdio.h&gt;\n\nint main()\n{\n    printf(\"Hello world!\\n\");\n    return 0;\n}\nkaba@kaba-Vostro-1550:~/Desktop/workbench/code\n$ \n</code></pre> Compile it with symbols and export it to an ARM64 target [Raspberry Pi 3 in my case]. Run it in the target with <code>gdbserver</code> and connect remote GDB as mentioned in previous post.</p> <p>The last function in <code>glibc</code> that issues <code>svc</code> is <code>__GI___libc_write</code>. I got this with the help of <code>GDB</code>. If you really want to go through <code>glibc</code> code to trace this function all the way from <code>printf</code>, prepare yourself. Here is the <code>GDB</code> backtrace output. <pre><code>(gdb) bt\n#0  __GI___libc_write (fd=1, buf=0x412260, nbytes=13) at /usr/src/debug/glibc/2.26-r0/git/sysdeps/unix/sysv/linux/write.c:26\n#1  0x0000007fb7eeddcc in _IO_new_file_write (f=0x7fb7fcd540 &lt;_IO_2_1_stdout_&gt;, data=0x412260, n=13) at /usr/src/debug/glibc/2.26-r0/git/libio/fileops.c:1255\n#2  0x0000007fb7eed160 in new_do_write (fp=0x7fb7fcd540 &lt;_IO_2_1_stdout_&gt;, data=0x412260 \"Hello world!\\n\", to_do=to_do@entry=13) at /usr/src/debug/glibc/2.26-r0/git/libio/fileops.c:510\n#3  0x0000007fb7eeefc4 in _IO_new_do_write (fp=fp@entry=0x7fb7fcd540 &lt;_IO_2_1_stdout_&gt;, data=&lt;optimized out&gt;, to_do=13) at /usr/src/debug/glibc/2.26-r0/git/libio/fileops.c:486\n#4  0x0000007fb7eef3f0 in _IO_new_file_overflow (f=0x7fb7fcd540 &lt;_IO_2_1_stdout_&gt;, ch=10) at /usr/src/debug/glibc/2.26-r0/git/libio/fileops.c:851\n#5  0x0000007fb7ee3d78 in _IO_puts (str=0x400638 \"Hello world!\") at /usr/src/debug/glibc/2.26-r0/git/libio/ioputs.c:41\n#6  0x0000000000400578 in main () at syscall.c:5\n</code></pre>  Lets us see the assembly instructions of <code>__GI_libc_write</code> function in GDB TUI. <pre><code>   |0x7fb7f407a8 &lt;__GI___libc_write&gt;        stp    x29, x30, [sp, #-48]!\n  &gt;\u25020x7fb7f407ac &lt;__GI___libc_write+4&gt;      adrp   x3, 0x7fb7fd1000 &lt;__libc_pthread_functions+184&gt;\n   \u25020x7fb7f407b0 &lt;__GI___libc_write+8&gt;      mov    x29, sp\n   \u25020x7fb7f407b4 &lt;__GI___libc_write+12&gt;     str    x19, [sp, #16]\n   \u25020x7fb7f407b8 &lt;__GI___libc_write+16&gt;     sxtw   x19, w0\n   \u25020x7fb7f407bc &lt;__GI___libc_write+20&gt;     ldr    w0, [x3, #264]\n   \u25020x7fb7f407c0 &lt;__GI___libc_write+24&gt;     cbnz   w0, 0x7fb7f407f0 &lt;__GI___libc_write+72&gt;\n   \u25020x7fb7f407c4 &lt;__GI___libc_write+28&gt;     mov    x0, x19\n   \u25020x7fb7f407c8 &lt;__GI___libc_write+32&gt;     mov    x8, #0x40                       // #64\n   \u25020x7fb7f407cc &lt;__GI___libc_write+36&gt;     svc    #0x0\n   .\n   .\n   .\n</code></pre></p> Instruction Description stp    x29, x30, [sp, #-48]! Increment stack pointer and back-upframe-pointer and link-register adrp   x3, 0x7fb7fd1000 Load the PC related addressof <code>SINGLE_THREAD_P</code> mov    x29, sp Move current stack-pointerinto frame-pointer str    x19, [sp, #16] Backup <code>X19</code> in stack sxtw   x19, w0 Backup <code>W0</code> into <code>X19</code> the first parameterThis function has 3 parametersSo they will be in <code>X0</code>, <code>X1</code>and<code>X2</code>. Thus they have to bebacked-up before using ldr    w0, [x3, #264] Load the global into <code>W0</code>This global tells whetherit is a multi-threadedprogram cbnz   w0, 0x7fb7f407f0 Conditional branch if <code>W0</code>is non zeroActually a jump in case ofmulti-threaded program.Our case is single threaded.So fall through mov    x0, x19 Restore <code>X19</code> into <code>X0</code>.The first argument.Here the <code>fd</code> mov    x8, #0x40 Load <code>X8</code> with the value <code>0x40</code>.Kernel will lookfor the system call numberat <code>X8</code>. So we load it with 64which is the system callnumber of <code>write</code> svc    #0x0 All set. Now <code>supervisor call</code> <p>From this point the almighty Linux kernel takes control. Pretty long article ahh!. At last the improbable drive comes to equilibrium as you complete this.</p>","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/06/11/anatomy-of-linux-system-call-in-arm64/#references","title":"References","text":"<ul> <li>http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/ch09s01s01.html</li> <li>http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.den0024a/CHDEEDDC.html</li> <li>https://lwn.net/Articles/604287/</li> <li>https://courses.cs.washington.edu/courses/cse469/18wi/Materials/arm64.pdf</li> <li>http://www.osteras.info/personal/2013/10/11/hello-world-analysis.html</li> <li>And the Linux kernel source. Thanks to https://elixir.bootlin.com</li> </ul>","tags":["system call","Linux","gdb","ARM","ARMv8"]},{"location":"blog/2018/09/08/the-volatile-keyword/","title":"The Volatile keyword","text":"<p>Recently I've interviewed some candidates for entry and intermediate level positions. One of the questions most of them struggled is about the <code>volatile</code> keyword. Some conversations went like this,  * Q: Why we use <code>volatile</code> keyword?  * A: It will tell compiler not to use any registers for the <code>volatile</code> variable.  * Q: Then how will it work in an ARM processor? In ARM no instruction other than load and store can use memory location.  * A: ??!!</p> <ul> <li>Q: What is the purpose <code>volatile</code> keyword?</li> <li>A: We'll use for IO memory</li> <li>Q: Why we need it for IO memory?</li> <li>A: So every time processor accesses the memory, it will go to IO device</li> <li>Q: So <code>volatile</code> is to tell processor not to cache data?</li> <li>A: Yes</li> <li>Q: Thus <code>volatile</code> is a processor directive not compiler directive?</li> <li>And confusion starts</li> </ul> <p>In this post, lets see how <code>volatile</code> works with two simple C programs. In complex programs with multiple variables and loops <code>volatile</code> keyword will make significant difference in speed and memory usage.</p> <p>GCC provides many compiler optimization flags. Enabling them will aggressively optimize the code and give better performance in terms of speed and memory footprint. As these optimizations make debugging harder, they are not suitable development. All available GCC compiler optimization flags can be get from following command. <pre><code>$ $CC --help=optimizers\nThe following options control optimizations:\n  -O&lt;number&gt;                  Set optimization level to &lt;number&gt;.\n  -Ofast                      Optimize for speed disregarding exact standards compliance.\n  -Og                         Optimize for debugging experience rather than speed or size.\n  -Os                         Optimize for space rather than speed.\n  -faggressive-loop-optimizations Aggressively optimize loops using language constraints.\n.\n.\n.\n</code></pre></p> <p>For simplicity, I used only <code>-Ofast</code> optimizer for the examples. It informs GCC to do its best to make the program run faster. We'll see how compiler builds, with and without <code>volatile</code>. GCC will give assembly code output with <code>-S</code> options.</p> <p>Take the following C program. <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int *x = (int *)0xc000;\n    int b = *x;\n    int c = *x;\n    *x = b + c;\n    return 0;\n}\n</code></pre> Don't worry about dereferencing a random virtual address. We are not going to run this program, just build the assembly code and examine manually. I use pointers in these programs. Because using immediate value makes no sense with volatile. We have an integer pointer <code>x</code> points to address <code>0xc000</code>. We initialize two variables <code>b</code> and <code>c</code> with value in address <code>0xc000</code>. And then addition of <code>b</code> and <code>c</code> is stored in location <code>0xc000</code>. So we read the value in location <code>0xc000</code> twice in this program. Let see how it gets compiled by GCC form ARMv8.</p> <pre><code>$ echo $CC\naarch64-poky-linux-gcc --sysroot=/opt/poky/2.4.2/sysroots/aarch64-poky-linux\nkaba@kaba-Vostro-1550:~/Desktop/volatile/single_varuable_two_reads\n$ $CC -S -Ofast ./code.c -o code.S\nkaba@kaba-Vostro-1550:~/Desktop/volatile/single_varuable_two_reads\n$\n</code></pre> <p><pre><code>    .arch armv8-a\n    .file   \"code.c\"\n    .text\n    .section    .text.startup,\"ax\",@progbits\n    .align  2\n    .p2align 3,,7\n    .global main\n    .type   main, %function\nmain:\n    mov x2, 49152\n    mov w0, 0\n    ldr w1, [x2]\n    lsl w1, w1, 1\n    str w1, [x2]\n    ret\n    .size   main, .-main\n    .ident  \"GCC: (GNU) 7.3.0\"\n    .section    .note.GNU-stack,\"\",@progbits\n</code></pre> The compiler intelligently finds that variable <code>b</code> and <code>c</code> have same value from address <code>0xc000</code> and addition of them is equivalent to multiplying the value at <code>0xc000</code> by two. So it loads the value into register <code>W1</code> and left shifts it by 1 (equivalent of multiplying with two) and then stores the new value into location <code>0xc000</code>.</p> <p>Now lets change the code to use <code>volatile</code> for variable <code>x</code>. And see how the assembly code looks. <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    volatile int *x = (int *)0xc000;\n    int b = *x;\n    int c = *x;\n    *x = b + c;\n    return 0;\n}\n</code></pre> <pre><code>    .arch armv8-a\n    .file   \"code.c\"\n    .text\n    .section    .text.startup,\"ax\",@progbits\n    .align  2\n    .p2align 3,,7\n    .global main\n    .type   main, %function\nmain:\n    mov x1, 49152\n    mov w0, 0\n    ldr w2, [x1]\n    ldr w3, [x1]\n    add w2, w2, w3\n    str w2, [x1]\n    ret\n    .size   main, .-main\n    .ident  \"GCC: (GNU) 7.3.0\"\n    .section    .note.GNU-stack,\"\",@progbits\n</code></pre> This time the compiler considers that the value at location <code>0xc000</code> may be different each time it reads. It thinks that the variables <code>b</code> and <code>c</code> could be initialized with different values. So it reads the location <code>0xc000</code> twice and adds both values.</p> <p>Lets see a simple loop case <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int *x = (int *)0xc000;\n    int *y = (int *)0xd000;\n    int sum = 0;\n    for (int i = 0; i &lt; *y; i++) {\n        sum = sum + *x;\n    }\n    *x = sum;\n    return 0;\n}\n</code></pre> This program initializes two pointers <code>x</code> and <code>y</code> to locations <code>0xc000</code> and <code>0xd000</code> respectively. It adds the value at <code>x</code> to itself as many times the value at <code>y</code>. Lets see how GCC sees it. <pre><code>    .arch armv8-a\n    .file   \"code.c\"\n    .text\n    .section    .text.startup,\"ax\",@progbits\n    .align  2\n    .p2align 3,,7\n    .global main\n    .type   main, %function\nmain:\n    mov x0, 53248\n    ldr w0, [x0]\n    cmp w0, 0\n    ble .L3\n    mov x1, 49152\n    ldr w1, [x1]\n    mul w1, w0, w1\n.L2:\n    mov x2, 49152\n    mov w0, 0\n    str w1, [x2]\n    ret\n.L3:\n    mov w1, 0\n    b   .L2\n    .size   main, .-main\n    .ident  \"GCC: (GNU) 7.3.0\"\n    .section    .note.GNU-stack,\"\",@progbits\n</code></pre> The compiler assigns register <code>X0</code> to <code>y</code> and register <code>X1</code> to <code>x</code>. The program compares the value at <code>[X0]</code> - value at the address in <code>X0</code> - with zero. If so, it jumps to <code>.L3</code> which sets <code>W1</code> to zero and jumps to <code>.L2</code>. Or it simply multiplies <code>[X0]</code> and <code>[X1]</code> and stores the value in <code>W1</code>. <code>.L2</code> stores the value in <code>W1</code> at <code>[X2]</code> and returns. The compiler intelligently identifies that adding <code>[X2]</code> to itself <code>[X1]</code> times is equivalent to multiplying both.</p> <p>With <code>volatile</code>, <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    volatile int *x = (int *)0xc000;\n    int *y = (int *)0xd000;\n    int sum = 0;\n    for (int i = 0; i &lt; *y; i++) {\n        sum = sum + *x;\n    }\n    *x = sum;\n    return 0;\n}\n</code></pre> the corresponding assembly code is <pre><code>    .arch armv8-a\n    .file   \"code.c\"\n    .text\n    .section    .text.startup,\"ax\",@progbits\n    .align  2\n    .p2align 3,,7\n    .global main\n    .type   main, %function\nmain:\n    mov x0, 53248\n    ldr w3, [x0]\n    cmp w3, 0\n    ble .L4\n    mov w0, 0\n    mov w1, 0\n    mov x4, 49152\n    .p2align 2\n.L3:\n    ldr w2, [x4]\n    add w0, w0, 1\n    cmp w0, w3\n    add w1, w1, w2\n    bne .L3\n.L2:\n    mov x2, 49152\n    mov w0, 0\n    str w1, [x2]\n    ret\n.L4:\n    mov w1, 0\n    b   .L2\n    .size   main, .-main\n    .ident  \"GCC: (GNU) 7.3.0\"\n    .section    .note.GNU-stack,\"\",@progbits\n</code></pre> This time GCC uses <code>X4</code> for the address <code>0xc000</code>, but its not significant for our problem. Look here the loop is <code>.L3</code>. It loads the value at location <code>X4</code> every time the loop runs, which is different than non-volatile behaviour. This time the compiler things the value at <code>X4</code> will be different each time it is read. So without any assumption, it adds the value to <code>sum</code> every time the loop runs.</p> <p>In both programs the value at the location <code>0xc000</code> can be cached by the processor. The subsequent read of the value at <code>0xc000</code> could be from processor's cache but not from main memory. It is responsibility of the memory controller to maintain coherency between memory and processor cache. The <code>volatile</code> keyword has nothing to do here.</p> <p>I believe these simple programs had explained the concept clear. The <code>volatile</code></p>","tags":["c","gcc","compiler"]},{"location":"blog/2018/09/08/the-volatile-keyword/#is","title":"IS","text":"<ul> <li>To tell compiler not to make any assumption about the value stored in the variable</li> </ul>","tags":["c","gcc","compiler"]},{"location":"blog/2018/09/08/the-volatile-keyword/#is-not","title":"IS NOT","text":"<ul> <li>To tell the compiler not to use any registers to hold the value</li> <li>To tell the processor not to cache the value</li> </ul>","tags":["c","gcc","compiler"]},{"location":"blog/2018/10/06/64-bit-mainline-kernel-on-raspberry-pi-3/","title":"64-bit Mainline kernel on Raspberry Pi 3","text":"<p>I've struggled a little recently on running vanilla kernel on Raspberry Pi 3. Still I didn't completely understand the internals. Anyway sharing the steps may be useful for someone like me.</p> <p>Download toolchain and rootfs from Linaro.</p> <p>And clone following repos  * Vanilla kernel  * Raspberry Pi kernel - Checkout same version as vanilla kernel you are going to use  * Raspberry pi firmware - Or download only the files under boot directory of this repo</p> <p>I've created a directory structure as below. You can have similar one based on your convenience.</p> <p><pre><code>$ ls ~/Desktop/kernel/\ntotal 44K\ndrwxr-xr-x  2 kaba kaba 4.0K Sep 23 20:04 downloads\ndrwxrwxr-x  2 kaba kaba 4.0K Oct  4 10:22 firmware\ndrwxr-xr-x 22 kaba kaba 4.0K Oct  6 11:55 kernel_out\ndrwxr-xr-x 18 kaba kaba 4.0K Sep 12  2013 rootfs\ndrwxr-xr-x 26 kaba kaba 4.0K Oct  3 21:30 rpi_kernel\ndrwxr-xr-x  2 kaba kaba 4.0K Oct  7 12:13 rpi_out\ndrwxr-xr-x  3 kaba kaba 4.0K Sep 23 19:43 toolchain\ndrwxr-xr-x 26 kaba kaba 4.0K Oct  3 22:04 vanila_kernel\nkaba@kaba-Vostro-1550:~/Desktop/kernel\n$\n</code></pre> Directory       | Purpose                                                   | ----------------|-----------------------------------------------------------| downloads       | Having tarballs of rootfs and toolchain                   | firmware        | boot directory of Raspberry Pi firmware repo              | kernel_out      | Output directory for Mainline kernel                      | rootfs          | rootfs tarball extracted                                  | rpi_kernel      | Raspberry Pi kernel repo                                  | rpi_out         | Output directory for Raspberry Pi kernel                  | toolchain       | toolchain tarball extracted                               | vanilla_kernel  | Mainline kernel repo                                      |</p> <p>Export <code>PATH</code> variable to include toolchain directory. <pre><code>$ echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/kaba/Desktop/kernel/toolchain/gcc-linaro-7.3.1-2018.05-x86_64_aarch64-linux-gnu/bin/\nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$\n</code></pre></p> <p>Configure and build 64-bit Vanilla kernel. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ make O=../kernel_out ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- defconfig\n.\n.\n.\nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ make -j4 O=../kernel_out ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu-\n</code></pre> Change the suffix to <code>-j</code> according to your machine. And wait for the build to complete.</p> <p>Now build device-tree in Raspberry Pi kernel repo. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/kernel/rpi_kernel\n$ make O=../rpi_out ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- defconfig\nmake[1]: Entering directory '/home/kaba/Desktop/kernel/rpi_out'\n  HOSTCC  scripts/basic/fixdep\n  GEN     ./Makefile\n  HOSTCC  scripts/kconfig/conf.o\n  YACC    scripts/kconfig/zconf.tab.c\n  LEX     scripts/kconfig/zconf.lex.c\n  HOSTCC  scripts/kconfig/zconf.tab.o\n  HOSTLD  scripts/kconfig/conf\n*** Default configuration is based on 'defconfig'\n#\n# configuration written to .config\n#\nmake[1]: Leaving directory '/home/kaba/Desktop/kernel/rpi_out'\nkaba@kaba-Vostro-1550:~/Desktop/kernel/rpi_kernel\n$ make O=../rpi_out ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- dtbs\n.\n.\n.\n</code></pre></p> <p>Partition your memory card into two. The first one should be FAT32 and second one should be EXT4. The first partition should be a boot partition. <pre><code>balakumaran@balakumaran-USB:~/Desktop/RPi/linux_build$ sudo parted /dev/sdd\n[sudo] password for balakumaran: \nGNU Parted 3.2\nUsing /dev/sdd\nWelcome to GNU Parted! Type 'help' to view a list of commands.\n(parted) print                                                            \nModel: MXT-USB Storage Device (scsi)\nDisk /dev/sdd: 31.9GB\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start   End     Size    Type     File system  Flags\n 1      1049kB  106MB   105MB   primary  fat32        boot, lba\n 2      106MB   31.9GB  31.8GB  primary\n\n(parted) help rm                                                          \n  rm NUMBER                                delete partition NUMBER\n\n        NUMBER is the partition number used by Linux.  On MS-DOS disk labels, the primary partitions number from 1 to 4, logical partitions from 5 onwards.\n(parted) rm 1                                                             \n(parted) rm 2                                                             \n(parted) print\nModel: MXT-USB Storage Device (scsi)\nDisk /dev/sdd: 31.9GB\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start  End  Size  Type  File system  Flags\n\n(parted) help mkpart\n  mkpart PART-TYPE [FS-TYPE] START END     make a partition\n\n        PART-TYPE is one of: primary, logical, extended\n        FS-TYPE is one of: zfs, btrfs, nilfs2, ext4, ext3, ext2, fat32, fat16, hfsx, hfs+, hfs, jfs, swsusp, linux-swap(v1), linux-swap(v0), ntfs, reiserfs, freebsd-ufs, hp-ufs, sun-ufs,\n        xfs, apfs2, apfs1, asfs, amufs5, amufs4, amufs3, amufs2, amufs1, amufs0, amufs, affs7, affs6, affs5, affs4, affs3, affs2, affs1, affs0, linux-swap, linux-swap(new), linux-swap(old)\n        START and END are disk locations, such as 4GB or 10%.  Negative values count from the end of the disk.  For example, -1s specifies exactly the last sector.\n\n        'mkpart' makes a partition without creating a new file system on the partition.  FS-TYPE may be specified to set an appropriate partition ID.\n(parted) mkpart primary fat32 2048s 206848s\n(parted) mkpart primary ext4 208896s -1s\n(parted) print                                                            \nModel: MXT-USB Storage Device (scsi)\nDisk /dev/sdd: 31.9GB\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start   End     Size    Type     File system  Flags\n 1      1049kB  106MB   105MB   primary  fat32        lba\n 2      107MB   31.9GB  31.8GB  primary  ext4         lba\n\n(parted) set 1 boot on                                                    \n(parted) print                                                            \nModel: MXT-USB Storage Device (scsi)\nDisk /dev/sdd: 31.9GB\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start   End     Size    Type     File system  Flags\n 1      1049kB  106MB   105MB   primary  fat32        boot, lba\n 2      107MB   31.9GB  31.8GB  primary  ext4         lba\n\n(parted) quit                                                             \nInformation: You may need to update /etc/fstab.\n\nbalakumaran@balakumaran-USB:~/Desktop/RPi/linux_build$ sudo fdisk -l /dev/sdd\nDisk /dev/sdd: 29.7 GiB, 31914983424 bytes, 62333952 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xcde890ba\n\nDevice     Boot  Start      End  Sectors  Size Id Type\n/dev/sdd1  *      2048   206848   204801  100M  c W95 FAT32 (LBA)\n/dev/sdd2       208896 62333951 62125056 29.6G 83 Linux\nbalakumaran@balakumaran-USB:~/Desktop/RPi/linux_build$\n</code></pre></p> <p>Copy firmware and kernel to boot partition. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ sudo mount /dev/sdb1 /mnt/boot/\nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ sudo cp ../kernel_out/arch/arm64/boot/Image /mnt/boot/kernel8.img\n[sudo] password for kaba: \nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ sudo cp ../firmware/* /mnt/boot/\nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$\n</code></pre></p> <p>Install device-tree blobs from Raspberry Pi repo into boot partition. The device-tree in upstream kernel is not working for some reason. I couldn't get more information regarding that. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/kernel/rpi_kernel\n$ make O=../rpi_out ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- bcmrpi3_defconfig\nkaba@kaba-Vostro-1550:~/Desktop/kernel/rpi_kernel\n$ sudo make O=../rpi_out ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- INSTALL_PATH=/mnt/boot/ dtbs_install\n</code></pre></p> <p>Copy rootfs into second partition. Also install kernel modules into that. <pre><code>kaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ sudo mount /dev/sdb2 /mnt/rootfs/\nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ sudo cp -rf ../rootfs/* /mnt/rootfs/\nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ sudo make O=../kernel_out ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- INSTALL_MOD_PATH=/mnt/rootfs/ modules_install\nkaba@kaba-Vostro-1550:~/Desktop/kernel/vanila_kernel\n$ sync\n</code></pre></p> <p>Create <code>config.txt</code> and <code>cmdline.txt</code> as follows. Make sure you update <code>device-tree</code> and <code>overlay_prefix</code> based on your configuration. <pre><code>kaba@kaba-Vostro-1550:/mnt/boot\n$ cat cmdline.txt \ndwc_otg.lpm_enable=0 console=serial0,115200 root=/dev/mmcblk0p2 rootfstype=ext4 rootwait    \nkaba@kaba-Vostro-1550:/mnt/boot\n$ cat config.txt \ndtoverlay=vc4-fkms-v3d,cma-256\ndisable_overscan=1\ndtparam=audio=on\ndevice_tree=dtbs/4.19.0-rc5-v8+/broadcom/bcm2710-rpi-3-b.dtb\noverlay_prefix=dtbs/4.19.0-rc5-v8+/overlays/\nenable_uart=1\nkaba@kaba-Vostro-1550:/mnt/boot\n$\n</code></pre></p> <p>Put the SD card in Raspberry Pi 3 and boot.</p>","tags":["raspberrypi","linux","kernel"]},{"location":"blog/2019/03/30/custom-build-kernel-for-raspberry-pi/","title":"Custom build kernel for Raspberry Pi","text":"<p>I've already written a post about how to cross-compile mainline kernel for Raspberry Pi. In this post I'm covering how to cross-compile Raspberry Pi Linux. This will be simple and straight forward. I may write a series of posts related to kernel debugging, optimization which will be based on Raspberry Pi kernel. So this post will be starting point for them.</p> <p>Directory structure, <pre><code>balakumaran@balakumaran-pc:~/Desktop/RPi$ ls -lh\ntotal 32K\ndrwxrwxr-x  3 balakumaran balakumaran 4.0K Mar  9 19:33 firmware\ndrwxr-xr-x  8 balakumaran balakumaran 4.0K Jan 23 01:52 gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu\ndrwxrwxr-x 22 balakumaran balakumaran 4.0K Mar 30 18:38 kernel_out\ndrwxrwxr-x 26 balakumaran balakumaran 4.0K Mar 30 18:13 linux-rpi-4.14.y\ndrwxrwxr-x 18 balakumaran balakumaran 4.0K Mar  9 19:34 rootfs\nbalakumaran@balakumaran-pc:~/Desktop/RPi$\n</code></pre> Directory       | Purpose                                                   | ----------------|-----------------------------------------------------------| gcc-li...       | GCC cross compiler from Linaro. Extracted                 | firmware/boot   | boot directory of Raspberry Pi firmware repo              | kernel_out      | Output directory for Raspberry kernel                     | rootfs          | rootfs from Linaro. Extracted                             | linux-rpi...    | Raspberry Pi kernel repo                                  |</p> <p>Used Ubuntu image rootfs from Linaro.</p>","tags":["raspberrypi","linux","kernel"]},{"location":"blog/2019/03/30/custom-build-kernel-for-raspberry-pi/#prepare-sd-card","title":"Prepare SD card","text":"<p>Make two partition as follows, <pre><code>balakumaran@balakumaran-pc:~/Desktop/RPi$ sudo fdisk -l /dev/sdc\n[sudo] password for balakumaran:\nDisk /dev/sdc: 29.7 GiB, 31914983424 bytes, 62333952 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xcde890ba\n\nDevice     Boot  Start      End  Sectors  Size Id Type\n/dev/sdc1  *      2048   133119   131072   64M  b W95 FAT32\n/dev/sdc2       133120 62333951 62200832 29.7G 83 Linux\nbalakumaran@balakumaran-pc:~/Desktop/RPi$\n</code></pre> Complete steps on how to do this is available in Appendix.</p> <p>Copy necessary files <pre><code>balakumaran@balakumaran-pc:/media/balakumaran$ sudo mount /dev/sdc1 /mnt/boot/\nbalakumaran@balakumaran-pc:/media/balakumaran$ sudo mount /dev/sdc2 /mnt/rootfs/\nbalakumaran@balakumaran-pc:~/Desktop$ sudo cp -rf ~/Desktop/RPi/firmware/boot/* /mnt/boot/\n[sudo] password for balakumaran:\nbalakumaran@balakumaran-pc:~/Desktop$\nbalakumaran@balakumaran-pc:~/Desktop$ sudo cp -rf ~/Desktop/RPi/rootfs/* /mnt/rootfs/\nbalakumaran@balakumaran-pc:~/Desktop$\n</code></pre></p>","tags":["raspberrypi","linux","kernel"]},{"location":"blog/2019/03/30/custom-build-kernel-for-raspberry-pi/#build-and-install-kernel","title":"Build and Install kernel","text":"<p>Unless you are ready for the pain, use stable kernel release.</p> <p>Setup following environmental variables, <pre><code>balakumaran@balakumaran-pc:~/Desktop/RPi$ source ~/setup_arm64_build.sh\nbalakumaran@balakumaran-pc:~/Desktop/RPi$ echo $CROSS_COMPILE\naarch64-linux-gnu-\nbalakumaran@balakumaran-pc:~/Desktop/RPi$ echo $ARCH\narm64\nbalakumaran@balakumaran-pc:~/Desktop/RPi$ echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/balakumaran/Desktop/RPi/gcc-linaro-7.4.1-2019.02-x86_64_aarch64-linux-gnu/bin/\nbalakumaran@balakumaran-pc:~/Desktop/RPi$\n</code></pre></p> <p>Cross compile Kernel, Device-tree, modules. <pre><code>balakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ time make ARCH=arm64 O=../kernel_out/ bcmrpi3_defconfig\nmake[1]: Entering directory '/home/balakumaran/Desktop/RPi/kernel_out'\n.\n.\n.\n\nbalakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ time make -j8  ARCH=arm64 O=../kernel_out/\nmake[1]: Entering directory '/home/balakumaran/Desktop/RPi/kernel_out'\n.\n.\n.\n\nbalakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ make ARCH=arm64 O=../kernel_out/ dtbs\nmake[1]: Entering directory '/home/balakumaran/Desktop/RPi/kernel_out'\n.\n.\n.\n\nbalakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ sudo cp ../kernel_out/arch/arm64/boot/Image /mnt/boot/kernel8.img\n[sudo] password for balakumaran:\nbalakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ sudo make  ARCH=arm64 O=../kernel_out/ INSTALL_PATH=/mnt/boot/ dtbs_install\nmake[1]: Entering directory '/home/balakumaran/Desktop/RPi/kernel_out'\narch/arm64/Makefile:27: ld does not support --fix-cortex-a53-843419; kernel may be susceptible to erratum\narch/arm64/Makefile:40: LSE atomics not supported by binutils\narch/arm64/Makefile:48: Detected assembler with broken .inst; disassembly will be unreliable\nmake[3]: Nothing to be done for '__dtbs_install'.\n  INSTALL arch/arm64/boot/dts/al/alpine-v2-evp.dtb\n.\n.\n.\n</code></pre></p> <p>Create <code>cmdline.txt</code> and <code>config.txt</code>. <pre><code>balakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ cat /mnt/boot/cmdline.txt\ndwc_otg.lpm_enable=0 console=serial0,115200 root=/dev/mmcblk0p2 rootfstype=ext4 rootwait\nbalakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ cat /mnt/boot/config.txt\ndtoverlay=pi3-disable-bt\ndisable_overscan=1\ndtparam=audio=on\ndevice_tree=dtbs/4.14.98-v8+/broadcom/bcm2710-rpi-3-b.dtb\noverlay_prefix=dtbs/4.14.98-v8+/overlays/\nenable_uart=1\nbalakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$\n</code></pre></p>","tags":["raspberrypi","linux","kernel"]},{"location":"blog/2019/03/30/custom-build-kernel-for-raspberry-pi/#prepare-rootfs","title":"Prepare rootfs","text":"<p>I'm going to use ubuntu-base images with some additional modification as rootfs here. Find ubuntu-base releases at http://cdimage.ubuntu.com/ubuntu-base/releases/. Latest stable is always better. Download and extract ubuntu-base rootfs. Install kernel modules into the rootfs extracted.</p> <pre><code>balakumaran@balakumaran-pc:~/Desktop/RPi/linux-rpi-4.14.y$ sudo make  ARCH=arm64 O=../kernel_out/ INSTALL_MOD_PATH=$HOME/ubuntu-base/ modules_install\n[sudo] password for balakumaran:\nmake[1]: Entering directory '/home/balakumaran/Desktop/RPi/kernel_out'\narch/arm64/Makefile:27: ld does not support --fix-cortex-a53-843419; kernel may be susceptible to erratum\narch/arm64/Makefile:40: LSE atomics not supported by binutils\narch/arm64/Makefile:48: Detected assembler with broken .inst; disassembly will be unreliable\n  INSTALL arch/arm64/crypto/aes-neon-blk.ko\n.\n.\n.\n</code></pre> <p>Copy your resolv.conf for network access. <pre><code>$ sudo cp -av /run/systemd/resolve/stub-resolv.conf $HOME/rootfs/etc/resolv.conf\n</code></pre></p> <p>Lets <code>chroot</code> into the new rootfs and install necessary packages. But its an arm64 rootfs. So you need <code>qemu</code> user-mode emulation. Install <code>qemu-user-static</code> in your host Ubuntu and copy that to new rootfs. And then chroot will work. <pre><code>$ sudo apt install qemu-user-static\n.\n.\n.\n\n$ sudo cp /usr/bin/qemu-aarch64-static $HOME/rootfs/usr/bin/\n$ sudo chroot $HOME/rootfs/\n</code></pre></p> <p>Change <code>root</code> user password and install necessary packages. As these binaries are running on emulator, they will be bit slower. Its just one time. <pre><code>$ passwd root\n$ apt-get update\n$ apt-get upgrade\n$ apt-get install sudo ifupdown net-tools ethtool udev wireless-tools iputils-ping resolvconf wget apt-utils wpasupplicant kmod systemd vim\n</code></pre></p> <p>NOTE: If you face any error like <code>cannot create key file at /tmp/</code>, change permission of <code>tmp</code>. <pre><code>$ chmod 777 /tmp\n</code></pre></p> <p>Download raspberry firmware-nonfree package from raspberry repository, extract wireless firmware and copy it to rootfs. Refer this answer for more details. As I'm having a RPI3b board, I copied <code>brcmfmac43430-sdio.bin</code> and <code>brcmfmac43430-sdio.txt</code> to <code>lib/firmware/brcm</code> <pre><code>$ mkdir -p $HOME/lib/modules/brcm/\n$ cp brcmfmac43430-sdio.txt brcmfmac43430-sdio.bin $HOME/lib/modules/brcm/\n</code></pre></p> <p>Edit <code>etc/fstab</code> or rootfs will be mounted as read-only. <pre><code>echo \"/dev/mmcblk0p2    /   ext4    defaults,noatime    0   1\" &gt;&gt; $HOME/rootfs/etc/fstab\n</code></pre> I referred this link for rootfs preparation. Though I'm not using, there are steps to remove unwanted files explained.</p>","tags":["raspberrypi","linux","kernel"]},{"location":"blog/2019/03/30/custom-build-kernel-for-raspberry-pi/#reference","title":"Reference","text":"<ul> <li>https://a-delacruz.github.io/ubuntu/rpi3-setup-64bit-kernel</li> <li>https://a-delacruz.github.io/ubuntu/rpi3-setup-filesystem.html</li> <li>https://www.linuxquestions.org/questions/slackware-arm-108/raspberry-pi-3-b-wifi-nic-not-found-4175627137/#post5840054</li> <li>http://cdimage.ubuntu.com/ubuntu-base/releases/</li> <li>https://raspberrypi.stackexchange.com/questions/61319/how-to-add-wifi-drivers-in-custom-kernel</li> </ul>","tags":["raspberrypi","linux","kernel"]},{"location":"blog/2019/03/30/custom-build-kernel-for-raspberry-pi/#appendix","title":"Appendix","text":"<pre><code>Command (m for help): p\nDisk /dev/sdc: 29.7 GiB, 31914983424 bytes, 62333952 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xcde890ba\n\nDevice     Boot  Start      End  Sectors  Size Id Type\n/dev/sdc1  *      2048   133119   131072   64M  b W95 FAT32\n/dev/sdc2       133120 62333951 62200832 29.7G 83 Linux\n\nCommand (m for help): d\nPartition number (1,2, default 2): 2\n\nPartition 2 has been deleted.\n\nCommand (m for help): d\nSelected partition 1\nPartition 1 has been deleted.\n\nCommand (m for help): p\nDisk /dev/sdc: 29.7 GiB, 31914983424 bytes, 62333952 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xcde890ba\n\nCommand (m for help): n\nPartition type\n   p   primary (0 primary, 0 extended, 4 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (1-4, default 1):\nFirst sector (2048-62333951, default 2048):\nLast sector, +sectors or +size{K,M,G,T,P} (2048-62333951, default 62333951): +64M\n\nCreated a new partition 1 of type 'Linux' and of size 64 MiB.\nPartition #1 contains a vfat signature.\n\nDo you want to remove the signature? [Y]es/[N]o: Y\n\nThe signature will be removed by a write command.\n\nCommand (m for help): n\nPartition type\n   p   primary (1 primary, 0 extended, 3 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (2-4, default 2):\nFirst sector (133120-62333951, default 133120):\nLast sector, +sectors or +size{K,M,G,T,P} (133120-62333951, default 62333951):\n\nCreated a new partition 2 of type 'Linux' and of size 29.7 GiB.\nPartition #2 contains a ext4 signature.\n\nDo you want to remove the signature? [Y]es/[N]o: Y\n\nThe signature will be removed by a write command.\n\nCommand (m for help): t\nPartition number (1,2, default 2): 1\nHex code (type L to list all codes): L\n\n 0  Empty           24  NEC DOS         81  Minix / old Lin bf  Solaris\n 1  FAT12           27  Hidden NTFS Win 82  Linux swap / So c1  DRDOS/sec (FAT-\n 2  XENIX root      39  Plan 9          83  Linux           c4  DRDOS/sec (FAT-\n 3  XENIX usr       3c  PartitionMagic  84  OS/2 hidden or  c6  DRDOS/sec (FAT-\n 4  FAT16 &lt;32M      40  Venix 80286     85  Linux extended  c7  Syrinx\n 5  Extended        41  PPC PReP Boot   86  NTFS volume set da  Non-FS data\n 6  FAT16           42  SFS             87  NTFS volume set db  CP/M / CTOS / .\n 7  HPFS/NTFS/exFAT 4d  QNX4.x          88  Linux plaintext de  Dell Utility\n 8  AIX             4e  QNX4.x 2nd part 8e  Linux LVM       df  BootIt\n 9  AIX bootable    4f  QNX4.x 3rd part 93  Amoeba          e1  DOS access\n a  OS/2 Boot Manag 50  OnTrack DM      94  Amoeba BBT      e3  DOS R/O\n b  W95 FAT32       51  OnTrack DM6 Aux 9f  BSD/OS          e4  SpeedStor\n c  W95 FAT32 (LBA) 52  CP/M            a0  IBM Thinkpad hi ea  Rufus alignment\n e  W95 FAT16 (LBA) 53  OnTrack DM6 Aux a5  FreeBSD         eb  BeOS fs\n f  W95 Ext'd (LBA) 54  OnTrackDM6      a6  OpenBSD         ee  GPT\n10  OPUS            55  EZ-Drive        a7  NeXTSTEP        ef  EFI (FAT-12/16/\n11  Hidden FAT12    56  Golden Bow      a8  Darwin UFS      f0  Linux/PA-RISC b\n12  Compaq diagnost 5c  Priam Edisk     a9  NetBSD          f1  SpeedStor\n14  Hidden FAT16 &lt;3 61  SpeedStor       ab  Darwin boot     f4  SpeedStor\n16  Hidden FAT16    63  GNU HURD or Sys af  HFS / HFS+      f2  DOS secondary\n17  Hidden HPFS/NTF 64  Novell Netware  b7  BSDI fs         fb  VMware VMFS\n18  AST SmartSleep  65  Novell Netware  b8  BSDI swap       fc  VMware VMKCORE\n1b  Hidden W95 FAT3 70  DiskSecure Mult bb  Boot Wizard hid fd  Linux raid auto\n1c  Hidden W95 FAT3 75  PC/IX           bc  Acronis FAT32 L fe  LANstep\n1e  Hidden W95 FAT1 80  Old Minix       be  Solaris boot    ff  BBT\nHex code (type L to list all codes): b\n\nChanged type of partition 'Linux' to 'W95 FAT32'.\n\nCommand (m for help): t\nPartition number (1,2, default 2): 2\nHex code (type L to list all codes): 83\n\nChanged type of partition 'Linux' to 'Linux'.\n\nCommand (m for help): w\nThe partition table has been altered.\nCalling ioctl() to re-read partition table.\nSyncing disks.\n\nbalakumaran@balakumaran-pc:/media/balakumaran$\nbalakumaran@balakumaran-pc:/media/balakumaran$ sudo fdisk -l /dev/sdc\nDisk /dev/sdc: 29.7 GiB, 31914983424 bytes, 62333952 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xcde890ba\n\nDevice     Boot  Start      End  Sectors  Size Id Type\n/dev/sdc1         2048   133119   131072   64M  b W95 FAT32\n/dev/sdc2       133120 62333951 62200832 29.7G 83 Linux\nbalakumaran@balakumaran-pc:/media/balakumaran$\nbalakumaran@balakumaran-pc:/media/balakumaran$ sudo mkfs.fat /dev/sdc1\nmkfs.fat 4.1 (2017-01-24)\nbalakumaran@balakumaran-pc:/media/balakumaran$ sudo mkfs.ext4 /dev/sdc2\nmke2fs 1.44.4 (18-Aug-2018)\nCreating filesystem with 7775104 4k blocks and 1945888 inodes\nFilesystem UUID: 5815d093-6381-4db7-b692-32192b24cf9c\nSuperblock backups stored on blocks:\n        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,\n        4096000\n\nAllocating group tables: done\nWriting inode tables: done\nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done\n\nbalakumaran@balakumaran-pc:/media/balakumaran$\nbalakumaran@balakumaran-pc:/media/balakumaran$ sudo fdisk /dev/sdc\n\nWelcome to fdisk (util-linux 2.32).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\n\n\nCommand (m for help): a\nPartition number (1,2, default 2): 1\n\nThe bootable flag on partition 1 is enabled now.\n\nCommand (m for help): p\nDisk /dev/sdc: 29.7 GiB, 31914983424 bytes, 62333952 sectors\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0xcde890ba\n\nDevice     Boot  Start      End  Sectors  Size Id Type\n/dev/sdc1  *      2048   133119   131072   64M  b W95 FAT32\n/dev/sdc2       133120 62333951 62200832 29.7G 83 Linux\n\nCommand (m for help): w\nThe partition table has been altered.\nCalling ioctl() to re-read partition table.\nSyncing disks.\n\nbalakumaran@balakumaran-pc:/media/balakumaran$\n</code></pre>","tags":["raspberrypi","linux","kernel"]},{"location":"blog/2019/06/29/variadic-functions-with-unknown-argument-count/","title":"Variadic functions with unknown argument count","text":"<p>One of my colleagues came across a peculiar problem. She had to write an API that accepts variable number of arguments, but number of arguments won't be passed in the arguments list. She cracked it intelligently with following hack.</p>","tags":["programming","C"]},{"location":"blog/2019/06/29/variadic-functions-with-unknown-argument-count/#the-hack","title":"The Hack","text":"<p>Heart of this hack is a macro that can count the number of arguments passed to it. It has a limitation. Maximum number of arguments can be passed to this macro should be known. For example, if maximum number of arguments can be passed is 5, the macro will look like,</p> <pre><code>#define COUNT5(...) _COUNT5(__VA_ARGS__, 5, 4, 3, 2, 1)\n#define _COUNT5(a, b, c, d, e, count, ...) count\n</code></pre> <p>If you want your macro to count 10 or lesser arguments,</p> <pre><code>#define COUNT10(...) _COUNT10(__VA_ARGS__, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1)\n#define _COUNT10(a, b, c, d, e, f, g, h, i, j, count, ...) count\n</code></pre> <p>Let me explain it. Consider below macro call. It will expand like this.</p> <pre><code>COUNT5(99, 98, 97);\n  |\n  |\n  V\n_COUNT5(99, 98, 97, 5, 4, 3, 2, 1)\n  |\n  |\n  V\n  3\n</code></pre> <p>The three arguments passed to <code>COUNT5</code> will occupy <code>a</code>, <code>b</code>, <code>c</code> of <code>_COUNT5</code>. <code>5</code> and <code>4</code> will occupy <code>d</code>, <code>e</code>. Next argument <code>3</code> will be in the place of <code>count</code>, that will be returned.</p>","tags":["programming","C"]},{"location":"blog/2019/06/29/variadic-functions-with-unknown-argument-count/#final-solution","title":"Final solution","text":"<p>So she exposed a macro that accepts variable number of arguments as the API requested. This macro internally used the <code>COUNTX</code> macro to get number of arguments passed. And she passed the <code>count</code> and variable arguments to the actual <code>C</code> function. </p>","tags":["programming","C"]},{"location":"blog/2019/06/29/variadic-functions-with-unknown-argument-count/#example","title":"Example","text":"<p>A small <code>C</code> program using this hack.</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdarg.h&gt;\n#include &lt;stdlib.h&gt;\n\nint _sum(int count, ...);\n\n#define COUNT(...) _COUNT(__VA_ARGS__, 5, 4, 3, 2, 1)\n#define _COUNT(a, b, c, d, e, count, ...) count\n\n#define sum(...) _sum(COUNT(__VA_ARGS__), __VA_ARGS__)\n\nint _sum(int count, ...) {\n    va_list arg_ptr;\n    int     sum = 0;\n    int     i = 0;\n\n    va_start(arg_ptr, count);\n\n    for (i = 0; i &lt; count; i++) {\n        sum += va_arg(arg_ptr, int);\n    }\n\n    return sum;\n}\n\nint main() {\n    printf(\"%d\\n\", sum(1, 2, 3, 4, 5));\n    printf(\"%d\\n\", sum(1, 2, 3));\n    printf(\"%d\\n\", sum(1));\n    printf(\"%d\\n\", sum(2, 2, 2, 2, 2));\n\n    return 0;\n}\n</code></pre> <p>And its output.</p> <pre><code>kaba@kaba-Vostro-1550:~/variadic\n$ gcc variadic.c\nkaba@kaba-Vostro-1550:~/variadic\n$ ./a.out\n15\n6\n1\n10\nkaba@kaba-Vostro-1550:~/variadic\n$\n</code></pre>","tags":["programming","C"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/","title":"kexec - A travel to the purgatory","text":"<p>This is one of the unforgettable experience in my engineering life. Last year when we brought-up a ARM64 based platform, we faced lots of hurdles. But this one was very interesting and had lots of surprises. Though this triaging effort tolled multiple frustrating days, I had very good learning. I had to understand <code>kexec_load</code> system call, kernel reboot path and kernel early boot path to root cause the issue. We were using 4.14.19 kernel for the bring-up. But I'll give code samples from 5.4.14 as it is latest. There is no much code difference.</p> <p>The boot flow of our new platform was <code>uboot --&gt; service Linux --&gt; main Linux</code>. We had <code>service Linux</code> to offload major hardware initialization work from the bootloader. Also to keep that code platform agnostic [same <code>service Linux</code> ran on x86 platforms too].</p> <p>To make the jump from <code>service Linux</code> to <code>main Linux</code> we used <code>kexec</code>. The problem here was it took almost 2 minutes for the <code>main Linux</code> to start booting. This was a significant difference while comparing with x86 platforms. And this kind of delay would definitely annoy our customers.</p> <p>After hundreds of rebuilds and thousands of print statements, I found the cause and fixed the issue. Now that 2 minutes delay was reduced to 4 seconds. Let me just tell you the code flow rather dump you with my debugging methods. I kept deferring from writing this article for almost an year. Because I was afraid of the code references and connecting them to show the flow.</p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#code-reference","title":"Code reference","text":"package version repo kexec-tools today's latest master 2c9f26ed20a791a7df0182ba82e93abb52f5a615 https://github.com/horms/kexec-tools linux 5.4.14 https://elixir.bootlin.com/linux/v5.4.14/source NOTE: I copied code of entire functions for reference. But explained only necessary lines for better understanding.","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#user-story","title":"User story","text":"<p>Login to the <code>service Linux</code>. Load <code>main Linux</code>'s kernel and initrd. The current <code>device-tree</code> will be taken for <code>main Linux</code> too. Boot to <code>main Linux</code>.</p> <pre><code># kexec -l /main/vmlinuz --initrd=/main/initrd.img\n# kexec -e\n</code></pre> <p>As I mentioned earlier there was 2 mins delay between last <code>Bye</code> from <code>service Linux</code> and first message from <code>main Linux</code>. So I started looking for time consuming operations between those two prints.</p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#kexec-tools-kexec-e","title":"kexec-tools <code>kexec -e</code>","text":"<p><code>kexec -e</code> calls <code>reboot()</code> system call with <code>LINUX_REBOOT_CMD_KEXEC</code> as argument. <pre><code>kexec/kexec.c my_exec +900:\n---\nreboot(LINUX_REBOOT_CMD_KEXEC);\n</code></pre> </p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#kernel-reboot-path","title":"Kernel reboot path","text":"","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#reboot-system-call","title":"reboot system call","text":"<p>Reboot system call calls <code>kernel_kexec()</code> for the argument <code>LINUX_REBOOT_CMD_KEXEC</code>. You can refer to my earlier article Anatomy of Linux system call in ARM64 to understand how arguments are passed from user space to kernel space. <pre><code>kernel/reboot.c SYSCALL_DEFINE4(reboot, ...) +380:\n---\n#ifdef CONFIG_KEXEC_CORE\n    case LINUX_REBOOT_CMD_KEXEC:\n        ret = kernel_kexec();\n        break;\n#endif\n</code></pre></p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#kernel_kexec","title":"kernel_kexec()","text":"<p><code>kernel_kexec()</code> at <code>kernel/kexec_core.c +1119</code> does following things. <pre><code>kernel/kexec_core.c +1119\n---\n/*\n * Move into place and start executing a preloaded standalone\n * executable.  If nothing was preloaded return an error.\n */\nint kernel_kexec(void)\n{\n    int error = 0;\n\n    if (!mutex_trylock(&amp;kexec_mutex))\n        return -EBUSY;\n    if (!kexec_image) {\n        error = -EINVAL;\n        goto Unlock;\n    }\n\n#ifdef CONFIG_KEXEC_JUMP\n    if (kexec_image-&gt;preserve_context) {\n        lock_system_sleep();\n        pm_prepare_console();\n        error = freeze_processes();\n        if (error) {\n            error = -EBUSY;\n            goto Restore_console;\n        }\n        suspend_console();\n        error = dpm_suspend_start(PMSG_FREEZE);\n        if (error)\n            goto Resume_console;\n        /* At this point, dpm_suspend_start() has been called,\n         * but *not* dpm_suspend_end(). We *must* call\n         * dpm_suspend_end() now.  Otherwise, drivers for\n         * some devices (e.g. interrupt controllers) become\n         * desynchronized with the actual state of the\n         * hardware at resume time, and evil weirdness ensues.\n         */\n        error = dpm_suspend_end(PMSG_FREEZE);\n        if (error)\n            goto Resume_devices;\n        error = suspend_disable_secondary_cpus();\n        if (error)\n            goto Enable_cpus;\n        local_irq_disable();\n        error = syscore_suspend();\n        if (error)\n            goto Enable_irqs;\n    } else\n#endif\n    {\n        kexec_in_progress = true;\n        kernel_restart_prepare(NULL);\n        migrate_to_reboot_cpu();\n\n        /*\n         * migrate_to_reboot_cpu() disables CPU hotplug assuming that\n         * no further code needs to use CPU hotplug (which is true in\n         * the reboot case). However, the kexec path depends on using\n         * CPU hotplug again; so re-enable it here.\n         */\n        cpu_hotplug_enable();\n        pr_emerg(\"Starting new kernel\\n\");\n        machine_shutdown();\n    }\n\n    machine_kexec(kexec_image);\n\n#ifdef CONFIG_KEXEC_JUMP\n    if (kexec_image-&gt;preserve_context) {\n        syscore_resume();\n Enable_irqs:\n        local_irq_enable();\n Enable_cpus:\n        suspend_enable_secondary_cpus();\n        dpm_resume_start(PMSG_RESTORE);\n Resume_devices:\n        dpm_resume_end(PMSG_RESTORE);\n Resume_console:\n        resume_console();\n        thaw_processes();\n Restore_console:\n        pm_restore_console();\n        unlock_system_sleep();\n    }\n#endif\n\n Unlock:\n    mutex_unlock(&amp;kexec_mutex);\n    return error;\n}\n</code></pre></p> line code explanation 1123 <code>mutex_trylock(&amp;kexec_mutex)</code> This lock is held to avoid multiple entrance into kexec 1131 <code>if (kexec_image-&gt;preserve_context) {</code> Something related to keep the device status not disturbed during kexec. We were not using it. Moving to else part. 1165 <code>migrate_to_reboot_cpu()</code> Continue execution from logical CPU 0. Only one control path is valid during reboot and startup. That will be executed from CPU-0. 1175 <code>machine_shutdown()</code> Don't get confused by the name of this function. Its just a wrapper around <code>disable_nonboot_cpus()</code> in arm64. It does nothing but disables other CPUs 1178 <code>machine_kexec(kexec_image)</code> Execution continues passing <code>kexec_image</code> as argument. This call should not return.","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#machine_kexec","title":"machine_kexec()","text":"<pre><code>arch/arm64/kernel/machine_kexec.c +144\n---\n/**\n * machine_kexec - Do the kexec reboot.\n *\n * Called from the core kexec code for a sys_reboot with LINUX_REBOOT_CMD_KEXEC.\n */\nvoid machine_kexec(struct kimage *kimage)\n{\n    phys_addr_t reboot_code_buffer_phys;\n    void *reboot_code_buffer;\n    bool in_kexec_crash = (kimage == kexec_crash_image);\n    bool stuck_cpus = cpus_are_stuck_in_kernel();\n\n    /*\n     * New cpus may have become stuck_in_kernel after we loaded the image.\n     */\n    BUG_ON(!in_kexec_crash &amp;&amp; (stuck_cpus || (num_online_cpus() &gt; 1)));\n    WARN(in_kexec_crash &amp;&amp; (stuck_cpus || smp_crash_stop_failed()),\n        \"Some CPUs may be stale, kdump will be unreliable.\\n\");\n\n    reboot_code_buffer_phys = page_to_phys(kimage-&gt;control_code_page);\n    reboot_code_buffer = phys_to_virt(reboot_code_buffer_phys);\n\n    kexec_image_info(kimage);\n\n    pr_debug(\"%s:%d: control_code_page:        %p\\n\", __func__, __LINE__,\n        kimage-&gt;control_code_page);\n    pr_debug(\"%s:%d: reboot_code_buffer_phys:  %pa\\n\", __func__, __LINE__,\n        &amp;reboot_code_buffer_phys);\n    pr_debug(\"%s:%d: reboot_code_buffer:       %p\\n\", __func__, __LINE__,\n        reboot_code_buffer);\n    pr_debug(\"%s:%d: relocate_new_kernel:      %p\\n\", __func__, __LINE__,\n        arm64_relocate_new_kernel);\n    pr_debug(\"%s:%d: relocate_new_kernel_size: 0x%lx(%lu) bytes\\n\",\n        __func__, __LINE__, arm64_relocate_new_kernel_size,\n        arm64_relocate_new_kernel_size);\n\n    /*\n     * Copy arm64_relocate_new_kernel to the reboot_code_buffer for use\n     * after the kernel is shut down.\n     */\n    memcpy(reboot_code_buffer, arm64_relocate_new_kernel,\n        arm64_relocate_new_kernel_size);\n\n    /* Flush the reboot_code_buffer in preparation for its execution. */\n    __flush_dcache_area(reboot_code_buffer, arm64_relocate_new_kernel_size);\n\n    /*\n     * Although we've killed off the secondary CPUs, we don't update\n     * the online mask if we're handling a crash kernel and consequently\n     * need to avoid flush_icache_range(), which will attempt to IPI\n     * the offline CPUs. Therefore, we must use the __* variant here.\n     */\n    __flush_icache_range((uintptr_t)reboot_code_buffer,\n                 arm64_relocate_new_kernel_size);\n\n    /* Flush the kimage list and its buffers. */\n    kexec_list_flush(kimage);\n\n    /* Flush the new image if already in place. */\n    if ((kimage != kexec_crash_image) &amp;&amp; (kimage-&gt;head &amp; IND_DONE))\n        kexec_segment_flush(kimage);\n\n    pr_info(\"Bye!\\n\");\n\n    local_daif_mask();\n\n    /*\n     * cpu_soft_restart will shutdown the MMU, disable data caches, then\n     * transfer control to the reboot_code_buffer which contains a copy of\n     * the arm64_relocate_new_kernel routine.  arm64_relocate_new_kernel\n     * uses physical addressing to relocate the new image to its final\n     * position and transfers control to the image entry point when the\n     * relocation is complete.\n     * In kexec case, kimage-&gt;start points to purgatory assuming that\n     * kernel entry and dtb address are embedded in purgatory by\n     * userspace (kexec-tools).\n     * In kexec_file case, the kernel starts directly without purgatory.\n     */\n    cpu_soft_restart(reboot_code_buffer_phys, kimage-&gt;head, kimage-&gt;start,\n#ifdef CONFIG_KEXEC_FILE\n                        kimage-&gt;arch.dtb_mem);\n#else\n                        0);\n#endif\n\n    BUG(); /* Should never get here. */\n}\n</code></pre> line code explanation 148 <code>bool in_kexec_crash = (kimage == kexec_crash_image)</code> Not true in our case. <code>kimage</code> is <code>kexec_image</code> 149 <code>bool stuck_cpus = cpus_are_stuck_in_kernel();</code> Get number of stuck CPUs. Ideally it should be 0 154 <code>BUG_ON(!in_kexec_crash &amp;&amp; (stuck_cpus || (num_online_cpus() &gt; 1)))</code> In non-crash situations, no CPU should be stuck and no CPU other than reboot CPU should be online. 158 <code>reboot_code_buffer_phys = page_to_phys(kimage-&gt;control_code_page)</code> Get the physical page address from <code>kimage-&gt;control_code_page</code>. <code>arm64_relocate_new_kernel</code> function will be copied to this special location. 159 <code>reboot_code_buffer = phys_to_virt(reboot_code_buffer_phys)</code> Store the virtual address of same to continue working on that area. 179 <code>memcpy(reboot_code_buffer, arm64_relocate_new_kernel, arm64_relocate_new_kernel_size)</code> Copies <code>arm64_relocate_new_kernel_size</code> routines address to the jump location. It is implemented in assembly. This is the routine that copies new kernel to its correct place. To make sure the memory where this routine resides doesn't get overwritten during the copy, it is copied inside <code>kexec_image</code> and executed from there. Never expected right? me too. 183 - 199 <code>__flush_dcache_area(reboot_code_buffer, arm64_relocate_new_kernel_size);</code><code>__flush_icache_range((uintptr_t)reboot_code_buffer, arm64_relocate_new_kernel_size);</code><code>kexec_list_flush(kimage);if ((kimage != kexec_crash_image) &amp;&amp; (kimage-&gt;head &amp; IND_DONE)) kexec_segment_flush(kimage);</code> Flush necessary memory areas 201 <code>pr_info(\"Bye!\\n\")</code> The last print message from current kernel 203 <code>local_daif_mask()</code> Disable all exceptions including interrupts. As we are entering into reboot path, don't expect any normal operations. 217 <code>cpu_soft_restart(reboot_code_buffer_phys, kimage-&gt;head, kimage-&gt;start,0)</code> This call won't return. <code>CONFIG_KEXEC_FILE</code> is not necessary in our case. The comment block above this call explains about purgatory code. But unfortunately that was not written when I was actually debugging this issue.","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#cpu_soft_restart","title":"cpu_soft_restart()","text":"<p>Its just a wrapper around <code>__cpu_soft_restart</code> which is an assembly routine. <code>el2_switch</code> would be set to 0 in our case. <pre><code>arch/arm64/kernel/cpu-reset.h +16\n---\nvoid __cpu_soft_restart(unsigned long el2_switch, unsigned long entry,\n    unsigned long arg0, unsigned long arg1, unsigned long arg2);\n\nstatic inline void __noreturn cpu_soft_restart(unsigned long entry,\n                           unsigned long arg0,\n                           unsigned long arg1,\n                           unsigned long arg2)\n{\n    typeof(__cpu_soft_restart) *restart;\n\n    unsigned long el2_switch = !is_kernel_in_hyp_mode() &amp;&amp;\n        is_hyp_mode_available();\n    restart = (void *)__pa_symbol(__cpu_soft_restart);\n\n    cpu_install_idmap();\n    restart(el2_switch, entry, arg0, arg1, arg2);\n    unreachable();\n}\n</code></pre></p> <p>An important call to note here is <code>cpu_install_idmap()</code>. This comes handy when MMU comes in or goes out. Linux kernel has a text section called <code>idmap.text</code>. <code>cpu_install_idmap()</code> will copy this section to two memory areas. There will be virtual memory mapping for one of these areas. The second area will be literal physical memory location of the virtual address. For example code in this section will be loaded at physical address 0x0 and 0x80000000. And the virtual address corresponding to 0x80000000 will be 0x0. It ensures continuous execution after MMU goes off. I'll write a separate article explaining in detail about this.</p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#__cpu_soft_restart","title":"__cpu_soft_restart()","text":"<pre><code>arch/arm64/kernel/cpu-reset.S +15\n---\n.text\n.pushsection    .idmap.text, \"awx\"\n\n/*\n * __cpu_soft_restart(el2_switch, entry, arg0, arg1, arg2) - Helper for\n * cpu_soft_restart.\n *\n * @el2_switch: Flag to indicate a switch to EL2 is needed.\n * @entry: Location to jump to for soft reset.\n * arg0: First argument passed to @entry. (relocation list)\n * arg1: Second argument passed to @entry.(physical kernel entry)\n * arg2: Third argument passed to @entry. (physical dtb address)\n *\n * Put the CPU into the same state as it would be if it had been reset, and\n * branch to what would be the reset vector. It must be executed with the\n * flat identity mapping.\n */\nENTRY(__cpu_soft_restart)\n    /* Clear sctlr_el1 flags. */\n    mrs x12, sctlr_el1\n    ldr x13, =SCTLR_ELx_FLAGS\n    bic x12, x12, x13\n    pre_disable_mmu_workaround\n    msr sctlr_el1, x12\n    isb\n\n    cbz x0, 1f              // el2_switch?\n    mov x0, #HVC_SOFT_RESTART\n    hvc #0              // no return\n\n1:  mov x18, x1             // entry\n    mov x0, x2              // arg0\n    mov x1, x3              // arg1\n    mov x2, x4              // arg2\n    br  x18\nENDPROC(__cpu_soft_restart)\n\n.popsection\n</code></pre> line instruction explanation 16 <code>.pushsection    .idmap.text, \"awx\"</code> As I mentioned earlier, this routine has to be pushed into <code>idmap.text</code> section. Because it is going to disable MMU. 34-38 <code>mrs     x12, sctlr_el1</code><code>ldr     x13, =SCTLR_ELx_FLAGS</code><code>bic     x12, x12, x13</code><code>pre_disable_mmu_workaround</code><code>msr     sctlr_el1, x12</code> Disable I,D cache and MMU 45-49 <code>mov     x18, x1</code><code>mov     x0, x2</code><code>mov     x1, x3</code><code>mov     x2, x4</code><code>br      x18</code> Set arguments and jump to <code>arm64_relocate_new_kernel routine</code>. In arm64 registers <code>x0</code>-<code>x6</code> are corresponding to first 7 arguments","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#arm64_relocate_new_kernel","title":"arm64_relocate_new_kernel()","text":"<p>The assembly routine can be found at <code>arch/arm64/kernel/relocate_kernel.S +29</code>. It does nothing significant. It sets dtb address at <code>x0</code> and jumps to <code>kexec_image-&gt;entry</code> which is [supposed to be] pointing to the new kernel.</p> <p></p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#kexec-load-kernel-path","title":"kexec load kernel path","text":"<p>As I expected if <code>kexec_image-&gt;entry</code> pointed to new kernel, I shouldn't had seen that delay. So to verify that I went through <code>kexec_load()</code> system call's path.</p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#kexec_load","title":"kexec_load()","text":"<p><pre><code>kernel/kexec.c +232\n---\nSYSCALL_DEFINE4(kexec_load, unsigned long, entry, unsigned long, nr_segments,\n        struct kexec_segment __user *, segments, unsigned long, flags)\n{\n    int result;\n\n    result = kexec_load_check(nr_segments, flags);\n    if (result)\n        return result;\n\n    /* Verify we are on the appropriate architecture */\n    if (((flags &amp; KEXEC_ARCH_MASK) != KEXEC_ARCH) &amp;&amp;\n        ((flags &amp; KEXEC_ARCH_MASK) != KEXEC_ARCH_DEFAULT))\n        return -EINVAL;\n\n    /* Because we write directly to the reserved memory\n     * region when loading crash kernels we need a mutex here to\n     * prevent multiple crash  kernels from attempting to load\n     * simultaneously, and to prevent a crash kernel from loading\n     * over the top of a in use crash kernel.\n     *\n     * KISS: always take the mutex.\n     */\n    if (!mutex_trylock(&amp;kexec_mutex))\n        return -EBUSY;\n\n    result = do_kexec_load(entry, nr_segments, segments, flags);\n\n    mutex_unlock(&amp;kexec_mutex);\n\n    return result;\n}\n</code></pre> It first checks permission with <code>kexec_load_check</code> function. Then it takes the <code>kexec_mutex</code> and goes into <code>do_kexec_load()</code></p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#do_kexec_load","title":"do_kexec_load()","text":"<pre><code>kernel/kexec.c +106\n---\nstatic int do_kexec_load(unsigned long entry, unsigned long nr_segments,\n        struct kexec_segment __user *segments, unsigned long flags)\n{\n    struct kimage **dest_image, *image;\n    unsigned long i;\n    int ret;\n\n    if (flags &amp; KEXEC_ON_CRASH) {\n        dest_image = &amp;kexec_crash_image;\n        if (kexec_crash_image)\n            arch_kexec_unprotect_crashkres();\n    } else {\n        dest_image = &amp;kexec_image;\n    }\n\n    if (nr_segments == 0) {\n        /* Uninstall image */\n        kimage_free(xchg(dest_image, NULL));\n        return 0;\n    }\n    if (flags &amp; KEXEC_ON_CRASH) {\n        /*\n         * Loading another kernel to switch to if this one\n         * crashes.  Free any current crash dump kernel before\n         * we corrupt it.\n         */\n        kimage_free(xchg(&amp;kexec_crash_image, NULL));\n    }\n\n    ret = kimage_alloc_init(&amp;image, entry, nr_segments, segments, flags);\n    if (ret)\n        return ret;\n\n    if (flags &amp; KEXEC_PRESERVE_CONTEXT)\n        image-&gt;preserve_context = 1;\n\n    ret = machine_kexec_prepare(image);\n    if (ret)\n        goto out;\n\n    /*\n     * Some architecture(like S390) may touch the crash memory before\n     * machine_kexec_prepare(), we must copy vmcoreinfo data after it.\n     */\n    ret = kimage_crash_copy_vmcoreinfo(image);\n    if (ret)\n        goto out;\n\n    for (i = 0; i &lt; nr_segments; i++) {\n        ret = kimage_load_segment(image, &amp;image-&gt;segment[i]);\n        if (ret)\n            goto out;\n    }\n\n    kimage_terminate(image);\n\n    /* Install the new kernel and uninstall the old */\n    image = xchg(dest_image, image);\n\nout:\n    if ((flags &amp; KEXEC_ON_CRASH) &amp;&amp; kexec_crash_image)\n        arch_kexec_protect_crashkres();\n\n    kimage_free(image);\n    return ret;\n}\n</code></pre> line code explanation 113-120 10-17 Its a simple if block choosing image based on context 135 <code>ret = kimage_alloc_init(&amp;image, entry, nr_segments, segments, flags);</code> Create a new image segment. The second argument is important. This jump address. 142 <code>ret = machine_kexec_prepare(image);</code> Check whether any other CPUs are stuck 154-158 52-57 Copies segments passed from user space to kernel space. 163 <code>image = xchg(dest_image, image);</code> Replaces the older image with new one <p> The <code>entry</code> argument passed to <code>kexec_alloc_init()</code> function is assigned to <code>kexec_image-&gt;start</code>. <pre><code>kernel/kexec.c kexec_alloc_init +60\n---\nimage-&gt;start = entry;\n</code></pre> This <code>kexec_image-&gt;start</code> is passed to <code>cpu_soft_restart()</code> as the third argument. If you follow the argument flow in kernel reboot path [as explained above], <code>arm64_relocate_new_kernel()</code> jumps to this address. So this must be the address of new kernel. </p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#kexec-tools","title":"kexec-tools","text":"<p>Now I went into <code>kexec-tools</code> source. Call to <code>kexec_load()</code> is as below. <pre><code> kexec/kexec.c my_load +821\n---\n    result = kexec_load(info.entry,\n                info.nr_segments, info.segment,\n                info.kexec_flags);\n</code></pre> <code>info.entry</code> is passed as the first argument. As we've seen in previous section, this the jump address. Lets see what is there in <code>info.entry</code>. The code flow goes like this</p> <p><pre><code>kexec/kexec.c main +1551\n---\n    result = my_load(type, fileind, argc, argv,\n                kexec_flags, skip_checks, entry);\n</code></pre> <pre><code>kexec/kexec.c my_load +772\n---\n    result = file_type[i].load(argc, argv, kernel_buf, kernel_size, &amp;info);\n</code></pre> <pre><code>kexec/arch/arm/kexec-arm.c +82\n---\nstruct file_type file_type[] = {\n    /* uImage is probed before zImage because the latter also accepts\n       uncompressed images. */\n    {\"uImage\", uImage_arm_probe, uImage_arm_load, zImage_arm_usage},\n    {\"zImage\", zImage_arm_probe, zImage_arm_load, zImage_arm_usage},\n};\n</code></pre> <pre><code>kexec/arch/arm64/kexec-zImage-arm64.c zImage_arm64_load +212\n---\n    result = arm64_load_other_segments(info, kernel_segment\n        + arm64_mem.text_offset);\n</code></pre> <pre><code>kexec/arch/arm64/kexec-arm64.c arm64_load_other_segments +743\n---\n    info-&gt;entry = (void *)elf_rel_get_addr(&amp;info-&gt;rhdr, \"purgatory_start\");\n</code></pre></p> <p>Address of a symbol called <code>purgatory_start</code> is assigned to <code>info-&gt;entry</code>. So old kernel makes a jump to this <code>purgatory_start</code> not to the new kernel. This was the big surprise. The last thing I expected was <code>kexec-tools</code> inserts a piece of code between kernels. I felt that I came close to the criminal.</p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/01/20/kexec---a-travel-to-the-purgatory/#the-purgatory","title":"The purgatory","text":"<p><pre><code>purgatory/arch/arm64/entry.S +9\n---\n.text\n\n.globl purgatory_start\npurgatory_start:\n\n    adr x19, .Lstack\n    mov sp, x19\n\n    bl  purgatory\n\n    /* Start new image. */\n    ldr x17, arm64_kernel_entry\n    ldr x0, arm64_dtb_addr\n    mov x1, xzr\n    mov x2, xzr\n    mov x3, xzr\n    br  x17\n\nsize purgatory_start\n</code></pre> <code>purgatory_start</code> is an assembly subroutine. It calls a function <code>purgatory</code> first. And then stores <code>arm64_dtb_addr</code> in register <code>x0</code> and jumps to <code>arm64_kernel_entry</code>. As arm64 kernel expects dtb address to be set in register <code>x0</code>, this jump is likely to be the jump to new kernel. We'll see where these symbols are set.</p> <p><pre><code>kexec/arch/arm64/kexec-arm64.c arm64_load_other_segments +\n---\n    elf_rel_build_load(info, &amp;info-&gt;rhdr, purgatory, purgatory_size,\n        hole_min, hole_max, 1, 0);\n\n    info-&gt;entry = (void *)elf_rel_get_addr(&amp;info-&gt;rhdr, \"purgatory_start\");\n\n    elf_rel_set_symbol(&amp;info-&gt;rhdr, \"arm64_kernel_entry\", &amp;image_base,\n        sizeof(image_base));\n\n    elf_rel_set_symbol(&amp;info-&gt;rhdr, \"arm64_dtb_addr\", &amp;dtb_base,\n        sizeof(dtb_base));\n</code></pre> As you see in the code snippet, <code>image_base</code> is <code>arm64_kernel_entry</code> and <code>dtb_base</code> is <code>arm64_dtb_addr</code>. If you go little above in the function, you'll see <code>image_base</code> and <code>dtb_base</code> are kernel address and device-tree address respectively. And the function <code>purgatory</code> is also loaded into the segments. The last function to check between two kernels is <code>purgatory</code>.</p> <p><pre><code>purgatory/purgatory.c\n---\nvoid purgatory(void)\n{\n    printf(\"I'm in purgatory\\n\");\n    setup_arch();\n    if (!skip_checks &amp;&amp; verify_sha256_digest()) {\n        for(;;) {\n            /* loop forever */\n        }\n    }\n    post_verification_setup_arch();\n}\n</code></pre> <code>setup_arch()</code> and <code>post_verification_setup_arch()</code> are no-op for arm64. The actual delay is caused by <code>verify_sha256_digest()</code>. There is nothing wrong with this function. But it is being executed with I and D caches off. The option <code>skip_checks</code> was not introduced by the time I was debugging this issue. So we solved it by enabling I &amp; D cache in <code>setup_arch()</code> and disabling it back in <code>post_verification_setup_arch()</code>.</p> <p>At last I felt like my purgatory sentence was over.</p>","tags":["linux","ARMv8","kernel","crazy debugging"]},{"location":"blog/2020/09/28/perf-kvm-to-profile-vm_exit/","title":"perf kvm to profile vm_exit","text":"<p>Optimizing VM_EXITs will significantly improve performance VMs. All the major improvements in VM world is mainly focusing on reducing the number of VM_EXITs. To optimize it, first we should able to measure it. Initially the tool <code>kvm_stat</code> was designed for this purpose, later it has been added inside <code>perf</code> itself.</p> <p>To profile VM_EXITs while running <code>sysbench</code>,  * Get <code>pid</code> of the VM task - 127894  * Get the IP of that machine - 192.168.122.194      Make sure you can ssh to that machine without password  * Install <code>sysbench</code> inside the VM</p> <pre><code>$ sudo perf kvm stat record -p 127894 ssh 192.168.122.194 -l test_user \"sysbench --test=cpu --cpu-max-prime=20000 run\"\nsysbench 0.4.12:  multi-threaded system evaluation benchmark\n\nRunning the test with following options:\nNumber of threads: 1\n\nDoing CPU performance benchmark\n\nThreads started!\nDone.\n\nMaximum prime number checked in CPU test: 20000\n\n\nTest execution summary:\n    total time:                          22.6607s\n    total number of events:              10000\n    total time taken by event execution: 22.6598\n    per-request statistics:\n         min:                                  2.13ms\n         avg:                                  2.27ms\n         max:                                 12.10ms\n         approx.  95 percentile:               2.88ms\n\nThreads fairness:\n    events (avg/stddev):           10000.0000/0.00\n    execution time (avg/stddev):   22.6598/0.00\n\n[ perf record: Woken up 2 times to write data ]\n[ perf record: Captured and wrote 4.779 MB perf.data.guest (52461 samples) ]\n$\n</code></pre> <p>Perf has recorded the data in <code>perf.data.guest</code> in the current directory. Now to view VM_EXITs,</p> <pre><code>$ sudo perf kvm stat report --event=vmexit\n\n\nAnalyze events for all VMs, all VCPUs:\n\n             VM-EXIT    Samples  Samples%     Time%    Min Time    Max Time         Avg time\n\n           MSR_WRITE       9167    35.40%     0.04%      0.45us   9554.94us      3.00us ( +-  41.94% )\n  EXTERNAL_INTERRUPT       5877    22.69%     0.02%      0.37us   1175.48us      2.43us ( +-  17.90% )\n    PREEMPTION_TIMER       5728    22.12%     0.01%      0.51us     21.14us      0.62us ( +-   0.87% )\n                 HLT       2232     8.62%    99.92%      0.56us 1001118.99us  30567.94us ( +-   9.88% )\n               CPUID       2160     8.34%     0.00%      0.40us     12.82us      0.65us ( +-   1.29% )\n   PAUSE_INSTRUCTION        390     1.51%     0.00%      0.38us   1490.19us      8.27us ( +-  62.22% )\n       EPT_MISCONFIG        303     1.17%     0.01%      1.04us    167.13us     13.33us ( +-   8.61% )\n         EOI_INDUCED         37     0.14%     0.00%      0.62us      3.00us      1.24us ( +-   6.58% )\n       EXCEPTION_NMI          4     0.02%     0.00%      0.42us      0.56us      0.47us ( +-   6.81% )\n\nTotal Samples:25898, Total events handled time:68281638.61us.\n\n$\n</code></pre>","tags":["linux","perf"]},{"location":"blog/2020/10/01/quick-kernel-upgrade-with-kexec/","title":"Quick kernel upgrade with kexec","text":"<p>One of the major issues we are facing is keeping up to date with security patches. That too keeping the kernel up to date is little harder. Because it requires a reboot. As reboot will take minutes to complete, there will be a significant service downtime. Or doing a service migration to avoid downtime will come with its own complexity.</p> <p><code>kexec</code> will be help in these situations. It can upgrade the kernel without complete reboot process. Though not zero, the downtime is very less compared to a full reboot. In this post, I'll demo upgrading kernel of a Virtual machine running Debian-9.</p> <p>This VM is running Debian <code>Linux-4.9.0-12</code>. Let me update to the latest kernel available now - <code>Linux-4.9.0-13</code>.</p> <p>Install <code>kexec-tools</code> <pre><code>root@debian:~# apt install kexec-tools -qq\n</code></pre></p> <p>Install latest <code>Linux-image</code> package. This will not overwrite the existing kernel or initrd image in your <code>/boot/</code> directory. So you can safely rollback if required. <pre><code>root@debian:~# ls -lh /boot/\ntotal 25M\n-rw-r--r-- 1 root root 3.1M Jan 21  2020 System.map-4.9.0-12-amd64\n-rw-r--r-- 1 root root 183K Jan 21  2020 config-4.9.0-12-amd64\ndrwxr-xr-x 5 root root 4.0K Apr 24 12:40 grub\n-rw-r--r-- 1 root root  18M Apr 24 12:23 initrd.img-4.9.0-12-amd64\n-rw-r--r-- 1 root root 4.1M Jan 21  2020 vmlinuz-4.9.0-12-amd64\n\nroot@debian:~# sudo apt update -qq\n43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n\nroot@debian:~# sudo apt install linux-image-amd64 -qq\nThe following additional packages will be installed:\n  linux-image-4.9.0-13-amd64\nSuggested packages:\n  linux-doc-4.9 debian-kernel-handbook\nThe following NEW packages will be installed:\n  linux-image-4.9.0-13-amd64\nThe following packages will be upgraded:\n  linux-image-amd64\n1 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\nNeed to get 39.3 MB of archives.\nAfter this operation, 193 MB of additional disk space will be used.\nDo you want to continue? [Y/n] y\nSelecting previously unselected package linux-image-4.9.0-13-amd64.\n(Reading database ... 26429 files and directories currently installed.)\nPreparing to unpack .../linux-image-4.9.0-13-amd64_4.9.228-1_amd64.deb ...\nUnpacking linux-image-4.9.0-13-amd64 (4.9.228-1) ...........................]\nPreparing to unpack .../linux-image-amd64_4.9+80+deb9u11_amd64.deb .........]\nUnpacking linux-image-amd64 (4.9+80+deb9u11) over (4.9+80+deb9u10) .........]\nSetting up linux-image-4.9.0-13-amd64 (4.9.228-1) ..........................]\nI: /vmlinuz is now a symlink to boot/vmlinuz-4.9.0-13-amd64.................]\nI: /initrd.img is now a symlink to boot/initrd.img-4.9.0-13-amd64\n/etc/kernel/postinst.d/initramfs-tools:\nupdate-initramfs: Generating /boot/initrd.img-4.9.0-13-amd64\n/etc/kernel/postinst.d/zz-update-grub:\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-4.9.0-13-amd64\nFound initrd image: /boot/initrd.img-4.9.0-13-amd64\nFound linux image: /boot/vmlinuz-4.9.0-12-amd64\nFound initrd image: /boot/initrd.img-4.9.0-12-amd64\ndone\nSetting up linux-image-amd64 (4.9+80+deb9u11) ...###########................]\n\nroot@debian:~# ls -lh /boot/\ntotal 50M\n-rw-r--r-- 1 root root 3.1M Jan 21  2020 System.map-4.9.0-12-amd64\n-rw-r--r-- 1 root root 3.1M Jul  6 02:59 System.map-4.9.0-13-amd64   &lt;---\n-rw-r--r-- 1 root root 183K Jan 21  2020 config-4.9.0-12-amd64\n-rw-r--r-- 1 root root 183K Jul  6 02:59 config-4.9.0-13-amd64       &lt;---\ndrwxr-xr-x 5 root root 4.0K Oct  1 17:25 grub\n-rw-r--r-- 1 root root  18M Apr 24 12:23 initrd.img-4.9.0-12-amd64\n-rw-r--r-- 1 root root  18M Oct  1 17:25 initrd.img-4.9.0-13-amd64   &lt;---\n-rw-r--r-- 1 root root 4.1M Jan 21  2020 vmlinuz-4.9.0-12-amd64\n-rw-r--r-- 1 root root 4.1M Jul  6 02:59 vmlinuz-4.9.0-13-amd64      &lt;---\n</code></pre></p> <p>Now copy the kernel command line from <code>/proc/cmdline</code>. We should pass this to <code>kexec</code>. <pre><code>root@debian:~# cat /proc/cmdline\nBOOT_IMAGE=/boot/vmlinuz-4.9.0-12-amd64 root=UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx ro net.ifnames=0 biosdevname=0 cgroup_enable=memory console=tty0 console=ttyS0,115200 notsc scsi_mod.use_blk_mq=Y quiet\n</code></pre></p> <p>Load the new kernel using <code>kexec -l</code>. <pre><code>root@debian:~# kexec -l /boot/vmlinuz-4.9.0-13-amd64 --initrd=/boot/initrd.img-4.9.0-13-amd64 --command-line=\"BOOT_IMAGE=/boot/vmlinuz-4.9.0-13-amd64 root=UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx ro net.ifnames=0 biosdevname=0 cgroup_enable=memory console=tty0 console=ttyS0,115200 notsc scsi_mod.use_blk_mq=Y quiet\"\nroot@debian:~#\n</code></pre></p> <p>Now upgrade to the new kernel. <pre><code>root@debian:~# uname -a\nLinux debian 4.9.0-12-amd64 #1 SMP Debian 4.9.210-1 (2020-01-20) x86_64 GNU/Linux\n\nroot@debian:~# systemctl start kexec.target\n[268181.341191] kexec_core: Starting new kernel\n/dev/sda1: clean, 35704/655360 files, 366185/2621179 blocks\nGROWROOT: NOCHANGE: partition 1 is size 20969439. it cannot be grown\n\nDebian GNU/Linux 9 debian ttyS0\n\ndebian login: root\nPassword:\nLast login: Mon Sep 28 14:59:30 IST 2020 on ttyS0\nLinux debian 4.9.0-13-amd64 #1 SMP Debian 4.9.228-1 (2020-07-05) x86_64\n\nThe programs included with the Debian GNU/Linux system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\npermitted by applicable law.\n\nroot@debian:~# uname -a\nLinux debian 4.9.0-13-amd64 #1 SMP Debian 4.9.228-1 (2020-07-05) x86_64 GNU/Linux\nroot@debian:~#\n</code></pre></p>","tags":["linux","debian","stretch","kernel","kexec","security","upgrade"]},{"location":"blog/2020/10/01/quick-kernel-upgrade-with-kexec/#time-to-upgrade","title":"Time to upgrade","text":"<p>This actually took no time. I was pinging this VM from its Host. There was a slight increase in latency while the upgrade was in progress. That was less than a second. But I didn't run any service and tested its status after reboot. Because it may vary from service to service.</p> <pre><code>64 bytes from 192.168.122.91: icmp_seq=176 ttl=64 time=0.465 ms\n64 bytes from 192.168.122.91: icmp_seq=177 ttl=64 time=0.408 ms\n64 bytes from 192.168.122.91: icmp_seq=181 ttl=64 time=8.32 ms   &lt;---\n64 bytes from 192.168.122.91: icmp_seq=182 ttl=64 time=0.452 ms\n64 bytes from 192.168.122.91: icmp_seq=183 ttl=64 time=0.198 ms\n</code></pre>","tags":["linux","debian","stretch","kernel","kexec","security","upgrade"]},{"location":"blog/2020/10/31/perf-setup/","title":"perf setup","text":"<p><code>Linux-perf</code> aka <code>perf</code> is versatile, like a batmobile. It has all the tools and functionalities you need. And you'll feel like a superhero once you master it.</p> <p>By default perf comes with may tools that relying on debug and trace symbols exported via <code>procfs</code>. But to add custom probes and probes with line numbers, kernel debug symbols and kernel source is necessary. In this post I'll walk you through the necessary setup process. I'm using an Ubuntu-20.04 VM running on Virtual box. I'm not going to rebuild and install kernel. The steps will be,</p> <ul> <li>Install Linux-kernel debug symbols</li> <li>Fetch Linux-kernel source</li> <li>Install perf</li> <li>First run</li> </ul>","tags":["linux","ubuntu","kernel","perf","setup"]},{"location":"blog/2020/10/31/perf-setup/#enable-non-common-repositories","title":"Enable non-common repositories","text":"<p>Enable debug repositories in <code>apt</code> source list.</p> <pre><code>bala@ubuntu-vm-1:~$ echo \"deb http://ddebs.ubuntu.com $(lsb_release -cs) main restricted universe multiverse\ndeb http://ddebs.ubuntu.com $(lsb_release -cs)-updates main restricted universe multiverse\ndeb http://ddebs.ubuntu.com $(lsb_release -cs)-proposed main restricted universe multiverse\" | sudo tee -a /etc/apt/sources.list.d/ddebs.list\n</code></pre> <p>Install debug keyring <pre><code>bala@ubuntu-vm-1:~$ sudo apt install ubuntu-dbgsym-keyring\n</code></pre></p> <p>Enable source repositories in <code>apt</code> source list. <pre><code>bala@ubuntu-vm-1:~$ grep deb-src /etc/apt/sources.list\ndeb-src http://in.archive.ubuntu.com/ubuntu/ focal main restricted\n</code></pre></p> <p>Do <code>apt</code> update <pre><code>bala@ubuntu-vm-1:~$ sudo apt update\n</code></pre></p>","tags":["linux","ubuntu","kernel","perf","setup"]},{"location":"blog/2020/10/31/perf-setup/#1-install-linux-kernel-debug-symbols","title":"1. Install Linux-kernel debug symbols","text":"<p>Install Linux debug symbols corresponding the kernel installed in your machine.</p> <pre><code>bala@ubuntu-vm-1:~$ sudo apt install -y linux-image-`uname -r`-dbgsym\n</code></pre> <p>Linux image with debug symbols will be installed in the directory <code>/usr/lib/debug/boot/</code> <pre><code>bala@ubuntu-vm-1:~$ ls -lh /usr/lib/debug/boot/vmlinux-5.4.0-52-generic\n-rw-r--r-- 2 root root 742M Oct 15 15:58 /usr/lib/debug/boot/vmlinux-5.4.0-52-generic\nbala@ubuntu-vm-1:~$\n</code></pre></p>","tags":["linux","ubuntu","kernel","perf","setup"]},{"location":"blog/2020/10/31/perf-setup/#2-fetch-linux-kernel-source","title":"2. Fetch Linux kernel source","text":"<p>Fetch the source package corresponding to the installed kernel. <pre><code>bala@ubuntu-vm-1:~$ sudo apt install linux-source\n</code></pre> Kernel source with debian packaging files will be installed in the path <code>/usr/src/linux-source-5.4.0</code>. The kernel source is available in a tarball inside this directory. Copy to your desired location and extract. <pre><code>bala@ubuntu-vm-1:~$ ls -lh /usr/src/linux-source-5.4.0/linux-source-5.4.0.tar.bz2\n-rw-r--r-- 1 root root 129M Oct 15 15:58 /usr/src/linux-source-5.4.0/linux-source-5.4.0.tar.bz2\nbala@ubuntu-vm-1:~$ cp -f /usr/src/linux-source-5.4.0/linux-source-5.4.0.tar.bz2 ~/source/\nbala@ubuntu-vm-1:~$ cd ~/source/\nbala@ubuntu-vm-1:~/source$ tar -xvf linux-source-5.4.0.tar.bz2\nbala@ubuntu-vm-1:~/source$ ls ~/source/linux-source-5.4.0/\narch   certs    CREDITS  Documentation  dropped.txt  include  ipc     Kconfig  lib       MAINTAINERS  mm   README   scripts   snapcraft.yaml  tools   update-version-dkms  virt\nblock  COPYING  crypto   drivers        fs           init     Kbuild  kernel   LICENSES  Makefile     net  samples  security  sound           ubuntu  usr\nbala@ubuntu-vm-1:~$\n</code></pre></p>","tags":["linux","ubuntu","kernel","perf","setup"]},{"location":"blog/2020/10/31/perf-setup/#3-install-linux-perf","title":"3. Install Linux perf","text":"<p>It comes with <code>linux-tools-generic</code> package on Ubuntu-20.04. <pre><code>bala@ubuntu-vm-1:~$ sudo apt install linux-tools-generic\n</code></pre></p>","tags":["linux","ubuntu","kernel","perf","setup"]},{"location":"blog/2020/10/31/perf-setup/#4-run-your-first-perf-command","title":"4. Run your first perf command","text":"<p>I want to count number of IPI (Inter Processor Interrupts) sent by <code>resched_curr</code>. It sends IPI when the target CPU is not the current CPU (the one executing the function itself). Here is the source code of that function. <pre><code>void resched_curr(struct rq *rq)\n{\n    struct task_struct *curr = rq-&gt;curr;\n    int cpu;\n\n    lockdep_assert_held(&amp;rq-&gt;lock);\n\n    if (test_tsk_need_resched(curr))\n        return;\n\n    cpu = cpu_of(rq);\n\n    if (cpu == smp_processor_id()) {\n        set_tsk_need_resched(curr);\n        set_preempt_need_resched();\n        return;\n    }\n\n    if (set_nr_and_not_polling(curr))\n        smp_send_reschedule(cpu);\n    else\n        trace_sched_wake_idle_without_ipi(cpu);\n}\n</code></pre></p> <p>So if target CPU is the current CPU, line number 14 will get executed. Otherwise execution continues from line number 18. Also I want to record the target CPU in both cases.</p> <p>Get the line numbers where you can insert probes from perf itself. <pre><code>bala@ubuntu-vm-1:~/source$ sudo perf probe -k /usr/lib/debug/boot/vmlinux-5.4.0-52-generic -s ~/source/linux-source-5.4.0 -L resched_curr\n&lt;resched_curr@/home/bala/source/linux-source-5.4.0//kernel/sched/core.c:0&gt;\n      0  void resched_curr(struct rq *rq)\n      1  {\n      2         struct task_struct *curr = rq-&gt;curr;\n                int cpu;\n\n                lockdep_assert_held(&amp;rq-&gt;lock);\n\n      7         if (test_tsk_need_resched(curr))\n                        return;\n\n     10         cpu = cpu_of(rq);\n\n     12         if (cpu == smp_processor_id()) {\n     13                 set_tsk_need_resched(curr);\n     14                 set_preempt_need_resched();\n     15                 return;\n                }\n\n     18         if (set_nr_and_not_polling(curr))\n     19                 smp_send_reschedule(cpu);\n                else\n     21                 trace_sched_wake_idle_without_ipi(cpu);\n         }\n\n         void resched_cpu(int cpu)\n\nbala@ubuntu-vm-1:~/source$\n</code></pre></p> <p>Here is the probe for non-IPI case. I name it as <code>resched_curr_same_cpu</code>. <pre><code>bala@ubuntu-vm-1:~$ sudo perf probe -k /usr/lib/debug/boot/vmlinux-5.4.0-52-generic -s source/linux-source-5.4.0 resched_curr_same_cpu='resched_curr:14 rq-&gt;cpu'\n</code></pre></p> <p>Probe for IPI case. And I name it as <code>resched_curr_send_ipi</code>. <pre><code>bala@ubuntu-vm-1:~$ sudo perf probe -k /usr/lib/debug/boot/vmlinux-5.4.0-52-generic -s source/linux-source-5.4.0 resched_curr_send_ipi='resched_curr:19 rq-&gt;cpu'\n</code></pre></p> <p>Note: To probe the function <code>resched_curr</code> and its argument <code>rq</code>, we need Linux debug symbols. And to probe on line numbers we need Linux source. So that we have installed both of them earlier.</p> <p>Now lets capture the execution of a <code>stress-ng</code> test. <pre><code>bala@ubuntu-vm-1:~$ sudo perf record -e probe:resched_curr_same_cpu,probe:resched_curr_send_ipi stress-ng --mq 8 -t 5 --metrics-brief\nstress-ng: info:  [22439] dispatching hogs: 8 mq\nstress-ng: info:  [22439] successful run completed in 5.01s\nstress-ng: info:  [22439] stressor       bogo ops real time  usr time  sys time   bogo ops/s   bogo ops/s\nstress-ng: info:  [22439]                           (secs)    (secs)    (secs)   (real time) (usr+sys time)\nstress-ng: info:  [22439] mq              2225397      5.00      3.57     16.14    445062.30    112907.00\n[ perf record: Woken up 421 times to write data ]\n[ perf record: Captured and wrote 105.404 MB perf.data (1380709 samples) ]\nbala@ubuntu-vm-1:~$\n</code></pre></p> <p>And the report is, <pre><code>bala@ubuntu-vm-1:~$ sudo perf report --stdio\n# To display the perf.data header info, please use --header/--header-only options.\n#\n#\n# Total Lost Samples: 0\n#\n# Samples: 1M of event 'probe:resched_curr_same_cpu'\n# Event count (approx.): 1380698\n#\n# Overhead  Trace output\n# ........  ........................\n#\n    29.13%  (ffffffff83ad740d) cpu=1\n    27.77%  (ffffffff83ad740d) cpu=2\n    24.74%  (ffffffff83ad740d) cpu=0\n    18.36%  (ffffffff83ad740d) cpu=3\n\n\n# Samples: 11  of event 'probe:resched_curr_send_ipi'\n# Event count (approx.): 11\n#\n# Overhead  Trace output\n# ........  ........................\n#\n    45.45%  (ffffffff83ad73af) cpu=1\n    36.36%  (ffffffff83ad73af) cpu=3\n     9.09%  (ffffffff83ad73af) cpu=0\n     9.09%  (ffffffff83ad73af) cpu=2\n\n\n#\n# (Cannot load tips.txt file, please install perf!)\n#\nbala@ubuntu-vm-1:~$\n</code></pre> As you can see only 11 times out of a million times an IPI is sent. More on this in later posts. Until then... \"Perhaps you should read the instructions first?\".</p>","tags":["linux","ubuntu","kernel","perf","setup"]},{"location":"blog/2020/10/31/perf-setup/#references","title":"References","text":"<ul> <li>http://www.brendangregg.com/perf.html</li> <li>https://wiki.ubuntu.com/Kernel/Reference/stress-ng</li> <li>https://man7.org/linux/man-pages/man1/perf-probe.1.html</li> <li>https://wiki.ubuntu.com/Debug%20Symbol%20Packages</li> <li>https://askubuntu.com/questions/50145/how-to-install-perf-monitoring-tool</li> </ul>","tags":["linux","ubuntu","kernel","perf","setup"]},{"location":"blog/2020/11/19/custom-perf-with-custom-kernel/","title":"Custom perf with custom kernel","text":"<p>There is no doubt that perf is an awesome tool. And eBPF enables us to attach an arbitrary code to any tracepoint. But both has one limitation. They cannot execute a kernel function. As if I want to call a kernel function whenever a tracepoint is hit. Its not possible today. It is not a big caveat. But will be very much useful while debugging kernel.</p> <p>Lets say I want to trace all IPIs and want to record the source CPU, target CPU. And I want to know whether Idle task was running on the target CPU during the IPI. Inter Processor Interrupt or IPI is and interrupt sent from one CPU to another to wake it up.</p> <p>NOTE: All the code referred here is from Linux-5.4.0</p> <p>The code that sends IPI in x86 platform is <code>native_smp_send_reschedule</code>. Below is the code of that function. <pre><code>void native_smp_send_reschedule(int cpu)\n{\n        if (unlikely(cpu_is_offline(cpu))) {\n                WARN(1, \"sched: Unexpected reschedule of offline CPU#%d!\\n\", cpu);\n                return;\n        }\n        apic-&gt;send_IPI(cpu, RESCHEDULE_VECTOR);\n}\n</code></pre> The only argument of this function <code>cpu</code> mentions the target CPU - to which CPU IPI is being sent. I want to source CPU from which the IPI is originates.</p> <p>To get current CPU kernel has a macro - <code>smp_processor_id()</code>.</p> <p>We can get the task-run-queue of that CPU using the macro <code>cpu_rq(int cpu)</code>. And <code>cpu_rq(int cpu)-&gt;curr</code> will point to the task that is currently running on that CPU. If that task's <code>pid</code> is 0, it is the Idle task.</p> <p>So my deduced requirement will be setting and tracepoint on <code>native_smp_send_reschedule</code> and call these kernel functions upon hitting that tracepoint. There is no way today. Thus I'm left with only one option - build my own kernel with necessary modifications.</p>","tags":["linux","kernel","perf"]},{"location":"blog/2020/11/19/custom-perf-with-custom-kernel/#building-custom-kernel","title":"Building custom kernel","text":"<p>I've cloned kernel repository from kernel.org. And checked-out to the desired branch. I'm running Ubuntu-20.05 inside a Virtualbox. So I'm using the kernel-5.4.0 which is already installed in my distribution. <pre><code>bala@ubuntu-vm-1:~/source/$ git clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git\nbala@ubuntu-vm-1:~/source/$ cd linux\nbala@ubuntu-vm-1:~/source/linux/$ git checkout -b v5.4.0 v5.4\nbala@ubuntu-vm-1:~/source/linux/$ head -n6 Makefile\n # SPDX-License-Identifier: GPL-2.0\n VERSION = 5\n PATCHLEVEL = 4\n SUBLEVEL = 0\n EXTRAVERSION =\n NAME = Kleptomaniac Octopus\nbala@ubuntu-vm-1:~/source/linux/$\n</code></pre></p> <p>Copied config from current OS. And <code>make oldconfig</code> <pre><code>bala@ubuntu-vm-1:~/source/linux/$ cp /boot/config-5.4.0-52-generic ./.config\nbala@ubuntu-vm-1:~/source/linux/$ make oldconfig\n</code></pre> If any new configs were, select appropriately if you know it. Otherwise simply ignore it.</p> <p>Applied following patch. <pre><code>bala@ubuntu-vm-1:~/source/linux$ git diff\ndiff --git a/arch/x86/kernel/apic/ipi.c b/arch/x86/kernel/apic/ipi.c\nindex 6ca0f91372fd..31df1e1f4922 100644\n--- a/arch/x86/kernel/apic/ipi.c\n+++ b/arch/x86/kernel/apic/ipi.c\n@@ -2,6 +2,7 @@\n\n #include &lt;linux/cpumask.h&gt;\n #include &lt;linux/smp.h&gt;\n+#include \"../../../../kernel/sched/sched.h\"\n\n #include \"local.h\"\n\n@@ -61,14 +62,22 @@ void apic_send_IPI_allbutself(unsigned int vector)\n  * wastes no time serializing anything. Worst case is that we lose a\n  * reschedule ...\n  */\n+#pragma GCC push_options\n+#pragma GCC optimize (\"O0\")\n void native_smp_send_reschedule(int cpu)\n {\n+       int this_cpu = smp_processor_id();\n+       int that_cpu = cpu;\n+       struct rq *rq = cpu_rq(that_cpu);\n+       int is_idle_task = ((rcu_dereference(rq-&gt;curr))-&gt;pid == 0);\n+\n        if (unlikely(cpu_is_offline(cpu))) {\n                WARN(1, \"sched: Unexpected reschedule of offline CPU#%d!\\n\", cpu);\n                return;\n        }\n        apic-&gt;send_IPI(cpu, RESCHEDULE_VECTOR);\n }\n+#pragma GCC pop_options\n\n void native_send_call_func_single_ipi(int cpu)\n {\ndiff --git a/lib/Kconfig.debug b/lib/Kconfig.debug\nindex 93d97f9b0157..706fdfc715ab 100644\n--- a/lib/Kconfig.debug\n+++ b/lib/Kconfig.debug\n@@ -359,6 +359,7 @@ config SECTION_MISMATCH_WARN_ONLY\n #\n config ARCH_WANT_FRAME_POINTERS\n        bool\n+       default y\n\n config FRAME_POINTER\n        bool \"Compile the kernel with frame pointers\"\nbala@ubuntu-vm-1:~/source/linux$\n</code></pre> This patch contains two changes,  * <code>lib/Kconfig.debug</code> - It is to enable frame pointers. Frame pointers will be very much useful for stack trace.  * <code>arch/x86/kernel/apic/ipi.c</code>      * <code>pragma</code> instructions are compiler directives. They tell GCC not to optimize the code below - <code>O0</code> optimization. Otherwise, GCC may optimize away the variables as they are unused variables.      * <code>this_cpu</code> is got from <code>smp_processor_id()</code>      * <code>is_idle_task</code> will be set to 1 if target CPU is executing Idle task.      * Last <code>pragma</code> instruction is to reset GCC options back to default.</p> <p>Enabled kernel debug info by setting <code>CONFIG_DEBUG_INFO</code> in the <code>.config</code> file. And built the kernel. My VM has 4 CPUs, so started 8 parallel build threads. <pre><code>bala@ubuntu-vm-1:~/source/linux$ make bzImage -j8\n...\n...\n...\n  OBJCOPY arch/x86/boot/setup.bin\n  BUILD   arch/x86/boot/bzImage\nSetup is 16412 bytes (padded to 16896 bytes).\nSystem is 8097 kB\nCRC 65acc728\nKernel: arch/x86/boot/bzImage is ready  (#1)\nbala@ubuntu-vm-1:~/source/linux$\n</code></pre></p> <p>Now copied the <code>bzImage</code> to <code>/boot/</code> directory and updated boot loader. <pre><code>bala@ubuntu-vm-1:~/source/linux$ sudo cp arch/x86/boot/bzImage /boot/vmlinuz-dbg-custom\nbala@ubuntu-vm-1:~/source/linux$ sudo update-grub\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/init-select.cfg'\nGenerating grub configuration file ...\ndpkg: warning: version 'dbg-custom' has bad syntax: version number does not start with digit\nFound linux image: /boot/vmlinuz-dbg-custom\nFound linux image: /boot/vmlinuz-5.4.0-52-generic\nFound initrd image: /boot/initrd.img-5.4.0-52-generic\ndone\nbala@ubuntu-vm-1:~/source/linux$\n</code></pre></p> <p>Rebooted the VM into newly built kernel.</p>","tags":["linux","kernel","perf"]},{"location":"blog/2020/11/19/custom-perf-with-custom-kernel/#custom-build-perf","title":"Custom build perf","text":"<p>As the kernel is custom built, the perf package comes with Ubuntu-20.04 didn't get executed. It threw following error. <pre><code>bala@ubuntu-vm-1:~/source/linux$ perf --help\nWARNING: perf not found for kernel 5.4.0+\n\n  You may need to install the following packages for this specific kernel:\n    linux-tools-5.4.0+-5.4.0+\n    linux-cloud-tools-5.4.0+-5.4.0+\n\n  You may also want to install one of the following packages to keep up to date:\n    linux-tools-5.4.0+\n    linux-cloud-tools-5.4.0+\nbala@ubuntu-vm-1:~/source/linux$\n</code></pre></p> <p>So I went ahead and build perf from kernel source itself. Perf requires following packages for adding a probe and source line probing.  * libelf-dev  * libdw-dev</p> <p>I installed both of them. <pre><code>bala@ubuntu-vm-1:~/source/linux/$ sudo apt install libelf-dev libdw-dev\n</code></pre></p> <p>Now built perf inside the kernel source tree. <pre><code>bala@ubuntu-vm-1:~/source/linux/$ cd tools/perf\nbala@ubuntu-vm-1:~/source/linux/tools/perf/$ make\n</code></pre></p>","tags":["linux","kernel","perf"]},{"location":"blog/2020/11/19/custom-perf-with-custom-kernel/#running-the-probe-on-custom-kernel-with-custom-perf","title":"Running the probe on custom kernel with custom perf","text":"<pre><code>bala@ubuntu-vm-1:~/source/linux/tools/perf$ sudo ./perf probe -s /home/bala/source/linux/ -k ../../vmlinux native_smp_send_reschedule=\"native_smp_send_reschedule:7 this_cpu that_cpu is_idle_task\"\nAdded new events:\n  probe:native_smp_send_reschedule (on native_smp_send_reschedule:7 with this_cpu that_cpu is_idle_task)\n  probe:native_smp_send_reschedule_1 (on native_smp_send_reschedule:7 with this_cpu that_cpu is_idle_task)\n\nYou can now use it in all perf tools, such as:\n\n        perf record -e probe:native_smp_send_reschedule_1 -aR sleep 1\n\nbala@ubuntu-vm-1:~/source/linux/tools/perf$ sudo ./perf record -e probe:native_smp_send_reschedule_1 -aR sleep 1\nCouldn't synthesize bpf events.\n[ perf record: Woken up 1 times to write data ]\nway too many cpu caches..[ perf record: Captured and wrote 0.101 MB perf.data (10 samples) ]\nbala@ubuntu-vm-1:~/source/linux/tools/perf$ sudo ./perf report --stdio\n# To display the perf.data header info, please use --header/--header-only options.\n#\n#\n# Total Lost Samples: 0\n#\n# Samples: 10  of event 'probe:native_smp_send_reschedule_1'\n# Event count (approx.): 10\n#\n# Overhead  Trace output\n# ........  .......................................................\n#\n    20.00%  (ffffffffa5e46bc9) this_cpu=0 that_cpu=1 is_idle_task=1\n    20.00%  (ffffffffa5e46bc9) this_cpu=3 that_cpu=0 is_idle_task=1\n    10.00%  (ffffffffa5e46bc9) this_cpu=0 that_cpu=2 is_idle_task=1\n    10.00%  (ffffffffa5e46bc9) this_cpu=0 that_cpu=3 is_idle_task=1\n    10.00%  (ffffffffa5e46bc9) this_cpu=1 that_cpu=0 is_idle_task=1\n    10.00%  (ffffffffa5e46bc9) this_cpu=2 that_cpu=3 is_idle_task=1\n    10.00%  (ffffffffa5e46bc9) this_cpu=3 that_cpu=1 is_idle_task=1\n    10.00%  (ffffffffa5e46bc9) this_cpu=3 that_cpu=2 is_idle_task=1\n\n\n#\n# (Tip: Skip collecting build-id when recording: perf record -B)\n#\nbala@ubuntu-vm-1:~/source/linux/tools/perf$\n</code></pre> <p>Why <code>is_idle_task</code> is always 1 during and IPI?! More on later post ;).</p>","tags":["linux","kernel","perf"]},{"location":"blog/2020/11/19/custom-perf-with-custom-kernel/#references","title":"References","text":"<ul> <li>https://github.com/iovisor/bpftrace/issues/792</li> <li>https://gcc.gnu.org/onlinedocs/gcc/Function-Specific-Option-Pragmas.html</li> <li>https://news.ycombinator.com/item?id=4711571</li> <li>https://www.cyberciti.biz/tips/compiling-linux-kernel-26.html</li> <li>https://tldp.org/LDP/lame/LAME/linux-admin-made-easy/kernel-custom.html</li> <li>https://stackoverflow.com/questions/28136815/linux-kernel-how-to-obtain-a-particular-version-right-upto-sublevel</li> <li>https://git-scm.com/docs/git-fetch</li> <li>https://www.quora.com/How-do-I-compile-a-Linux-perf-tool-with-all-features-For-Linux-4-0-on-Ubuntu</li> <li>https://serverfault.com/questions/251134/how-to-compile-the-kernel-with-debug-symbols</li> </ul>","tags":["linux","kernel","perf"]},{"location":"blog/2021/07/31/copy-paste-in-tmux-session-inside-ssh/","title":"Copy paste in tmux session inside ssh","text":"<p>When you access your tmux session on a remote machine from different client machines, based on the client machine configuration and terminal, etc., some features will not work. One of them is copy-paste.</p> <p>I used to have tmux-yank configured to <code>xclip</code>. But it didn't work will when I accessed my remote VM from a Windows machine running putty. Similarly I had to configure every new machine I start using which was time consuming. Majority of my copy-paste activities will be between tmux panes and windows. I don't usually copy from the local machine to the remote machine. So I used this simple hack.</p> <p>Configured my <code>.tmux.conf</code> to redirect <code>yank</code>-ed to a temporary file as below. <pre><code>bind -T copy-mode-vi y send-keys -X copy-pipe-and-cancel 'cat &gt; /tmp/clipboard'\n</code></pre></p> <p>Configured <code>.zshrc</code> to load an environment variable from that temporary file on every prompt. <pre><code>precmd() { export p=`cat /tmp/clipboard` }\n</code></pre></p> <p>So if I copy any text in tmux using <code>y</code>, it will be populated into a environmental variable named <code>p</code>.</p> <p>NOTE: As the environmental variable will be loaded upon next prompt only, after copying, you need to press <code>enter</code> once it to take effect</p> <p>Below gif will show how it works </p>","tags":["tmux","linux","hack"]},{"location":"blog/2021/07/31/windows-10-port-forwarding/","title":"Windows-10 port forwarding","text":"<p>Below is the command to port-forward in Windows-10. It can be executed in normal command-prompt not in PowerShell. Make sure you've administrator privilege. This will be useful in when you run a VM in your Windows workstation. So you can directly ssh to the VM instead of opening RDB every time.</p> <pre><code>netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=18001 connectaddress=192.168.105.115 connectport=22\n</code></pre> <p>Now this machine forwards any incoming connection on port number 18001 to 192.168.105.115 IP's port 22. To ssh into the machine 192.168.105.115 from an external network, you should issue the following command (provided that windows machine IP is 112.172.29.29).</p> <pre><code> $ ssh 112.172.29.29 -p 18001\n</code></pre>","tags":["windows","hack","portproxy"]},{"location":"blog/2025/08/23/fine-tuning-llms-to-predict-nifty50-prices/","title":"Fine Tuning LLMs to Predict NIFTY50 Price","text":"<p>Language has developed naturally over time, without strict rules in its early stages. This makes it one of the first structured systems shaped by human behavior. By studying language, we can uncover patterns in human thinking and actions. Large Language Models (LLMs) have shown impressive abilities to understand and predict human language. In a similar way, business systems like pricing have also evolved organically. Pricing patterns existed long before formal accounting rules were created. If LLMs can predict the next word in a sentence by understanding language patterns, could they also predict the next price using historical data? This article dives into how LLMs can be trained and fine-tuned to analyze NIFTY50 price trends and predict future movements, exploring its potential in financial markets.</p> <p>My focus here is only Intraday trading. Because we don't have any price value for NIFTY in the closed hours of market and there can be too many factors affecting the next day opening price. So, trying to predict price across days with current data is nothing but halucination.</p> <p>Our system will keep watching the market movement until 2.30 PM. And based on the movement so far, it will make a call or put at 2.30 and square off before the market ends. We'll ask the model to predit a target price. There will be a stop-loss calculated programatically. Additionally, the cutoff time will be 3.25 PM to close the position. So, if neither target price or stop-loss is reached, the position will be squared off for whatever price at 3.25 PM.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series"]},{"location":"blog/2025/08/23/fine-tuning-llms-to-predict-nifty50-prices/#data-preperation","title":"Data Preperation","text":"<p>The performance of a model is relying only on the data it is trained. So, collecting and preparing the historical Nifty50 data is very crucial. Simply dumping the historical data to any LLM won't make any good other than getting you pay for the GPUs.</p> <p>Prepare the environment <pre><code> $ python3 -m venv .venv\n $ source .venv/bin/activate\n $ pip install pandas\n</code></pre></p> <p>I took the 9 years of Nifty-50 candlestick data from this GitHub repo. It has minute level Open, High, Low and Close information for every market functioning day. We just need one number per minute. So, let's remove everything else other than open. <pre><code>import pandas as pd\n\ncandle_stick_data = pd.read_csv(\"dataset/nifty50_candlestick_data.csv\")\ncandle_stick_data[\"datetime\"] = pd.to_datetime(candle_stick_data[\"Date\"] + \" \" + candle_stick_data[\"Time\"], format=\"%d-%m-%Y %H:%M:%S\")\ncandle_stick_data.set_index(\"datetime\", inplace=True)\ncandle_stick_data.drop(columns=[\"Date\", \"Time\", \"High\", \"Low\", \"Close\", \"Instrument\"], inplace=True, errors=\"ignore\")\n\nn50_minute_level_opens = candle_stick_data\nn50_minute_level_opens.head()\n</code></pre> Output</p> Open datetime 2015-01-09 09:15:00 8285.45 2015-01-09 09:16:00 8292.60 2015-01-09 09:17:00 8287.40 2015-01-09 09:18:00 8294.25 2015-01-09 09:19:00 8300.60 <p></p> <p>We need to train the model on daily movements. So, the data should be grouped date-wise. In the dataset the second value is not proper. As we don't worry much about the second, we'll unify that to the the minute. Also, remove any data that is beyond typical Indian market hours.</p> <p><pre><code>market_hours_filter = (n50_minute_level_opens.index.time &gt;= pd.Timestamp('09:15:00').time()) &amp; \\\n                      (n50_minute_level_opens.index.time &lt;= pd.Timestamp('15:30:00').time())\n\nn50_min_opens = n50_minute_level_opens[market_hours_filter].copy()\n\nn50_min_opens['date'] = n50_min_opens.index.date\nn50_min_opens['time'] = n50_min_opens.index.strftime('%H:%M')\n\nn50_daily_opens = n50_min_opens.pivot_table(\n    index='date',\n    columns='time',\n    values='Open',\n    aggfunc='first'  # In case there are duplicates, take the first value\n)\n\nn50_daily_opens.head()\n</code></pre> Output</p> time 09:15 09:16 09:17 09:18 09:19 09:20 09:21 09:22 09:23 09:24 ... 15:20 15:21 15:22 15:23 15:24 15:25 15:26 15:27 15:28 15:29 date 2015-01-09 8285.45 8292.60 8287.40 8294.25 8300.6 8300.50 8300.65 8302.45 8294.85 8295.20 ... 8280.8 8282.35 8283.40 8284.35 8286.9 8286.65 8283.45 8282.35 8283.25 8280.50 2015-01-12 8291.35 8254.20 8255.25 8258.15 8263.2 8267.45 8266.05 8268.80 8273.85 8266.75 ... 8329.5 8326.55 8328.05 8328.05 8327.2 8330.20 8330.90 8329.95 8329.95 8328.85 2015-01-13 8346.15 8355.15 8348.70 8344.50 8342.5 8340.35 8339.75 8340.45 8333.30 8326.05 ... 8304.9 8305.75 8306.50 8307.15 8308.0 8308.20 8308.25 8307.25 8305.85 8308.20 2015-01-14 8307.25 8300.85 8307.00 8309.05 8305.4 8304.70 8302.20 8293.10 8296.70 8306.85 ... 8280.1 8278.90 8280.90 8283.60 8284.3 8285.35 8285.50 8286.95 8288.30 8288.90 2015-01-15 8425.20 8440.45 8394.35 8386.05 8401.1 8428.00 8408.25 8398.00 8416.70 8421.95 ... 8497.6 8491.80 8482.05 8477.25 8468.0 8463.80 8469.05 8464.80 8467.25 8467.45 <p></p> <p>This data is very refreshing. Nifty50 was in its eight thousands in 2015. If you have invested in the index, you would've trippled your money in the past 10 years. This gives us a small problem. I try to make the LLM to understand the trend in human price setting behaviour. Whether the price is 8000 or 24000, the trend should be same. But if I pass different prices (tokens), LLM may consider them as different behaviours. This may lead to a situation where LLM will give less importance to the original feature that defines the trend.</p> <p>So, I decide to pass the price difference in percentage instead of passing the price itself. The idea here is to keep the open price at 9.15 as the reference and calculate the difference in percentage for 9:16. Then using price of 9.16 as reference calculate the different for 9.17. Like this we continue for the whole day with respective to the previous minute price. My assumption is whatever the price is, we humans tend to set the new price relatively. <pre><code># Calculate percentage price movements within each day\n# For each day, calculate percentage change from previous minute\nn50_daily_price_movements = n50_daily_opens.pct_change(axis=1, fill_method=None) * 100\n\n# Set the first column (first minute of each day) to 0 as there's no reference price\nn50_daily_price_movements.iloc[:, 0] = 0\n\nn50_daily_price_movements.head()\n</code></pre> Output</p> time 09:15 09:16 09:17 09:18 09:19 09:20 09:21 09:22 09:23 09:24 ... 15:20 15:21 15:22 15:23 15:24 15:25 15:26 15:27 15:28 15:29 date 2015-01-09 0.0 0.086296 -0.062707 0.082656 0.076559 -0.001205 0.001807 0.021685 -0.091539 0.004219 ... -0.008453 0.018718 0.012678 0.011469 0.030781 -0.003017 -0.038616 -0.013279 0.010866 -0.033200 2015-01-12 0.0 -0.448057 0.012721 0.035129 0.061152 0.051433 -0.016934 0.033269 0.061073 -0.085813 ... 0.042037 -0.035416 0.018015 0.000000 -0.010206 0.036027 0.008403 -0.011403 0.000000 -0.013205 2015-01-13 0.0 0.107834 -0.077198 -0.050307 -0.023968 -0.025772 -0.007194 0.008394 -0.085727 -0.087000 ... 0.080137 0.010235 0.009030 0.007825 0.010232 0.002407 0.000602 -0.012036 -0.016853 0.028293 2015-01-14 0.0 -0.077041 0.074089 0.024678 -0.043928 -0.008428 -0.030103 -0.109610 0.043410 0.122338 ... 0.078563 -0.014493 0.024158 0.032605 0.008450 0.012675 0.001810 0.017500 0.016291 0.007239 2015-01-15 0.0 0.181005 -0.546179 -0.098876 0.179465 0.320196 -0.234338 -0.121904 0.222672 0.062376 ... -0.042347 -0.068255 -0.114817 -0.056590 -0.109116 -0.049598 0.062029 -0.050183 0.028943 0.002362 <p>Let's analyze the price movement for insights. Since the true value of an asset (like Nifty50) is often unclear, we use the current and previous prices as proxies\u2014a concept tied to Daniel Kahneman's Anchoring Bias. This bias suggests that sudden price increases are likely to be corrected downward, while sharp decreases are adjusted upward. As a result, the average price movement should ideally converge to zero. <pre><code># Calculate statistics excluding NaN values and the first column (which is all zeros)\nmovements_data = n50_daily_price_movements.iloc[:, 1:].values.flatten()  # Exclude first column\nmovements_data_clean = movements_data[~pd.isna(movements_data)]  # Remove NaN values\n\nprint(f\"Total data points: {len(movements_data_clean):,}\")\nprint(f\"Mean movement: {movements_data_clean.mean():.4f}%\")\nprint(f\"Std deviation: {movements_data_clean.std():.4f}%\")\nprint(f\"Min movement: {movements_data_clean.min():.4f}%\")\nprint(f\"Max movement: {movements_data_clean.max():.4f}%\")\n</code></pre> Output <pre><code>Total data points: 849,150\nMean movement: -0.0002%\nStd deviation: 0.0409%\nMin movement: -2.4480%\nMax movement: 6.2991%\n</code></pre></p> <p>Our assumption didn't go wrong. The mean value is very close to zero, -0.0002%. If you see the standard deviation, it is around 0.04%. It means in the last 10 years, for more than 68% percentage of the time we quoated the new price with is \u00b10.04% relative to the current price.</p> <p>Note: In our case the \u00b11\u03c3 is 82.69%. It means 82% of times we qouted the new price within \u00b10.04% of the current price. Similarly Nifty50 movement distribution's \u00b12\u03c3 is 96.01% whereas typical distribution is 95.45%. At 2\u03c3 only the price movement converges to normal statistics. The \u00b13\u03c3 is 98.64% but normal is 99.73% - we have more outliers here.</p> <p>This insight reveals that we tend to adopt a more cautious approach under normal circumstances. However, when pushed to the edge, we take significantly higher risks. The difference in the 3\u03c3 range is notable. While 4.28% of data points are expected in this range, only 2.63% are present\u2014nearly half are missing. This suggests that when we decide to take risks, we often overextend, taking unnecessary risks about 50% of the time.</p> <p>I'm leaving it as an excercise for you to calculate the ranges yourself.</p> <p>Let's clean the data.</p> <ol> <li>Fill any NaN with previous value</li> <li>Fix the precision to two decimal degits. It's safe to have 0.04% as our approximate std. <pre><code># Fill NaN values with previous value (forward fill along rows)\nn50_daily_price_movements = n50_daily_price_movements.ffill(axis=1)\n\n# Round to 2 decimal places\nn50_daily_price_movements = n50_daily_price_movements.round(2)\n\nprint(f\"Price movements DataFrame shape: {n50_daily_price_movements.shape}\")\nprint(f\"NaN values remaining: {n50_daily_price_movements.isna().sum().sum()}\")\n\nn50_daily_price_movements.head()\n</code></pre> Output <pre><code>Price movements DataFrame shape: (2273, 375)\nNaN values remaining: 0\n</code></pre></li> </ol> time 09:15 09:16 09:17 09:18 09:19 09:20 09:21 09:22 09:23 09:24 ... 15:20 15:21 15:22 15:23 15:24 15:25 15:26 15:27 15:28 15:29 date 2015-01-09 0.0 0.09 -0.06 0.08 0.08 -0.00 0.00 0.02 -0.09 0.00 ... -0.01 0.02 0.01 0.01 0.03 -0.00 -0.04 -0.01 0.01 -0.03 2015-01-12 0.0 -0.45 0.01 0.04 0.06 0.05 -0.02 0.03 0.06 -0.09 ... 0.04 -0.04 0.02 0.00 -0.01 0.04 0.01 -0.01 0.00 -0.01 2015-01-13 0.0 0.11 -0.08 -0.05 -0.02 -0.03 -0.01 0.01 -0.09 -0.09 ... 0.08 0.01 0.01 0.01 0.01 0.00 0.00 -0.01 -0.02 0.03 2015-01-14 0.0 -0.08 0.07 0.02 -0.04 -0.01 -0.03 -0.11 0.04 0.12 ... 0.08 -0.01 0.02 0.03 0.01 0.01 0.00 0.02 0.02 0.01 2015-01-15 0.0 0.18 -0.55 -0.10 0.18 0.32 -0.23 -0.12 0.22 0.06 ... -0.04 -0.07 -0.11 -0.06 -0.11 -0.05 0.06 -0.05 0.03 0.00 <p> To further refine the dataset, we will remove high-stress days. These are days where extreme emotions drive price movements, leading to significant variations. Including such days in the dataset might confuse the model, as it could attempt to scale these outliers alongside normal trading days. By excluding these high-stress days, we aim to create a more consistent dataset that better represents typical market behavior.</p> <p>By removing these high-stress days, we ensure that the dataset focuses on regular market conditions, which are more representative of typical trading patterns. This adjustment will help the model generalize better and avoid overfitting to rare, extreme scenarios.</p> <p>To identify and filter out high-stress days, we will use the daily standard deviation as a measure of volatility. Days with a standard deviation outside the range of \u00b12\u03c3 (calculated from the mean daily standard deviation) will be considered high-stress days and excluded from the dataset. Here we calculate the std of every day. And put that in a series then calculate mean and std of that series. So, please don't get confused with <code>std of std</code>. <pre><code># Calculate daily standard deviation for each trading day\ndaily_std = n50_daily_price_movements.std(axis=1)  # std across columns (time) for each day\n\nprint(f\"Daily std statistics:\")\nprint(f\"Mean daily std: {daily_std.mean():.4f}%\")\nprint(f\"Std of daily std: {daily_std.std():.4f}%\")\nprint(f\"Min daily std: {daily_std.min():.4f}%\")\nprint(f\"Max daily std: {daily_std.max():.4f}%\")\n\n# Calculate the mean and std of daily standard deviations\nmean_daily_std = daily_std.mean()\nstd_daily_std = daily_std.std()\n\n# Define the acceptable range (\u00b12\u03c3)\nlower_bound = mean_daily_std - 2 * std_daily_std\nupper_bound = mean_daily_std + 2 * std_daily_std\n\nprint(f\"\\nAcceptable daily std range: {lower_bound:.4f}% to {upper_bound:.4f}%\")\n\n# Filter days that fall within \u00b12\u03c3 of mean daily std\ndays_within_2sigma = (daily_std &gt;= lower_bound) &amp; (daily_std &lt;= upper_bound)\n\nprint(f\"\\nDays analysis:\")\nprint(f\"Total days before filtering: {len(n50_daily_price_movements)}\")\nprint(f\"Days within \u00b12\u03c3: {days_within_2sigma.sum()}\")\nprint(f\"Days to remove: {len(n50_daily_price_movements) - days_within_2sigma.sum()}\")\nprint(f\"Percentage kept: {days_within_2sigma.sum() / len(n50_daily_price_movements) * 100:.2f}%\")\n\n# Apply the filter\nn50_daily_price_movements_filtered = n50_daily_price_movements[days_within_2sigma]\nn50_daily_opens_filtered = n50_daily_opens[days_within_2sigma]\n\nprint(f\"\\nFiltered dataset shape:\")\nprint(f\"Price movements: {n50_daily_price_movements_filtered.shape}\")\nprint(f\"Daily opens: {n50_daily_opens_filtered.shape}\")\n\n# Show some examples of removed days (outliers)\noutlier_days = n50_daily_price_movements[~days_within_2sigma]\nif len(outlier_days) &gt; 0:\n    print(f\"\\nExamples of removed days (high/low volatility):\")\n    print(f\"Highest volatility day: {daily_std.idxmax()} (std: {daily_std.max():.4f}%)\")\n    print(f\"Lowest volatility day: {daily_std.idxmin()} (std: {daily_std.min():.4f}%)\")\n\nn50_daily_price_movements_filtered.head()\n</code></pre> Output <pre><code>Daily std statistics:\nMean daily std: 0.0351%\nStd of daily std: 0.0211%\nMin daily std: 0.0104%\nMax daily std: 0.4735%\n\nAcceptable daily std range: -0.0070% to 0.0772%\n\nDays analysis:\nTotal days before filtering: 2273\nDays within \u00b12\u03c3: 2221\nDays to remove: 52\nPercentage kept: 97.71%\n\nFiltered dataset shape:\nPrice movements: (2221, 375)\nDaily opens: (2221, 375)\n\nExamples of removed days (high/low volatility):\nHighest volatility day: 2020-03-13 (std: 0.4735%)\nLowest volatility day: 2024-03-02 (std: 0.0104%)\n</code></pre></p> time 09:15 09:16 09:17 09:18 09:19 09:20 09:21 09:22 09:23 09:24 ... 15:20 15:21 15:22 15:23 15:24 15:25 15:26 15:27 15:28 15:29 date 2015-01-09 0.0 0.09 -0.06 0.08 0.08 -0.00 0.00 0.02 -0.09 0.00 ... -0.01 0.02 0.01 0.01 0.03 -0.00 -0.04 -0.01 0.01 -0.03 2015-01-12 0.0 -0.45 0.01 0.04 0.06 0.05 -0.02 0.03 0.06 -0.09 ... 0.04 -0.04 0.02 0.00 -0.01 0.04 0.01 -0.01 0.00 -0.01 2015-01-13 0.0 0.11 -0.08 -0.05 -0.02 -0.03 -0.01 0.01 -0.09 -0.09 ... 0.08 0.01 0.01 0.01 0.01 0.00 0.00 -0.01 -0.02 0.03 2015-01-14 0.0 -0.08 0.07 0.02 -0.04 -0.01 -0.03 -0.11 0.04 0.12 ... 0.08 -0.01 0.02 0.03 0.01 0.01 0.00 0.02 0.02 0.01 2015-01-15 0.0 0.18 -0.55 -0.10 0.18 0.32 -0.23 -0.12 0.22 0.06 ... -0.04 -0.07 -0.11 -0.06 -0.11 -0.05 0.06 -0.05 0.03 0.00 <p>This cleansing removed 52 days from the market.</p> <p>Most of the blogs and influencers will take only these 52 days. Probably, if we have of all the blogs and youtube transcripts and arrange the dates mentioned in a series, 2\u03c3 dates will fall in these 52 days only. You decide whether to make profit in 2221 normal days or you're going to wait for one of those 52 days.</p> <p>Finally, let's split the dataset into training and validation sets and store them as separate files. The model will be trained only on the training dataset. The validation set will not be exposed to the model during training or fine-tuning. We'll use the validation dataset to backtest whether the model is makeing any profit for us.</p> <p>Let's take out every nineth day into validation set. As it is more than seven, the subsequent nineth day will be different day of the week.</p> <pre><code># Split into training and validation sets\n# Every 9th day goes to validation, rest goes to training\ntotal_days = len(n50_daily_price_movements_filtered)\n\n# Create boolean masks for train/validation split\nvalidation_mask = [(i % 9 == 8) for i in range(total_days)]  # Every 9th day (0-indexed, so 8th position)\ntraining_mask = [not val for val in validation_mask]\n\n# Split the datasets\ntrain_price_movements = n50_daily_price_movements_filtered[training_mask]\nval_price_movements = n50_daily_price_movements_filtered[validation_mask]\n\ntrain_daily_opens = n50_daily_opens_filtered[training_mask]\nval_daily_opens = n50_daily_opens_filtered[validation_mask]\n\n# Save the datasets to CSV files in the dataset directory\nimport os\n\n# Create dataset directory if it doesn't exist\nos.makedirs('dataset', exist_ok=True)\n\n# Save training datasets\ntrain_price_movements.to_csv('dataset/train_price_movements.csv')\ntrain_daily_opens.to_csv('dataset/train_daily_opens.csv')\n\n# Save validation datasets\nval_price_movements.to_csv('dataset/val_price_movements.csv')\nval_daily_opens.to_csv('dataset/val_daily_opens.csv')\n\nprint(\"Datasets saved successfully!\")\n</code></pre>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series"]},{"location":"blog/2025/08/23/fine-tuning-llms-to-predict-nifty50-prices/#conclusion","title":"Conclusion","text":"<p>Let's wrap up this blog here. In the upcoming posts, we will explore various models and training methodologies to determine which approach yields the best results. The dataset prepared in this blog will serve as the foundation for all those experiments. You can access the code discussed in this blog in the n50_dataset_prep notebook.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/","title":"Fine-Tuning GPT model to Predict NIFTY50 Prices","text":"<p>In the previous post we have prepared a dataset consisting of minute level Nifty50 index price for the last 10 years. We've done some data cleansing and split the dataset into training and validation set. Please skim through that if you haven't yet. In this blog we're going to use the training set to fine-tune a GPT model in Azure AI Foundry. Then we'll use the validation set to check whether the fine-tuned model can make a profit for us.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#prepare-training-conversations","title":"Prepare Training Conversations","text":"<p>The prepared dataset is stored under the dataset directory. We'll use the train_price_movements.csv for fine-tuning the model. Here is the glimpse of the content of the file. <pre><code>import pandas as pd\ntraining_set = pd.read_csv('dataset/train_price_movements.csv')\ntraining_set.head()\n</code></pre> Output</p> date 09:15 09:16 09:17 09:18 09:19 09:20 09:21 09:22 09:23 ... 15:20 15:21 15:22 15:23 15:24 15:25 15:26 15:27 15:28 15:29 0 2015-01-09 0.0 0.09 -0.06 0.08 0.08 -0.00 0.00 0.02 -0.09 ... -0.01 0.02 0.01 0.01 0.03 -0.00 -0.04 -0.01 0.01 -0.03 1 2015-01-12 0.0 -0.45 0.01 0.04 0.06 0.05 -0.02 0.03 0.06 ... 0.04 -0.04 0.02 0.00 -0.01 0.04 0.01 -0.01 0.00 -0.01 2 2015-01-13 0.0 0.11 -0.08 -0.05 -0.02 -0.03 -0.01 0.01 -0.09 ... 0.08 0.01 0.01 0.01 0.01 0.00 0.00 -0.01 -0.02 0.03 3 2015-01-14 0.0 -0.08 0.07 0.02 -0.04 -0.01 -0.03 -0.11 0.04 ... 0.08 -0.01 0.02 0.03 0.01 0.01 0.00 0.02 0.02 0.01 4 2015-01-15 0.0 0.18 -0.55 -0.10 0.18 0.32 -0.23 -0.12 0.22 ... -0.04 -0.07 -0.11 -0.06 -0.11 -0.05 0.06 -0.05 0.03 0.00 <p></p> <p>Our expectation is when feed the price movement till 2.30PM, the model should tell us what is the most probable price it will reach before the market close. Based on that price we'll either buy or sell (short) the index at 2.30.</p> <p>OpenAI GPTs are conversational models. So, the training dataset should be converted into set of conversations. Each conversation should have a system message, a user message and an assistant message. In our case the system message will be a role assignment for the LLM. The user message will be the market movement and the assistant message will be the target price for that day.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#how-to-determine-target-price","title":"How to determine target price?","text":"<p>Before converting the current dataset into conversations, we need to establish a method for determining the target price for each day in the training dataset. This is essential because the training conversation requires the <code>target_price</code> to be included as the assistant's response. Therefore, we must devise a systematic approach to calculate the target price using the price movement data available after 2:30 PM.</p> <p>We have price movement in percentage from 2.31PM to 3.29PM. As we're going to square off our position by 3.25PM itself, let's consider only upto 3.25PM. And each number in the dataset is relative percentage difference from the previous price. So, it is a geometric progression.</p> <p>We should know price oscillations with respect to the price at 2.30 at every minute until 3.25. So, if we're buying at 2.30, we should sell at a time when the price reached the peak. Or if we're selling (shorting) we should buy back when the price is at the bottom of the graph. Here we are going to identify the most probable price (or movement) close to the maximum or minimum and use that as the target price.</p> <p>Please note that we're not taking the best price here. Because the best price will be an outlier in the sequence. It may not properly represent the trend in the price setting. So, take the price ocillation range and take the first standard deviation. By this we have approximately 66% possibility to reach the target price. And this is our expectation from the LLM also. To make profit in long-term, we trade high profit in a day for more stable returns.</p> <p>Let's create a sequence of price difference with respect to the base price at 2.30 and find it's mean and std. If the mean is positive, let's set the target price as <code>mean+std</code>. Otherwise the short target price is <code>mean-std</code>. Let's do this for first sequence in this dataset. <pre><code># Set the date column as index\ntraining_set_indexed = training_set.set_index('date')\n\ntarget_date = '2015-01-09'\ntarget_time = '14:30'\n\n# Get data for the target date\nday_data = training_set_indexed.loc[target_date]\n\n# Print the price movement at 2:30 PM\nprint(f\"Price movement at {target_time} on {target_date}: {day_data.loc[target_time]}%\")\n\n# Calculate cumulative price changes from 2:30 PM onwards\n# Starting with 1.0 at 2:30 PM (baseline)\nprice_diff_with_respect_to_230 = []\ntime_labels = []\ncur = 1.0  # Starting value at 2:30 PM\n\n# Generate times from 2:31 PM to 3:25 PM\nfor hour in [14, 15]:  # 2 PM and 3 PM in 24-hour format\n    start_min = 31 if hour == 14 else 0  # Start from 31 min for 2 PM, 0 min for 3 PM\n    end_min = 60 if hour == 14 else 26   # End at 60 min for 2 PM, 26 min for 3 PM\n\n    for minute in range(start_min, end_min):\n        time_str = f\"{hour}:{minute:02d}\"\n\n        if time_str in day_data.index:\n            # Apply the percentage change: new_value = current * (1 + percentage_change/100)\n            percentage_change = day_data.loc[time_str]\n            cur = cur * (1 + percentage_change / 100)\n            price_diff_with_respect_to_230.append(cur)\n            time_labels.append(time_str)\n\n# Create DataFrame with results\nresult_df = pd.DataFrame({\n    'time': time_labels,\n    'cumulative_price_factor': price_diff_with_respect_to_230\n})\n\nprint(f\"\\nCumulative price changes from 2:30 PM baseline:\")\nprint(f\"Mean: {result_df['cumulative_price_factor'].mean():.2f}\")\nprint(f\"Std: {result_df['cumulative_price_factor'].std():.2f}\")\nprint(f\"Min: {result_df['cumulative_price_factor'].min():.2f}\")\nprint(f\"Max: {result_df['cumulative_price_factor'].max():.2f}\")\n\nmean = result_df['cumulative_price_factor'].mean()\nstd = result_df['cumulative_price_factor'].std()\n\nif mean &gt;= 1:\n    position = \"Call\"\n    target_percentage = round((mean + std), 4)\nelse:\n    position = \"Put\"\n    target_percentage = round((mean - std), 4)\n\nprint(f\"{position}. Target Percentage: {target_percentage}\")\n</code></pre> Output <pre><code>Price movement at 14:30 on 2015-01-09: 0.01%\n\nCumulative price changes from 2:30 PM baseline:\nMean: 1.35\nStd: 0.24\nMin: 0.90\nMax: 1.84\nCall. Target Percentage: 1.0053\n</code></pre></p> <p>By multiplying the <code>target_percentage</code> with the base price i.e. the price at 2.30PM, we'll get the target price. We have the price sequence data in <code>dataset/train_daily_opens.csv</code>. Let's calculate the position and target price for <code>09-01-2015</code>. <pre><code>training_prices = pd.read_csv('dataset/train_daily_opens.csv')\n# training_prices.head()\ntraining_prices.set_index('date', inplace=True)\nday_price = training_prices.loc[target_date]\nbase_price = day_price.loc[target_time]\nprint(f\"Base Price at {target_time} on {target_date}: {base_price}\")\ntarget_price = base_price * target_percentage\nprint(f\"{position}. Target Price: {target_price}\")\n</code></pre> OutPut <pre><code>Base Price at 14:30 on 2015-01-09: 8240.0\nCall. Target Price: 8283.672\n</code></pre> This price calculation is only for illustration purposes. We're not going to pass it to the LLM. Because, we're not going to give it any price related context. We'll pass only the price movement in percentage and expect it to provide the multiplication factor. We'll multiply the factor with the base price and determine the target price programmatically.</p> <p>To prepare the training dataset for fine-tuning, we need to convert the entire dataset into a conversational format and save it as a JSONL file. For each day in the dataset, we will calculate the multiplication factor (<code>target_percentage</code>) following the above method. The model will be trained to predict only the <code>target_percentage</code> given the market movement sequence. The <code>target_price</code> will be derived later using the <code>base_price</code> at the time of evaluation. <pre><code>training_conversations = []\n\n# traverse every item in the training_set_indexed\nfor index, row in training_set_indexed.iterrows():\n    date = index  # The date is the index, not row['date']\n\n    # Convert date string to datetime to get day name\n    date_obj = pd.to_datetime(date)\n    day_of_the_date = date_obj.day_name()\n\n    # Filter row to only include times from 9:15 AM to 2:30 PM\n    # Create time range from 09:15 to 14:30\n    times_until_230 = []\n    for hour in range(9, 15):  # 9 AM to 2 PM\n        start_min = 15 if hour == 9 else 0  # Start from 15 min for 9 AM, 0 min for others\n        end_min = 60\n        for minute in range(start_min, end_min):\n            times_until_230.append(f\"{hour:02d}:{minute:02d}\")\n\n    # Add 14:30 (2:30 PM)\n    times_until_230.append(\"14:30\")\n\n    # Filter row data to only include times until 2:30 PM\n    row_until_230 = row[row.index.isin(times_until_230)]\n\n    # Get the current day's data for post-2:30 PM calculations\n    day_data = row  # This is the full day's data\n\n    percent_diff_with_respect_to_230 = []\n    time_labels = []\n    cur = 1.0  # Take the base as 1\n\n    for hour in [14, 15]:\n        start_min = 31 if hour == 14 else 0\n        end_min = 60 if hour == 14 else 26  # Fixed: changed 'or' to 'else'\n\n        for minute in range(start_min, end_min):  # Fixed: added colon\n            time_str = f\"{hour}:{minute:02d}\"\n\n            if time_str in day_data.index:\n                percentage_change = day_data.loc[time_str]\n                cur = cur * (1 + percentage_change / 100)\n                percent_diff_with_respect_to_230.append(cur)\n                time_labels.append(time_str)\n\n    result_df = pd.DataFrame({\n        'time': time_labels,\n        'cumulative_price_factor': percent_diff_with_respect_to_230\n    })\n\n    mean = result_df['cumulative_price_factor'].mean()\n    std = result_df['cumulative_price_factor'].std()\n\n    if mean &gt;= 1:\n        position = \"Call\"\n        target_percentage = round((mean + std), 4)\n    else:\n        position = \"Put\"\n        target_percentage = round((mean - std), 4)\n\n    base_price = training_prices.loc[date].loc['14:30']\n    target_price = base_price * target_percentage\n\n    # Use row_until_230 instead of full row for the conversation\n    conversation = {\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a stock market expert. You will predict the most profit making position and what is the expected percentage change in the next 1 hour given the percentage movement between 9.15 AM and 2.30 PM.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"The price movements in percentage from 9:15 AM to 2:30 PM on \" + str(date) + \" (\" + day_of_the_date + \") are as follows: \" + str(row_until_230.values.tolist()) + \".\"\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": str(target_percentage)\n            }\n        ]\n    }\n\n    training_conversations.append(conversation)\n\nprint(f\"Generated {len(training_conversations)} training conversations\")\nprint(training_conversations[0])\n\n# Store `training_conversations` in a jsonl file.\nimport json\n\nwith open(\"dataset/training_conversations_gpt.jsonl\", \"w\", encoding=\"utf-8\") as f:\n    for conversation in training_conversations:\n        f.write(json.dumps(conversation) + \"\\n\")\n</code></pre> Output <pre><code>Generated 1975 training conversations\n{'messages': [{'role': 'system', 'content': 'You are a stock market expert. You will predict the most profit making position and what is the expected percentage change in the next 1 hour given the percentage movement between 9.15 AM and 2.30 PM.'}, {'role': 'user', 'content': 'The price movements in percentage from 9:15 AM to 2:30 PM on 2015-01-09 (Friday) are as follows: [0.0, 0.09, -0.06, 0.08, 0.08, -0.0, 0.0, 0.02, -0.09, 0.0, 0.08, -0.08, -0.07, 0.03, -0.02, 0.04, -0.06, 0.0, -0.08, 0.01, 0.07, -0.05, 0.0, -0.07, 0.04, 0.01, 0.02, 0.01, -0.07, -0.03, 0.09, -0.05, 0.02, -0.04, -0.01, 0.06, -0.1, 0.04, 0.0, -0.0, -0.03, -0.01, 0.05, 0.04, -0.04, 0.03, 0.02, 0.01, 0.02, -0.02, 0.01, -0.02, 0.02, -0.03, 0.0, -0.04, -0.01, -0.04, 0.02, 0.02, 0.03, 0.01, -0.02, -0.0, 0.03, 0.01, 0.0, 0.03, 0.0, -0.01, 0.02, 0.01, -0.02, 0.02, 0.03, -0.0, -0.01, -0.04, 0.02, -0.01, 0.01, -0.02, -0.03, 0.02, -0.01, 0.02, -0.02, -0.02, 0.0, -0.01, -0.01, -0.02, 0.03, -0.02, 0.01, -0.0, -0.01, 0.02, 0.02, -0.01, 0.02, -0.02, 0.03, 0.01, 0.0, -0.01, 0.02, -0.01, -0.05, 0.03, 0.03, -0.02, -0.02, -0.02, -0.02, 0.01, 0.02, -0.01, -0.0, 0.04, -0.04, -0.0, -0.03, -0.02, -0.01, 0.02, -0.01, 0.02, 0.02, -0.13, -0.03, -0.03, 0.02, -0.0, -0.0, -0.02, -0.13, 0.06, -0.07, 0.0, 0.04, 0.05, -0.02, -0.05, -0.01, -0.01, 0.02, 0.03, 0.04, -0.04, 0.0, 0.04, 0.0, -0.01, 0.01, -0.06, 0.02, -0.01, 0.01, -0.01, 0.02, 0.02, -0.01, 0.01, 0.0, 0.0, -0.04, 0.02, -0.05, -0.06, -0.45, 0.05, 0.02, 0.01, 0.04, -0.11, -0.09, -0.02, 0.12, 0.04, 0.08, 0.02, -0.05, 0.01, 0.01, -0.0, -0.05, 0.03, 0.0, 0.07, -0.06, 0.04, -0.09, -0.08, 0.05, 0.01, -0.0, 0.03, 0.0, -0.06, -0.04, 0.0, 0.01, -0.02, -0.08, 0.03, 0.06, -0.02, -0.01, 0.06, 0.03, 0.07, -0.03, -0.1, 0.1, -0.01, -0.02, 0.15, 0.21, -0.02, 0.03, -0.17, 0.15, 0.07, 0.12, 0.08, -0.06, 0.07, -0.06, -0.05, 0.02, 0.07, -0.03, 0.07, 0.01, 0.02, 0.01, -0.06, -0.01, -0.07, -0.15, -0.07, -0.04, 0.01, -0.07, 0.16, -0.06, -0.07, -0.04, 0.02, 0.01, 0.0, 0.04, 0.02, 0.04, -0.0, 0.04, -0.08, 0.02, 0.07, -0.0, -0.07, 0.03, 0.03, 0.04, 0.02, -0.07, 0.01, -0.04, 0.01, 0.03, -0.0, 0.01, -0.02, 0.0, -0.1, 0.04, 0.02, -0.03, -0.04, -0.05, -0.02, -0.01, -0.17, -0.12, 0.14, -0.09, 0.02, 0.0, 0.1, 0.01, 0.04, 0.03, 0.05, -0.08, 0.08, -0.0, 0.04, 0.07, -0.02, -0.16, 0.09, 0.06, 0.04, 0.01, 0.01, 0.02, 0.04, 0.01, -0.05, -0.08, 0.04, -0.03, 0.0, 0.02, 0.01, 0.06, -0.04, -0.12, 0.01, 0.06, 0.04, 0.05, 0.01, 0.07, 0.01, -0.05, -0.01, 0.06, 0.02, -0.03, -0.0, -0.03, 0.14, -0.01, -0.03, 0.07, 0.05, 0.09, 0.02, -0.02, -0.07, -0.0, -0.02, -0.04].'}, {'role': 'assistant', 'content': '1.0053'}]}\n</code></pre></p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#training-gpt-llm","title":"Training GPT LLM","text":"<p>I refer to the Azure Documentation to fine-tune an OpenAI GPT model. To do so we need to use Serverless Training Infrastructure. This is cheaper and easier to handle compared to the alternative - Managed Compute Infrastructure. Azure offers three kinds of fine-tuning.</p> <ol> <li>SFT - Supervised Fine Tuning</li> <li>DPO - Direct Preference Optimization</li> <li>RFT - Reinforcement Fine-Tuning</li> </ol> <p>Though RFT is recommended for tasks like stock market prediction, we'll go with SFT this time for its simplicity. Our training file is prepared for SFT only. In the future articles, we'll explore other training methods and compare their performance.</p> <p>To follow this article and fine-tune a model, you need to be an owner or contributor of a Paid Azure Account. Free or trail Azure accounts may not work. We Microsoft employees get $150 Azure credit per month. I'm using that credit for this project.</p> <p>Now we're going to follow the step-by-step instructions provided at Customize a model with fine-tuning. It is a well written document with screenshots. So, I'm not going to elaborate each step here in this blog.</p> <p>Here is a short quote from the Azure documentation.</p> <p>If you train the model on a large amount of internal data, without first pruning the dataset for only the highest quality examples you could end up with a model that performs much worse than expected.</p> <p>This is the reason we have done all the pre-processing in the previous dataset prep blog.</p> <p>Below are the steps to be followed to fine-tune a GPT model. I've decided to use <code>gpt-4.1-mini</code> as the base model. It is faster and cheaper because of it's size.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#1-create-an-azure-hub-project","title":"1. Create an Azure Hub project","text":"<p>Create a new Azure Hub resource in Azure AI Foundry. Because Azure AI Foundry resource doesn't have an option to fune-tune models.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#2-start-fine-tuning","title":"2. Start Fine-Tuning","text":"<p>Once the project is opened in the Azure AI Foundry, select Fine-Tuning from ...More. Refer to the screenshot below.</p> <p></p> <p>In the newly opened dialogue, click the Fine Tune a Model button and select your base model. In my case, I selected gpt-4.1-mini. This step will deploy a resource with the pre-trained model in a specified location. Detailed step-by-step instructions with screenshots can be found in the documentation mentioned above.</p> <p>Once the resource is created, you'll be asked to select the fine-tuning method and training data.</p> <p></p> <p>Keep the method to default as Supervised and then select the jsonl file we have created in the previous section using \"Upload Files\" method. We don't have any validation set. We could have split the training-set into training and validation. Just for simplicity we're not doing it now.</p> <p>The validation set prepared in the previous blog is intentionally excluded from this step. This ensures that the validation dataset remains untouched by the LLM during training, preserving its integrity for independent backtesting and evaluation purposes.</p> <p>Leave every other value to default and click Submit. Now fine-tuning will start. This will take some hours based on how quickly the descent of loss occurs. Here is the loss function drop graph after 45 minutes of training.</p> <p></p> <p>The training got completed after 2 hours. Suppose if you're using bigger models like gpt-4.1, you can expect even longer.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#backtesting","title":"Backtesting","text":"<p>With the training complete and the final loss stabilizing around 0.6, the model shows potential for making accurate predictions. The next step is to test its real-world performance by evaluating if GPT-4.1-mini can generate profitable trades in NIFTY50 index trading on our validation set.</p> <p>Deploy the fine-tuned model and copy it's API key and Endpoints to the <code>.env</code> file as below. <pre><code>(.venv) bala@bala-swe-test-ubuntu-2404:~/0xba1a.github.com$ cat .env\nAZURE_API_KEY='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\nAZURE_API_BASE='https://ai-bkannan-per-XXXXXXXXXXXXXXXXX.cognitiveservices.azure.com/'\nAZURE_API_VERSION='2024-12-01-preview'\nAZURE_DEPLOYMENT=\"gpt-4-1-mini-2025-04-14-ft-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n</code></pre> Replace <code>XXX</code> with appropriate values.</p> <p>We use AzureOpenAI library to inference with the newly deployed model. Similar to the training conversation prep we need to prepare conversations using the validation dataset and send to the LLM.</p> <p>Let me give 1 million rupees initial value and see how much profit it makes in the 247 days of validation dataset.</p> <p>Make sure that you've installed <code>openai</code> in your <code>venv</code>. <pre><code>(.venv) $ pip install openai\n</code></pre>  This is a big chunk of code. In the notebook you will find it in a single cell. For easier understanding let me give it in multiple chunks. Import all the necessary modules and setup variables to interact with Azure-Foundry. And create a AzureOpenAI client to start the inferencing.</p> <pre><code>import pandas as pd\nfrom dotenv import load_dotenv\nimport os\nfrom openai import AzureOpenAI\nimport time\nimport random\n\nload_dotenv()\n\nendpoint = os.environ.get('AZURE_API_BASE')\nmodel_name = \"gpt-4.1-mini\"\ndeployment = os.environ.get('AZURE_DEPLOYMENT')\n\nsubscription_key = os.environ.get('AZURE_API_KEY')\napi_version = os.environ.get('AZURE_API_VERSION')\n\nclient = AzureOpenAI(\n    api_version=api_version,\n    azure_endpoint=endpoint,\n    api_key=subscription_key,\n)\n</code></pre> <p>Next setup some global variables to control the flow. We've <code>offset</code> and <code>count</code> to test the validation set partially. <code>cash_in_hand</code> represents how much cash is available at any day of the trade. Initially we set it to 1 million INR. Then we've metrics object which counts number profits, losses, time-outs, etc.,. Time out is nothing but time 3.26 PM is reached before making any trade for the day. The variable <code>message</code> is going to have the system and user prompt before sending them to the LLM. <pre><code>count = 1000\noffset = 0\nfor_index = 0\ncash_in_hand = 1000000 # 1 Million starting investment\nmetrics = {\n    \"total_trades\": 0,\n    \"profitable_trades\": 0,\n    \"unprofitable_trades\": 0,\n    \"total_profit\": 0,\n    \"total_loss\": 0,\n    \"time_up\": 0,\n    \"stop_loss\": 0\n}\nmessage = []\n</code></pre></p> <p>Load the validation datasets - price movement and open prices itself. <pre><code>validation_set = pd.read_csv('dataset/val_price_movements.csv')\nvalidation_set.set_index('date', inplace=True)\n\nvalidation_price = pd.read_csv('dataset/val_daily_opens.csv')\nvalidation_price.set_index('date', inplace=True)\n</code></pre></p> <p>Create a list which has time string from 09:15 AM to 02:30 PM. This list will serve as an index filter for filtering the validation dataset. <pre><code># Create time range from 09:15 to 14:30\ntimes_until_230 = []\nfor hour in range(9, 14):  # 9 AM to 2 PM\n    start_min = 15 if hour == 9 else 0  # Start from 15 min for 9 AM, 0 min for others\n    end_min = 30 if hour == 14 else 60\n    for minute in range(start_min, end_min):\n        times_until_230.append(f\"{hour:02d}:{minute:02d}\")\n\n# Add 14:30 (2:30 PM)\ntimes_until_230.append(\"14:30\")\n</code></pre></p> <p>Start iterating over every day in the validation set. Skip <code>offset</code> days without processing them. <pre><code>for index, row in validation_set.iterrows():\n\n    for_index += 1\n    if for_index &lt;= offset:\n        continue\n</code></pre></p> <p>When starting to process a day, first print the date and the cash-in-hand. Then there is a random delay between 3 and 5 seconds. This delay is to avoid hitting the LLM API ratelimiting. Then we get the date from the index and find out the day of the week of that trade day. <pre><code>print(f\"\\n*** Processing date: {index}. Cash In Hand: {cash_in_hand} ***\")\n    # random delay between 3 to 6 seconds for avoiding rate limiting\n    time.sleep(random.uniform(3, 6))\n    date = index  # The date is the index, not row['date']\n\n    # Convert date string to datetime to get day name\n    date_obj = pd.to_datetime(date)\n    day_of_the_date = date_obj.day_name()\n</code></pre></p> <p>Filter only the values upto 2.30 PM from validation dataset and construct a chat message to be passed to the LLM. <pre><code>    # Filter row data to only include times until 2:30 PM\n    row_until_230 = row[row.index.isin(times_until_230)]\n\n    # Get the current day's data for post-2:30 PM calculations\n    day_data = row  # This is the full day's data\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a stock market expert. You will predict the most profit making position and what is the expected percentage change in the next 1 hour given the percentage movement between 9.15 AM and 2.30 PM.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"The price movements in percentage from 9:15 AM to 2:30 PM on \" + str(date) + \" (\" + day_of_the_date + \") are as follows: \" + str(row_until_230.values.tolist()) + \".\"\n        }\n    ]\n</code></pre></p> <p>Call the LLM and get its response. <pre><code>    response = client.chat.completions.create(\n        messages=messages,\n        temperature=1.0,\n        top_p=1.0,\n        frequency_penalty=0.0,\n        presence_penalty=0.0,\n        model=deployment\n    )\n\n    print(response.choices[0].message.content)\n</code></pre></p> <p>Calculate the position and target price based on the response from the LLM. If the LLM provided output is more than 1, it is a \"Call\" otherwise it's a \"Put\". We don't have corresponding options prices. So, we take it against the index price itself. <pre><code>    target_percentage = float(response.choices[0].message.content.strip())\n    base_price = validation_price.loc[date].loc['14:30']\n    metrics['total_trades'] += 1\n    if target_percentage &gt;= 1:\n        position = \"Call\"\n        target_price = base_price * target_percentage\n        stop_loss = base_price * 0.99 # 1% fall is stop loss\n        print(f\"Position: {position}, Base Price: {base_price} Target Price: {target_price}, Stop Loss: {stop_loss}\")\n    else:\n        position = \"Put\"\n        target_price = base_price * target_percentage\n        stop_loss = base_price * 1.01 # 1% raise is stop loss\n        print(f\"Position: {position}, Base Price: {base_price} Target Price: {target_price}, Stop Loss: {stop_loss}\")\n</code></pre></p> <p>Now the actual validation happens. Iterate over the price from 2:31 PM to 3:26 PM. Book profit or loss based on below rules.</p> <ul> <li>If target price reached, book profit</li> <li>If stop-loss reached, book loss</li> <li>If neither of the above condition occurred by time reached 3.26 PM, close the position with whatever price at the moment</li> </ul> <pre><code>    # Price sequence from 2.31 to 3.30\n    price_after_230 = validation_price.loc[date].loc['14:30':]\n    for time_str, price in price_after_230.items():\n        # Calculate the expected price movement\n        if position == \"Call\":\n\n            if price &gt;= target_price:\n                # Book profit\n                total_stocks = cash_in_hand // base_price\n                profit = (price - base_price) * total_stocks\n                cash_in_hand += profit\n                print(f\"{time_str}: Booking Profit Rs.{profit} at price {price} for {total_stocks} stocks\")\n                metrics['profitable_trades'] += 1\n                break\n\n            elif price &lt;= stop_loss:\n                total_stocks = cash_in_hand // base_price\n                loss = (base_price - price) * total_stocks\n                cash_in_hand -= loss\n                print(f\"{time_str}: Booking Loss Rs.{loss} at price {price} for {total_stocks} stocks\")\n                metrics['unprofitable_trades'] += 1\n                metrics['stop_loss'] += 1\n                break\n\n            if time_str == '15:26':\n                metrics['time_up'] += 1\n                total_stocks = cash_in_hand // base_price\n                final_price = price\n\n                if final_price &gt; base_price:\n                    profit = (final_price - base_price) * total_stocks\n                    cash_in_hand += profit\n                    print(f\"{time_str}: Booking Profit Rs.{profit} at price {final_price} for {total_stocks} stocks\")\n                    metrics['profitable_trades'] += 1\n\n                else:\n                    loss = (base_price - final_price) * total_stocks\n                    cash_in_hand -= loss\n                    print(f\"{time_str}: Booking Loss Rs.{loss} at price {final_price} for {total_stocks} stocks\")\n                    metrics['unprofitable_trades'] += 1\n\n                break\n\n        else:\n\n            if price &lt;= target_price:\n                total_stocks = cash_in_hand // base_price\n                profit = (base_price - price) * total_stocks\n                cash_in_hand += profit\n                print(f\"{time_str}: Booking Profit Rs.{profit} at price {price} for {total_stocks} stocks\")\n                metrics['profitable_trades'] += 1\n                break\n\n            elif price &gt;= stop_loss:\n                total_stocks = cash_in_hand // base_price\n                loss = (price - base_price) * total_stocks\n                cash_in_hand -= loss\n                print(f\"{time_str}: Booking Loss Rs.{loss} at price {price} for {total_stocks} stocks\")\n                metrics['unprofitable_trades'] += 1\n                break\n\n            if time_str == '15:26':\n                metrics['time_up'] += 1\n                total_stocks = cash_in_hand // base_price\n                final_price = price\n\n                if final_price &lt; base_price:\n                    profit = (base_price - final_price) * total_stocks\n                    cash_in_hand += profit\n                    print(f\"{time_str}: Booking Profit Rs.{profit} at price {final_price} for {total_stocks} stocks\")\n                    metrics['profitable_trades'] += 1\n\n                else:\n                    loss = (final_price - base_price) * total_stocks\n                    cash_in_hand -= loss\n                    print(f\"{time_str}: Booking Loss Rs.{loss} at price {final_price} for {total_stocks} stocks\")\n                    metrics['unprofitable_trades'] += 1\n\n                break\n</code></pre> <p>At the end of the <code>for</code> loop, break it if expected number of trades have happened. At the end print the final cash-in-hand and metrics. <pre><code>    if metrics['total_trades'] &gt;= count:\n        break\n\nprint(f\"\\n*** Final Cash in Hand: Rs.{cash_in_hand} ***\")\nprint(f\"Metrics: {metrics}\")\n</code></pre></p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#verdict","title":"Verdict","text":"<p>Overall gpt-4.1-fine-tuned made a loss for me! At end of 246 trades final cash remaining in hand was Rs.97,4114. Approximately Rs.26,000 loss. <pre><code>...\n...\n...\n*** Processing date: 2015-03-18. Cash In Hand: 971755.4500000001 ***\n0.9978\nPosition: Put, Base Price: 8693.8 Target Price: 8674.673639999999, Stop Loss: 8780.738\n14:43: Booking Profit Rs.2358.75 at price 8672.55 for 111.0 stocks\n\n*** Final Cash in Hand: Rs.974114.2000000001 ***\nMetrics: {'total_trades': 246, 'profitable_trades': 145, 'unprofitable_trades': 101, 'total_profit': 0, 'total_loss': 0, 'time_up': 136, 'stop_loss': 3}\n</code></pre>  But the important statistics here is Total number of profitable trades are higher than total number of unprofitable trades. Profitable trades account for 60% of the times. We hit stop-loss only 3 times.</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/08/25/fine-tuning-gpt-model-to-predict-nifty50-prices/#conclusion","title":"Conclusion","text":"<p>The concerning aspect is the high occurrence of time-outs, which happened 55% (136) of the time. This suggests that the 1-sigma target price might be overly ambitious. While theoretically, it should be achievable 66% of the time, the results indicate otherwise. This approach is quite rudimentary, relying on the LLM to deduce all patterns and features from the limited data provided. To improve, we should consider engineering additional features, such as the day of the week, and implementing a custom loss function that accounts for time-outs more effectively.</p> <p>Despite the lack of profitability, this experiment was a fascinating learning experience. The absence of profit doesn't necessarily mean there's no underlying pattern\u2014it simply means we haven't uncovered it yet. If you have suggestions or ideas for a better approach, feel free to connect with me on LinkedIn. Additionally, if you spot any issues in the code, please raise them on GitHub. Thank you for taking the time to read and engage with this content!</p> <p>The complete code can be found in this Notebook</p>","tags":["LLM","Fine-Tuning","Azure","NIFTY50","Stock Market","Time Series","GPT","OpenAI"]},{"location":"blog/2025/09/01/gpt-5-for-programmers/","title":"GPT-5 For Programmers","text":"<p> GPT models started as tools for creating content like text generation, summarization, and even image or video generation. But recently, the focus has shifted towards coding. After the success of Anthropic's Claude models among developers, many LLM companies, including OpenAI, began prioritizing coding tasks. This shift is largely driven by the potential for enterprise subscriptions, as coding models generate more revenue compared to individual creator tools.</p> <p>OpenAI launched GPT-5 last month, aligning with this market trend. The new model places a strong emphasis on coding capabilities. The Cursor team, after testing it internally, called it the best OpenAI model yet. Our benchmarks also showed significant improvements in solving simple and moderate problems. However, for harder problems, the success rate remains similar to GPT-4.1.</p> <p>We trained GPT-5 with developers in mind: we\u2019ve focused on improving tool calling, instruction following, and long-context understanding to serve as the best foundation model for agentic applications.</p> <p><p>- OpenAI </p></p>","tags":["LLM","OpenAI","GPT","GPT-5","Coding Agent"]},{"location":"blog/2025/09/01/gpt-5-for-programmers/#bias-to-tools","title":"Bias to tools","text":"<p>It seems GPT-5 was fine-tuned extensively for coding tasks after its initial training. The model frequently uses the <code>sed -n</code> command to read specific portions of code, typically between 100 to 200 lines. Though the new model is having a larger context window instead of reading the entire file with <code>cat</code> it chooses to stick with the <code>sed</code> command. This approach helps keep the context focused. Interestingly, even when the agent has access to a more versatile tool like <code>edit_anthropic</code> for both reading and writing, the model still prefers <code>sed -n</code> in most cases. This strong bias towards a specific tool suggests that the fine-tuning process emphasized specific coding related fine-tuning, going beyond general training on publicly available data.</p>","tags":["LLM","OpenAI","GPT","GPT-5","Coding Agent"]},{"location":"blog/2025/09/01/gpt-5-for-programmers/#levers-to-adjust","title":"Levers to adjust","text":"<p>OpenAI Cookbook has published a GPT-5 Prompting Guide, which dives into the model's personality, API parameters, and prompt techniques. One highlight from the guide is how the Cursor team optimized the model's thought responses. In agentic workflows, the model often only needs to perform actions, so it typically sends a function call without explaining its reasoning. While this saves tokens, it can make debugging or auditing difficult since the reasoning behind actions isn't clear. This explanation, when included, is called a <code>tool-preamble</code>.</p> <p>The GPT-5 inference API allows developers to adjust the verbosity of these tool-preambles. OpenAI also introduced a <code>&lt;tool_preambles&gt;</code> tag in prompts to control this directly. By default, GPT-5 is more verbose, so the Cursor team experimented with reducing verbosity via the API. However, this made the model too silent. They eventually adopted a hybrid approach: setting minimal verbosity in the API while asking the model to provide concise explanations using the <code>&lt;tool_preambles&gt;</code> tag. This balance ensures just enough context is provided before tool calls, offering flexibility for developers to fine-tune the behavior to their needs.</p> <p>OpenAI has made significant strides in Instruction Following with GPT-5, claiming the model now follows prompts with surgical precision. While this is a powerful improvement, it comes with a caveat: poorly crafted prompts can lead to unintended results. Essentially, the model prioritizes the given instructions over its own reasoning, putting the responsibility\u2014and control\u2014squarely in the hands of developers.</p> <p>However, there\u2019s a quirk. When given a list of tasks and instructed to handle them one at a time\u2014completing the first task before moving to the next\u2014the model tends to process all tasks together. This behavior can be problematic when tasks are independent and require a fresh context. To work around this, developers often spin up a new agent instance for each task, which adds some overhead but ensures clean execution. While not perfect, this approach highlights the importance of understanding and adapting to the model\u2019s behavior.</p> <p>Another big leap with GPT-5 is its reasoning ability. By default, the model is thorough\u2014it reads multiple files to gather all the context it needs before making changes. We've seen this firsthand in our workflows, and it's a game-changer for solving simple to moderate problems. While this approach uses more tokens, the quality of results makes it worth it. OpenAI calls this behavior Naturally Introspective.</p> <p>However, not all workflows need the model to handle context building. Some agent setups already prepare precise tasks, resolving all the dependencies beforehand. In such cases, GPT-5\u2019s eagerness to gather context again can be unnecessary. To address this, OpenAI offers two ways to manage it: the <code>&lt;context_gathering&gt;</code> tag in prompts or the <code>reasoning_effort</code> parameter in the API. Both options let developers fine-tune how much reasoning the model applies, ensuring it fits seamlessly into different workflows.</p>","tags":["LLM","OpenAI","GPT","GPT-5","Coding Agent"]},{"location":"blog/2025/09/01/gpt-5-for-programmers/#conclusion","title":"Conclusion","text":"<p>Taken together, the improvements \u2014 Natural awareness of reading code iteratively, ability to control reasoning, precise instruction following \u2014 are clearly aimed at coding. That focus is actually good for developers: the new parameters and prompting techniques give you the levers to control the model. The LLM becomes another tool in your automation workflow arsenal, making previously brittle tasks (like web scraping) automatable quickly and even non\u2011linearly. </p>","tags":["LLM","OpenAI","GPT","GPT-5","Coding Agent"]},{"location":"blog/archive/2025/","title":"September 2025","text":""},{"location":"blog/archive/2021/","title":"July 2021","text":""},{"location":"blog/archive/2020/","title":"November 2020","text":""},{"location":"blog/archive/2019/","title":"June 2019","text":""},{"location":"blog/archive/2018/","title":"October 2018","text":""},{"location":"blog/archive/2017/","title":"September 2017","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/page/4/","title":"Blog","text":""},{"location":"blog/page/5/","title":"Blog","text":""},{"location":"blog/page/6/","title":"Blog","text":""},{"location":"blog/page/7/","title":"Blog","text":""},{"location":"blog/page/8/","title":"Blog","text":""},{"location":"blog/page/9/","title":"Blog","text":""},{"location":"blog/page/10/","title":"Blog","text":""},{"location":"blog/page/11/","title":"Blog","text":""},{"location":"blog/page/12/","title":"Blog","text":""},{"location":"blog/page/13/","title":"Blog","text":""},{"location":"blog/page/14/","title":"Blog","text":""},{"location":"blog/page/15/","title":"Blog","text":""},{"location":"blog/page/16/","title":"Blog","text":""},{"location":"blog/page/17/","title":"Blog","text":""},{"location":"blog/page/18/","title":"Blog","text":""},{"location":"blog/page/19/","title":"Blog","text":""},{"location":"blog/page/20/","title":"Blog","text":""},{"location":"blog/page/21/","title":"Blog","text":""},{"location":"blog/page/22/","title":"Blog","text":""},{"location":"blog/page/23/","title":"Blog","text":""},{"location":"blog/page/24/","title":"Blog","text":""},{"location":"blog/page/25/","title":"Blog","text":""},{"location":"blog/page/26/","title":"Blog","text":""},{"location":"blog/page/27/","title":"Blog","text":""},{"location":"blog/archive/2025/page/2/","title":"September 2025","text":""},{"location":"blog/archive/2025/page/3/","title":"September 2025","text":""},{"location":"blog/archive/2021/page/2/","title":"July 2021","text":""},{"location":"blog/archive/2020/page/2/","title":"November 2020","text":""},{"location":"blog/archive/2020/page/3/","title":"November 2020","text":""},{"location":"blog/archive/2020/page/4/","title":"November 2020","text":""},{"location":"blog/archive/2020/page/5/","title":"November 2020","text":""},{"location":"blog/archive/2019/page/2/","title":"June 2019","text":""},{"location":"blog/archive/2018/page/2/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/3/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/4/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/5/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/6/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/7/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/8/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/9/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/10/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/11/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/12/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/13/","title":"October 2018","text":""},{"location":"blog/archive/2018/page/14/","title":"October 2018","text":""}]}